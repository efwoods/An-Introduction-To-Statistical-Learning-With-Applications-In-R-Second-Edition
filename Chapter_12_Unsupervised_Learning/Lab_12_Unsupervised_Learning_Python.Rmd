---
title: "Lab 12 Unsupervised Learning (Python)"
author: "Evan Woods"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
# knitr::opts_chunk$set(fig.width = 6)
# knitr::opts_chunk$set(fig.asp = 0.618)
# knitr::opts_chunk$set(out.width = "70%")
# knitr::opts_chunk$set(fig.align = "center")
# knitr::opts_chunk$set(
#   comment = ""
# )
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression
if(!require("splines")) install.packages("splines")
if(!require("gam")) install.packages("gam")
if(!require("akima")) install.packages("akima")
if(!require("tree")) install.packages("tree") # Classification and Regression Trees
if(!require("randomForest")) install.packages("randomForest")
if(!require("gbm")) install.packages("gbm") # Boosted Trees
if(!require("BART")) install.packages("BART")
if(!require("reticulate")) install.packages("reticulate") # Use python objects in R
if(!require("ROCR")) install.packages("ROCR")
if(!require("keras")) install.packages("keras") # Install keras for deep learning
if(!require("jpeg")) install.packages("jpeg")
if(!require("imager")) install.packages("imager")
if(!require("survival")) install.packages("survival")

library(survival)
library(imager)
library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
library(ROCR)
library(reticulate)
library(BART)
library(gbm)
library(randomForest)
library(tree)
library(akima)
library(gam)
library(splines)
library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
library(jpeg)
```

```{r output=FALSE, results = 'hide', message=FALSE}
# keras::install_keras(method = "conda", python_version = "3.10")
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```

```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.datasets import get_rdataset
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from ISLP import load_data
```

```{python}
from sklearn.cluster import \
  (KMeans, AgglomerativeClustering)
from scipy.cluster.hierarchy import \
  (dendrogram,
  cut_tree)
from ISLP.cluster import compute_linkage
```

## Principal Component Analysis
```{python}
USArrests = get_rdataset("USArrests").data
```

```{python}
USArrests.columns
```

```{python}
USArrests.mean()
```

```{python}
USArrests.var()
```

```{python}
scaler = StandardScaler(with_std=True, with_mean=True)
USArrests_scaled = scaler.fit_transform(USArrests)
```

```{python}
pcaUS = PCA()
```

```{python}
pcaUS.fit(USArrests_scaled)
```

```{python}
pcaUS.mean_
```

```{python}
scores = pcaUS.transform(USArrests_scaled)
```

```{python}
i, j = 0, 1 # which components
fig, ax = plt.subplots(1, 1, figsize = (8, 8))
ax.scatter(scores[:, 0], scores[:, 1])
ax.set_xlabel('PC%d' % (i + 1))
ax.set_ylabel('PC%d' % (j + 1))
for k in range(pcaUS.components_.shape[1]):
  ax.arrow(0, 0, pcaUS.components_[i, k], pcaUS.components_[j, k])
  ax.text(pcaUS.components_[i, k],
          pcaUS.components_[j, k],
          USArrests.columns[k])
plt.show()
```

```{python}
scale_arrow = s_ = 2
scores[:,1] *= -1
pcaUS.components_[1] *= -1
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
ax.scatter(scores[:,0], scores[:,1])
ax.set_xlabel('PC%d' % (i+1))
ax.set_ylabel('PC%d' % (j+1))
for k in range(pcaUS.components_.shape[1]):
  ax.arrow(0, 0, s_*pcaUS.components_[i,k], s_*pcaUS.components_[j,k])
  ax.text(s_*pcaUS.components_[i,k],
          s_*pcaUS.components_[j,k],
          USArrests.columns[k])
plt.show()
```

```{python}
scores.std(0, ddof=1)
```
```{python}
pcaUS.explained_variance_
```

```{python}
pcaUS.explained_variance_ratio_
```

```{python}
# %%capture
fig, axes = plt.subplots(1, 2, figsize=(15, 6))
ticks = np.arange(pcaUS.n_components_)+1
ax = axes[0]
ax.plot(ticks, pcaUS.explained_variance_ratio_, marker='o')
ax.set_xlabel('Principal Component')
ax.set_ylabel('Proportion of Variance Explained')
ax.set_ylim([0, 1])
ax.set_xticks(ticks)
ax = axes[1]
ax.plot(ticks, 
        pcaUS.explained_variance_ratio_.cumsum(),
        marker = 'o')
ax.set_xlabel('Principal Component')
ax.set_ylabel('Cumulative Proportion of Variance Explained')
ax.set_ylim([0, 1])
ax.set_xticks(ticks)
fig
plt.show()
```

```{python}
a = np.array([1,2,8,-3])
np.cumsum(a)
```

## Matrix Completion
```{python}
X = USArrests_scaled
U, D, V = np.linalg.svd(X, full_matrices=False)
U.shape, D.shape, V.shape
```

```{python}
V
```

```{python}
pcaUS.components_
```

```{python}
(U * D[None,:])[:3]
```

```{python}
scores[:3]
```

```{python}
n_omit = 20
np.random.seed(15)
r_idx = np.random.choice(np.arange(X.shape[0]),
                         n_omit,
                         replace = False)
c_idx = np.random.choice(np.arange(X.shape[1]),
                         n_omit,
                         replace = True)
Xna = X.copy()
Xna[r_idx, c_idx] = np.nan
```

```{python}
def low_rank(x, M=1):
  U, D, V = np.linalg.svd(X)
  L = U[:, :M] * D[None,:M]
  return L.dot(V[:M])
Xhat = Xna.copy()
Xbar = np.nanmean(Xhat, axis = 0)
Xhat[r_idx, c_idx] = Xbar[c_idx]
```

```{python}
thresh = 1e-7
rel_err = 1
count = 0
ismiss = np.isnan(Xna)
mssold = np.mean(Xhat[~ismiss]**2)
mss0 = np.mean(Xna[~ismiss]**2)
```

```{python}
while rel_err > thresh:
  count += 1
  # Step 2(a)
  Xapp = low_rank(Xhat, M=1)
  # Step 2(b)
  Xhat[ismiss] = Xapp[ismiss]
  # Step 2(c)
  mss = np.mean(((Xna - Xapp)[~ismiss])**2)
  rel_err = (mssold - mss) / mss0
  mssold = mss
  print("Iteration: {0}, MSS:{1:.3f}, Rel.Err {2:.2e}"
  .format(count, mss, rel_err))
```

```{python}
np.corrcoef(Xapp[ismiss], X[ismiss])[0,1]
```

```{python}
np.random.seed(0);
X = np.random.standard_normal((50, 2));
X[:25, 0] += 3;
X[:25, 1] -= 4;
```

```{python}
kmeans = KMeans(n_clusters=2,
                random_state=2,
                n_init=20).fit(X) 
```

```{python}
kmeans.labels_
```

```{python}
fig, ax = plt.subplots(1, 1, figsize=(8,8))
ax.scatter(X[:, 0], X[:,1], c=kmeans.labels_)
ax.set_title("K-Means Clustering Results with K=2");
plt.show()
```

```{python}
kmeans = KMeans(n_clusters=3, 
                random_state=3,
                n_init=20).fit(X)
fig, ax = plt.subplots(figsize=(8,8))
ax.scatter(X[:,0], X[:,1], c=kmeans.labels_)
ax.set_title("K-Means Clustering Results with K=3")
plt.show()
```

```{python}
kmeans1 = KMeans(n_clusters=3,
                 random_state=3,
                 n_init=1).fit(X)
kmeans20 = KMeans(n_clusters=3, 
                  random_state=3,
                  n_init=20).fit(X);
kmeans1.inertia_, kmeans20.inertia_
```


```{python}
## Agglomerative Clustering
HClust = AgglomerativeClustering
hc_comp = HClust(distance_threshold=0,
                 n_clusters=None,
                 linkage='complete')
hc_comp.fit(X)
```


```{python}
hc_avg = HClust(distance_threshold=0,
                n_clusters=None,
                linkage='average');
hc_avg.fit(X);
hc_sing = HClust(distance_threshold=0,
                 n_clusters=None,
                 linkage='single');
hc_sing.fit(X);
```


```{python}
D = np.zeros((X.shape[0], X.shape[0]));
for i in range(X.shape[0]):
  x_ = np.multiply.outer(np.ones(X.shape[0]), X[i])
  D[i] = np.sqrt(np.sum((X-x_)**2, 1));
hc_sing_pre = HClust(distance_threshold=0,
                     n_clusters=None,
                     metric="precomputed",
                     linkage='single')
hc_sing_pre.fit(D)
```

```{python}
cargs = {'color_threshold':-np.inf, 
         'above_threshold_color':'black'}
linkage_comp = compute_linkage(hc_comp)
fig, ax = plt.subplots(1, 1, figsize=(8,8))
dendrogram(linkage_comp, ax=ax, **cargs);
plt.show()
```

```{python}
fig, ax = plt.subplots(1, 1, figsize=(8, 8))
dendrogram(linkage_comp,
           ax = ax,
           color_threshold=4,
           above_threshold_color='black');
plt.show()
```

```{python}
cut_tree(linkage_comp, n_clusters=4).T
```

```{python}
cut_tree(linkage_comp, height=5)
```

```{python}
scaler = StandardScaler()
X_scale = scaler.fit_transform(X)
hc_comp_scale = HClust(distance_threshold = 0,
                       n_clusters=None,
                       linkage='complete').fit(X_scale)

linkage_comp_scale = compute_linkage(hc_comp_scale)

fig, ax = plt.subplots(1, 1, figsize=(8, 8))
dendrogram(linkage_comp_scale, ax=ax, **cargs)
ax.set_title("Hierarchical Clustering with Scaled Features");
plt.show()
```


```{python}
X = np.random.standard_normal ((30, 3))
corD = 1 - np.corrcoef(X)
hc_cor = HClust(linkage='complete',
                distance_threshold=0,
                n_clusters=None,
                metric='precomputed')
                
hc_cor.fit(corD)
linkage_cor = compute_linkage(hc_cor)
fig , ax = plt.subplots (1, 1, figsize =(8, 8))
dendrogram(linkage_cor , ax=ax , **cargs)
ax.set_title("Complete Linkage with Correlation-Based Dissimilarity");
plt.show()
```

## NCI60 Data Example
```{python}
NCI60 = load_data('NCI60')
nci_labs = NCI60['labels']
nci_data = NCI60['data']
```

```{python}
nci_data.shape
```

```{python}
nci_labs.value_counts()
```

```{python}
scaler = StandardScaler()
nci_scaled = scaler.fit_transform(nci_data)
nci_pca = PCA()
```

```{python}
nci_scores = nci_pca.fit_transform(nci_scaled)
```

```{python}
cancer_types = list(np.unique(nci_labs))
nci_groups = np.array([cancer_types.index(lab)
                       for lab in nci_labs.values])
fig, axes = plt.subplots(1, 2, figsize=(15,6))
ax = axes[0]
ax.scatter(nci_scores[:, 0],
           nci_scores[:, 1], 
           c=nci_groups, 
           marker='o',
           s=50)
ax.set_xlabel('PC1'); ax.set_ylabel('PC2')
ax = axes[1]
ax.scatter(nci_scores[:,0], 
           nci_scores[:,2],
           c=nci_groups,
           marker='o',
           s=50)
ax.set_xlabel('PC1'); ax.set_ylabel('PC3');
plt.show()
```


```{python}
fig, axes = plt.subplots(1, 2, figsize=(15, 6))
ax = axes[0]
ticks = np.arange(nci_pca.n_components_)+1

ax.plot(ticks,
        nci_pca.explained_variance_ratio_,
        marker='o')
ax.set_xlabel('Principal Component')
ax.set_ylabel('PVE')
ax = axes[1]
ax.plot(ticks,
        nci_pca.explained_variance_ratio_.cumsum(),
        marker='o');
ax.set_xlabel('Principal Component')
ax.set_ylabel('Cumulative PVE');
plt.show()
```

```{python}
def plot_nci(linkage, ax, cut=-np.inf):
  cargs = {'above_threshold_color':'black',
  'color_threshold':cut}
  hc = HClust(n_clusters=None,
              distance_threshold=0,
              linkage=linkage.lower()).fit(nci_scaled)
  linkage_ = compute_linkage(hc)
  dendrogram(linkage_, 
             ax=ax,
             labels=np.asarray(nci_labs),
             leaf_font_size=10,
             **cargs)
  ax.set_title('%s Linkage' % linkage)
  return hc
```

```{python}
fig, axes = plt.subplots(3, 1, figsize=(15,30))
ax = axes[0]; hc_comp = plot_nci('Complete', ax)
ax = axes[1]; hc_avg = plot_nci('Average', ax)
ax = axes[2]; hc_sing = plot_nci('Single', ax)
```

```{python}
linkage_comp = compute_linkage(hc_comp)
comp_cut = cut_tree(linkage_comp, n_clusters = 4).reshape(-1)
pd.crosstab(nci_labs['label'], pd.Series(comp_cut.reshape(-1), name='Complete'))
```


```{python}
fig, ax = plt.subplots(figsize=(10, 10))
plot_nci('Complete', ax, cut=140)
ax.axhline(140, c='r', linewidth=4);
plt.show()
```

```{python}
nci_kmeans = KMeans(n_clusters=4, random_state=0, n_init=20).fit(nci_scaled)
pd.crosstab(pd.Series(comp_cut, name='HClust'), pd.Series(nci_kmeans.labels_, name='K-means'))
```

```{python}
hc_pca = HClust(n_clusters=None,
                distance_threshold=0,
                linkage='complete').fit(nci_scores[:,:5])
linkage_pca = compute_linkage(hc_pca)
```


```{python}
fig, ax = plt.subplots(figsize=(8,8))
dendrogram(linkage_pca, labels=np.asarray(nci_labs),
           leaf_font_size=10,
           ax=ax,
           **cargs)
plt.show()
```
