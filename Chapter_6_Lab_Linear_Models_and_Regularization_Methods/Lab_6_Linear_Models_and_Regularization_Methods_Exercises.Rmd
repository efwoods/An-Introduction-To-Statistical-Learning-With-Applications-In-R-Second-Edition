---
title: "Lab 6 Linear Models and Regularization Methods Exercises"
author: "Evan Woods"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
  comment = ""
)
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression

library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
```

```{r include=FALSE}
LoadLibraries <- function() {
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")

library(caret)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
  print("Libraries have been loaded!")
}
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```

```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

```{r echo=FALSE}
set.seed(1)
```

## Applied:
### Question 8: 
In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

* **Question 8-a**: Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector ε of length n = 100.
  * **Answer**:
```{r echo=TRUE}
X = rnorm(100)
ε = rnorm(100)
```

* **Question 8-b**: Generate a response vector Y of length n = 100 according to the model Y = β~0~ + β~1~X + β~2~X^2^ + β~3~X^3^ + ε, where β~0~,  β~1~,  β~2~, and  β~3~ are constants of your choice.
  * **Answer**:
```{r echo=TRUE}
β_0 <- 10
β_1 <- 1
β_2 <- 2
β_3 <- 3
n = 100
Y <- β_0 + (β_1 * X) + (β_2 * X^2) + (β_3 * X^3) + ε
```

* **Question 8-c**: Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X, X^2^, ..., X^10^. What is the best model obtained according to C~p~, BIC, and adjusted R^2^? Show plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y.
  * **Answer**:

```{r echo=FALSE}
df <- tibble(Y, X, X^2, X^3, X^4, X^5, X^6, X^7, X^8, X^9, X^10)
best_subset <- regsubsets(Y ~ ., data = df, nvmax = 10)
summary_best_subset <- summary(best_subset)
summary_best_subset
```


```{r echo=FALSE}
f_print(sprintf("The best model obtained according to Mallows' Cp is model %0.0f.", which.min(summary_best_subset$cp)))
cat("\n")
f_print(sprintf("The best model obtained according to Bayesian Information Criterion is model %0.0f.", which.min(summary_best_subset$bic)))
cat("\n")
f_print(sprintf("The best model obtained according to Adjusted R-squared is model %0.0f.", which.max(summary_best_subset$adjr2)))
cat("\n")
```

```{r echo=FALSE}
plot(summary_best_subset$cp, type = "l", ylab = "Mallows' Cp", xlab = "Model Index", main = "Best Subset: Mallows' Cp vs. Model Index")
points(which.min(summary_best_subset$cp), summary_best_subset$cp[which.min(summary_best_subset$cp)], col = "red", cex = 2, pch = 20)
plot(summary_best_subset$bic, type = "l", xlab = "Model Index", ylab = "Bayesian Information Criterion", main = "Best Subset: BIC vs. Model Index")
points(which.min(summary_best_subset$bic), summary_best_subset$bic[which.min(summary_best_subset$bic)], col = "red", cex = 2, pch = 20)
plot(summary_best_subset$adjr2, type = "l", xlab = "Model Index", ylab = "Adjusted R-squared", main = "Best Subset: Adjusted R-squared vs. Model Index")
points(which.max(summary_best_subset$adjr2), summary_best_subset$adjr2[which.max(summary_best_subset$adjr2)], col = "red", cex = 2, pch = 20)
```

```{r echo=FALSE}
f_print(sprintf("The coefficients for the best model obtained:"))
cat("\n")
coef(best_subset, which.max(summary_best_subset$adjr2))
```

* **Question 8-d**: Repeat the previous question using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in the previous question?
  * **Answer**:
```{r echo=FALSE}
forward <- regsubsets(Y ~ ., data = df, nvmax = 10, method = "forward")
backward <- regsubsets(Y ~ ., data = df, nvmax = 10, method = "backward")

forward.summary <- summary(forward)
backward.summary <- summary(backward)
```

```{r echo=FALSE}
forward.summary
f_print(sprintf("The best model obtained implementing forward stepwise selection according to Mallows' Cp is model %0.0f.", which.min(forward.summary$cp)))
cat("\n")
f_print(sprintf("The best model obtained implementing forward stepwise selection according to Bayesian Information Criterion is model %0.0f.", which.min(forward.summary$bic)))
cat("\n")
f_print(sprintf("The best model obtained implementing forward stepwise selection according to Adjusted R-squared is model %0.0f.", which.max(forward.summary$adjr2)))
cat("\n")
```

```{r echo=FALSE}
plot(forward.summary$cp, type = "l", main = "Forward Stepwise: Mallows' Cp vs. Model Index", xlab = "Model Index", ylab = "Mallows' Cp")
points(which.min(forward.summary$cp), forward.summary$cp[which.min(forward.summary$cp)], col = "red", cex = 2, pch = 20)

plot(forward.summary$bic, type = "l", main = "Forward Stepwise: BIC vs. Model Index", xlab = "Model Index", ylab = "Bayesian Information Criterion")
points(which.min(forward.summary$bic), forward.summary$bic[which.min(forward.summary$bic)], col = "red", cex = 2, pch = 20)

plot(forward.summary$adjr2, type = "l", main = "Forward Stepwise: Adjusted R-squared vs. Model Index", xlab = "Model Index", ylab = "Adjusted R-squared")
points(which.max(forward.summary$adjr2), forward.summary$adjr2[which.max(forward.summary$adjr2)], col = "red", cex = 2, pch = 20)

```

```{r echo=FALSE}
backward.summary
f_print(sprintf("The best model obtained implementing backward stepwise selection according to Mallows' Cp is model %0.0f.", which.min(backward.summary$cp)))
cat("\n")
f_print(sprintf("The best model obtained implementing backward stepwise selection according to Bayesian Information Criterion is model %0.0f.", which.min(backward.summary$bic)))
cat("\n")
f_print(sprintf("The best model obtained implementing backward stepwise selection according to Adjusted R-squared is model %0.0f.", which.max(backward.summary$adjr2)))
cat("\n")
```


```{r echo=FALSE}
plot(backward.summary$cp, type = "l", main = "Backward Stepwise: Mallows' Cp vs. Model Index", xlab = "Model Index", ylab = "Mallows' Cp")
points(which.min(backward.summary$cp), backward.summary$cp[which.min(backward.summary$cp)], col = "red", cex = 2, pch = 20)

plot(backward.summary$bic, type = "l", main = "Backward Stepwise: BIC vs. Model Index", xlab = "Model Index", ylab = "Bayesian Information Criterion")
points(which.min(backward.summary$bic), backward.summary$bic[which.min(backward.summary$bic)], col = "red", cex = 2, pch = 20)

plot(backward.summary$adjr2, type = "l", main = "Backward Stepwise: Adjusted R-squared vs. Model Index", xlab = "Model Index", ylab = "Adjusted R-squared")
points(which.max(backward.summary$adjr2), backward.summary$adjr2[which.max(backward.summary$adjr2)], col = "red", cex = 2, pch = 20)
```


```{r echo=FALSE}
f_print(sprintf("3 different rubrics (Mallows' Cp, Bayesian Information Criterion, and Adjusted R-squared) identified the same best models independent of whether those models were created using the forward stepwise selection method or the best subset selection method. Those models were models 4, 3, & 4 respectively. However, the same three metrics identified models 7, 5, & 7 as the best models when those models were created using backward stepwise selection."))
```
* **Question 8-e**: Now fit a lasso model to the simulated data, again using X, X^2^, ..., X^10^ as predictors. Use cross-validation to select the optimal value of λ. Create plots of the cross-validation error as a function of λ. Report the resulting coefficient estimates, and discuss the results obtained. 
  * **Answer**:
```{r echo=FALSE}
set.seed(42)
x <- model.matrix(Y ~ ., data = df)

train <- sample(nrow(x), nrow(x)/2)
test <- (!train)
y.test <- Y[test]

grid <- 10^seq(10, -2, length = 100)
lasso_cv_fit <- cv.glmnet(x[train, ], Y[train], alpha = 1)
plot(lasso_cv_fit)

lasso_full_fit <- glmnet(x, Y, data = df, alpha = 1, lambda = grid)
lasso_full_fit_pred <- predict(lasso_full_fit, s = lasso_cv_fit$lambda.min, type = "coefficients")[1:11, ]
f_print(sprintf("The coefficient estimates of the full model fit for the best value of lambda that minimizes mean square error are the following:"))
cat("\n")
lasso_full_fit_pred
```
* **Question 8-f**: Now generate a response vector Y according to the model Y = β~0~ + β~7~X^7^ + ε and perform best subset selection and the lasso. Discuss the results obtained. 
  * **Answer**:
  
```{r echo=FALSE}
set.seed(1)
# Best Subset Selection
X = rnorm(100)
ε = rnorm(100)
β_7 <- 7
Y <- β_0 + (β_7*(X^7)) + ε

df <- tibble(Y, X, X^2, X^3, X^4, X^5, X^6, X^7, X^8, X^9, X^10)

best_subset <- regsubsets(Y ~ ., data = df, nvmax = 10)
summary_best_subset <- summary(best_subset)
summary_best_subset

f_print(sprintf("The best model obtained according to Mallows' Cp is model %0.0f.", which.min(summary_best_subset$cp)))
cat("\n")
f_print(sprintf("The best model obtained according to Bayesian Information Criterion is model %0.0f.", which.min(summary_best_subset$bic)))
cat("\n")
f_print(sprintf("The best model obtained according to Adjusted R-squared is model %0.0f.", which.max(summary_best_subset$adjr2)))
cat("\n")

plot(summary_best_subset$cp, type = "l", ylab = "Mallows' Cp", xlab = "Model Index", main = "Best Subset: Mallows' Cp vs. Model Index")
points(which.min(summary_best_subset$cp), summary_best_subset$cp[which.min(summary_best_subset$cp)], col = "red", cex = 2, pch = 20)
plot(summary_best_subset$bic, type = "l", xlab = "Model Index", ylab = "Bayesian Information Criterion", main = "Best Subset: BIC vs. Model Index")
points(which.min(summary_best_subset$bic), summary_best_subset$bic[which.min(summary_best_subset$bic)], col = "red", cex = 2, pch = 20)
plot(summary_best_subset$adjr2, type = "l", xlab = "Model Index", ylab = "Adjusted R-squared", main = "Best Subset: Adjusted R-squared vs. Model Index")
points(which.max(summary_best_subset$adjr2), summary_best_subset$adjr2[which.max(summary_best_subset$adjr2)], col = "red", cex = 2, pch = 20)
```
```{r echo=FALSE}
set.seed(42)
x <- model.matrix(Y ~ ., data = df)

train <- sample(nrow(x), nrow(x)/2)
test <- (!train)
y.test <- Y[test]

grid <- 10^seq(10, -2, length = 100)
lasso_cv_fit <- cv.glmnet(x[train, ], Y[train], alpha = 1)
plot(lasso_cv_fit)

lasso_full_fit <- glmnet(x, Y, data = df, alpha = 1, lambda = grid)
lasso_full_fit_pred <- predict(lasso_full_fit, s = lasso_cv_fit$lambda.min, type = "coefficients")[1:11, ]
f_print(sprintf("The coefficient estimates of the full model fit for the best value of lambda that minimizes mean square error are the following:"))
cat("\n")
lasso_full_fit_pred
```
```{r echo=FALSE}
f_print(sprintf("Using best subset selection to generate models, Mallows' Cp, Bayesian Information Criterion, & Adjusted R-squared all indicated distinct best models. BIC indicated that model 1, which was composed of a predictor of only X^7, was the best model. This was followed by Mallows' Cp which indicated X^7 paired with X^2 best modeled the true function of ƒ. Adjusted R-squared was the poorest indicator of the best model. Adjusted R-squared indicated that model 4 (composed of X, X^2, X^3, & X^7) was the best model. Lasso Regression, however, indicated the true function of ƒ where the selected coefficients of the model which is fit for the best value of lambda are the intercept and X^7. This indicates that in a real world scenario, it is best (if feasable) to fit muliple models to compare and contrast between the selected coefficients so as to determine the true function of ƒ. Had the true function of ƒ not been known, it would have been clear that X^7 and the intercept are related to the response in a significant way due to the fact that both lasso regression and another statistic, BIC in this case, identified these coefficients as pertinent to a model which predicts the desired response."))
```
```{r include = FALSE}
college <- na.omit(College)
attach(college)
```

### Question 9: 
In this exercise, we will predict the number of applications received using the other variables in the College dataset. 
* **Question 9-a**: Split the data into a training set and a test set.
```{r}
train <- sample(nrow(college), nrow(college)/2)
test <- (-train)
```

* **Question 9-b**: Fit a linear model using least squares on the training set, and report the test error obtained.
  * **Answer**:
```{r echo=FALSE}
glm_9.fit <- glm(Accept ~ ., data = college, subset = train)
glm_9.pred <- predict(glm_9.fit, college)
glm_9.test_error <- mean((glm_9.pred - college$Accept)[-train]^2)
f_print(sprintf("The test error of the linear model is: %0.1f.", glm_9.test_error))
```

* **Question 9-c**: Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.
  * **Answer**:
```{r echo=FALSE}
grid <- 10^seq(10, -2, length = 100)
x <- model.matrix(Accept ~ ., data = college)[, -1]
y <- college$Accept

ridge.model <- cv.glmnet(x[train, ], y[train], lambda = grid, alpha = 0)
ridge.model.predict <- predict(ridge.model, s = ridge.model$lambda.min, newx = x[test, ])
ridge.model.mse <- mean((ridge.model.predict - y[test])^2)
f_print(sprintf("The test error for the ridge regression model is %0.3f for the chosen value of λ: %0.3f.", ridge.model.mse, ridge.model$lambda.min))
```

* **Question 9-d**: Fit a lasso model on the training set, with λ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates. 
  * **Answer**:
```{r echo=FALSE}
set.seed(42)
lasso.model <- cv.glmnet(x[train, ], y[train], lambda = grid, alpha = 1)
lasso.model.predict <- predict(lasso.model, s = lasso.model$lambda.min, newx = x[test, ])
lasso.model.mse <- mean((lasso.model.predict - y[test])^2)
f_print(sprintf("The test error for the lasso model is %0.3f for the chosen value of λ: %0.3f.", lasso.model.mse, lasso.model$lambda.min))
cat("\n")
f_print(sprintf("The number of non-zero coefficient estimates is 8."))
```

* **Question 9-e**: Fit a PCR model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation.
  * **Answer**:

```{r echo=FALSE}
college.pcr.fit <- pcr(Accept ~ ., data = college, subset = train, scale = TRUE, validation = "CV")
summary(college.pcr.fit)
validationplot(college.pcr.fit, val.type = "MSEP")
# M = 17
```


```{r echo=FALSE}
college.pcr.pred <- predict(college.pcr.fit, college[test, ], ncomp = 17)
college.pcr.test_error <- mean((college$Accept[test] - college.pcr.pred)^2)
f_print(sprintf("The test error of the predictions generated from the pcr model is: %0.1f where the value of M that captures most of the variability in the response is: 17", college.pcr.test_error))
```

* **Question 9-f**: Fit a PLS model on the dataset with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation.
  * **Answer**:
```{r echo=FALSE}
college.pls.fit <- plsr(Accept ~ ., data = college, subset = train, scale = TRUE, validation = "CV")
summary(college.pls.fit)
validationplot(college.pls.fit, val.type = "MSEP")
# M = 11
college.pls.pred <- predict(college.pls.fit, college[test, ], ncomp = 11)

college.pls.test_error <- mean((college.pls.pred - college$Accept[test])^2)
f_print(sprintf("The test error of the partial least squares function is: %0.1f. The ideal value of M that captures the most variablility in the response with the least number of components is: 11", college.pls.test_error))
```
* **Question 9-g**: Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference resulting from the test errors among these five approaches?
  * **Answer**:
```{r echo=FALSE}
test_errors <- tibble(
  linear_model_test_error  = glm_9.test_error,
  ridge_model_test_error = ridge.model.mse,
  lasso_model_test_error = lasso.model.mse,
  PCR_test_error = college.pcr.test_error,
  PLS_test_error = college.pls.test_error
)

mean_test_error <- sum(test_errors) / length(test_errors)
sd_test_error <- sd(test_errors)
f_print(sprintf("Among the five methods, the mean test error is: %0.0f. The standard deviation of the test error is: %0.0f. There is not much difference among the test errors between these five approaches. The the principal component regression and the linear model had precisely the same test error. This is because all principal components were used to capture the most variability in the response, and using all principal components results in a model that has no dimensionality reduction. This means that the PCR does not differ from a least squares linear model fit. The ridge regression was penalized for the extra predictors, and the partial least squares resulted in the lowest test error. This is due to the fact that partial least squares required less components to capture the variability in the response than the principal component regression and therefore there was a dimensionality reduction which reduced the inclusion of predictors that did not contribute to predicting the response.", mean_test_error, sd_test_error))
test_errors %>% select(everything(), -'PCR_test_error', -'PLS_test_error')
test_errors %>% select(everything(), -'linear_model_test_error', -'ridge_model_test_error', -'lasso_model_test_error')
```


```{r echo=FALSE}
calculate_accuracy_within_tolerance <- function(truth_df, pred_df, tolerance){
  all_tolerance <- truth_df * tolerance
  predictions <- abs(truth_df - pred_df) < all_tolerance
  n_accurate_predictions <- length(predictions[predictions == 'TRUE'])
  n_inaccurate_predictions <- length(predictions[predictions == 'FALSE'])
  total_predictions <- length(predictions)
  
  accuracy <- n_accurate_predictions / total_predictions
  return(accuracy)
}


model_accuracies <- tibble(
  set_tolerance = 0,
  lm_accuracy = calculate_accuracy_within_tolerance(college[, "Accept"], glm_9.pred, set_tolerance),
  ridge_accuracy = calculate_accuracy_within_tolerance(y[test], ridge.model.predict, set_tolerance),
  lasso_accuracy = calculate_accuracy_within_tolerance(y[test], lasso.model.predict, set_tolerance),
  pcr_accuracy = calculate_accuracy_within_tolerance(college$Accept[test], college.pcr.pred, set_tolerance),
  pls_accuracy = calculate_accuracy_within_tolerance(college$Accept[test], college.pls.pred, set_tolerance)
)

for(tolerance in seq(0.05:1, by = 0.05)) {
model_accuracies <- add_row(tibble(
  set_tolerance = tolerance,
  lm_accuracy = calculate_accuracy_within_tolerance(college[, "Accept"], glm_9.pred, tolerance),
  ridge_accuracy = calculate_accuracy_within_tolerance(y[test], ridge.model.predict, tolerance),
  lasso_accuracy = calculate_accuracy_within_tolerance(y[test], lasso.model.predict, tolerance),
  pcr_accuracy = calculate_accuracy_within_tolerance(college$Accept[test], college.pcr.pred, tolerance),
  pls_accuracy = calculate_accuracy_within_tolerance(college$Accept[test], college.pls.pred, tolerance)
), model_accuracies)  
}

```

```{r echo=FALSE}
model_accuracies %>% filter(set_tolerance == 0.1)

f_print(sprintf("The model accuracies have been calculated with respect to a set tolerance of 0.1 from the true number of accepted applications as shown above. Predictions greater than or less than this tolerance have been classified as inaccurate, and predictions within this tolerance were classified as accurate predictions. Given a tolerance of 0.1, the accuracies of the models range from 31.877%% to 33.933%% accurate. At this given tolerance, these models are inaccurate more often than not. The full spectrum of model accuracies among the five models between tolerances of 0 to 1 is plotted below."))
```


```{r echo=FALSE, warning=FALSE, message=FALSE}
size = 0.2
ggplot(model_accuracies) + 
  geom_point(aes(model_accuracies$set_tolerance, model_accuracies$lm_accuracy), color = 'red') + 
  geom_smooth(aes(set_tolerance, model_accuracies$lm_accuracy), se = FALSE, color = 'red', size = size) +
  geom_point(aes(set_tolerance, model_accuracies$ridge_accuracy), color = 'green') +
  geom_smooth(aes(set_tolerance, model_accuracies$ridge_accuracy), se = FALSE, color = 'green', size = size) +
  geom_point(aes(set_tolerance, model_accuracies$lasso_accuracy), color = 'blue') +
  geom_smooth(aes(set_tolerance, model_accuracies$lasso_accuracy), se = FALSE, color = 'blue', size = size) +
  geom_point(aes(set_tolerance, model_accuracies$pcr_accuracy), color = 'gold') +
  geom_smooth(aes(set_tolerance, model_accuracies$pcr_accuracy), se = FALSE, color = 'gold', size = size) + 
  geom_point(aes(set_tolerance, model_accuracies$pls_accuracy), color = 'purple') +
  geom_smooth(aes(set_tolerance, model_accuracies$pls_accuracy), se = FALSE, color = 'purple', size = size) +
  labs(title = "Accuracies vs. Tolerance From Number of Accepted Applications", x = "Percent Tolerance From True Number of Accepted Applications", y = "Model Accuracy", subtitle = "Linear Model Accuracy: Red\nRidge Accuracy: Green\nLasso Accuracy: Blue\nPCR Accuracy: Gold\nPLS Accuracy: Purple") +
  theme_linedraw() 
  
```

### Question 10: 
We have seen that as the number of features in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated dataset.

```{r}
set.seed(42) 
```


* **Question 10-a**: Generate a data set with p = 20 features, n = 1,000 observa-
tions, and an associated quantitative response vector generated
according to the model Y = Xβ + ε
  * **Answer**:
```{r echo=TRUE}
set.seed(42) 
X <- rnorm(1000)
ε <- rnorm(100)
df <- tibble(X, X^2, X^3, X^4, X^5, X^6, X^7, X^8, X^9, X^10, X^11, X^12, X^13, X^14, X^15, X^16, X^17, X^18, X^19, X^20)

β_0 <- 10
β_1 <- 1
β_2 <- 2
β_3 <- 3
n = 100
Y <- β_0 + (β_1 * X) + (β_2 * X^2) + (β_3 * X^3) + ε

```

```{r}
df_10 <- df %>% mutate(Y = Y) %>% select(Y, everything())
```

* **Question 10-b**: Split your data into a training set containing 100 observations and a test set containing 900 observations.
  * **Answer**:
```{r echo=TRUE}
set.seed(42) 
train <- sample(1:900)
test <- (-train)
```

* **Question 10-c**: Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.
  * **Answer**:

```{r echo=FALSE}
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]]) # Identify the formula of the fit of the model. 
  mat <- model.matrix(form, newdata) # Create a matrix using the formula and the subset of the data
  coefi <- coef(object, id = id) # select the coefficients of the model with i number of predictors
  xvars <- names(coefi) # identify the predictors by name
  mat[, xvars] %*% coefi # make a prediction by all the observations in the matrix of observations that is subset by the predictors included in each i model with the corresponding coefficients of that model.
}
```


```{r echo=FALSE}
k <- 10  
n <- nrow(df_10)
set.seed(1)
p <- length(df_10) - 1
folds <- sample(rep(1:k, length = n)) # grouping each observation into k groups
cv.errors <- matrix(NA, k, p, dimnames = list(NULL, paste(1:p))) # Creating a matrix to store the errors
```

```{r echo=FALSE}
# This performs training and testing 10 times. For each trained group of models this algorithm will make a prediction for each of the best models for i number of predictors. There are p models with 1 to n number of predictors used in each model where p is the number of predictors and n is the number of predictors used in the model.
for (j in 1:k) {
  best.fit <- regsubsets(Y ~ .,
                         data = df_10[folds != j, ], nvmax = 20) # Training on all but k. Returns the best fit for 1 to 20 predictors. 
  for (i in 1:20) {
    pred <- predict(best.fit, df_10[folds != j, ], id = i) # Predict using k as the training set. This will use the custom predict function above. Best i variable model. This is making predictions for all 20 models 10 times. 
    cv.errors[j, i] <-
      mean((df_10$Y[folds != j] - pred) ^2) # Mean Square Error of each predictor within a k fold.
  }
}
```

```{r}
mean.cv.errors.train <- apply(cv.errors, 2, mean) # Identify the k which has the lowest mean train error. 2 selects the average of the rows using each column. 1 selects the average of the columns using each row.
par(mfrow = c(1,1))
plot(mean.cv.errors.train, type = "b", main = "Training Set Mean Squared Error Per Model Of Each Size", ylab = "Average Train MSE", xlab = "Number of Predictors in Model")
```

* **Question 10-d**: Plot the test set MSE associated with the best model of each size. 
  * **Answer**:
```{r}
# This performs training and testing 10 times. For each trained group of models this algorithm will make a prediction for each of the best models for i number of predictors. There are p models with 1 to n number of predictors used in each model where p is the number of predictors and n is the number of predictors used in the model.
for (j in 1:k) {
  best.fit <- regsubsets(Y ~ .,
                         data = df_10[folds != j, ], nvmax = 20) # Training on all but k. Returns the best fit for 1 to 20 number of predictors. 
  for (i in 1:20) {
    pred <- predict(best.fit, df_10[folds == j, ], id = i) # Predict using k as test. This will use the custom predict function above. Best i variable model. This is making predictions for all 20 models 10 times. 
    cv.errors[j, i] <-
      mean((df_10$Y[folds == j] - pred) ^2) # Mean Square Error of each predictor within a k fold.
  }
}
```

```{r}
mean.cv.errors.test <- apply(cv.errors, 2, mean) # Identify the k which has the lowest mean test error. 2 selects the average of the rows using each column. 1 selects the average of the columns using each row.
par(mfrow = c(1,1))
plot(mean.cv.errors.test, type = "b", main = "Test Set Mean Squared Error Per Model Of Each Size", ylab = "Average Test MSE", xlab = "Number of Predictors in Model")
```

* **Question 10-e**: For which model size does the test set MSE take on its minimum
value? Comment on your results. If it takes on its minimum value
for a model containing only an intercept or a model containing
all of the features, then work with the way that you are
generating the data in question 10-a until you come up with a scenario in
which the test set MSE is minimized for an intermediate model
size.
  * **Answer**:
```{r}
f_print(sprintf("The test set MSE takes on its minimum value for the model with %0.0f predictors.", which.min(mean.cv.errors.test)))
cat("\n")
mean.cv.errors.test
```

* **Question 10-f**: How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.
  * **Answer**:
```{r}
f_print(sprintf("The model with %0.0f predictors is close to the true function of ƒ which is comprised of 3 predictors and an intercept. The selected predictors of the model with the best fit from best subset selection and estimated coefficients thereof are included below.", which.min(mean.cv.errors.test)))
coef(best.fit, which.min(mean.cv.errors.test))
```
* **Question 10-g**: Create a plot displaying the square root of the sum of squared errors for a range of values, r, where the prediction is the jth coefficient estimate for the best model containing r coefficients. Comment on what you observe. How does this compare to the test MSE plot?
  * **Answer**:

```{r}
true_coefficients <- c(β_0, β_1, β_2, β_3, 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
val <- c(integer(20))
for (r in seq(1,20)) {
coef_estimates <- c(coef(best.fit,r), rep(0, length(true_coefficients) - length(coef(best.fit,r))))
delta <- true_coefficients - coef_estimates
 val[r] <- (sqrt(sum(delta^2)))
}
data <- tibble(y = val, x = seq(1, 20))
```


```{r}
# data %>% mutate(row_number = row_number()) %>% filter(row_number == 21)
# data
```


```{r}
ggplot(data) +
  geom_point(aes(x, y, color = y)) +
  geom_line(aes(x, y, color = y)) +
  labs(x = "Number of Coefficients in Model", y = "ƒ(x) = Root of the Sum of Square Errors", title = "Root of the Sum of Square Errors Per Best Fit Model", color = "ƒ(x)") + 
  theme_linedraw()
f_print(sprintf("The minimal value of the root of the sum of the square errors is equal to the true number of coefficients in the true function of ƒ. The shape of this graph resembles that of the graph of the Test Set Mean Squared Error Per Model of Each Size for models which have 3 to 15 coefficients. Furthermore, there is an upward trend after 15 coefficients in both plots."))
```

```{r}
plot(mean.cv.errors.test, type = "b", main = "Test Set Mean Squared Error Per Model Of Each Size", ylab = "Average Test MSE", xlab = "Number of Predictors in Model")
```


### Question 11:
We will now try to predict per capita crime rate in teh Boston data set.

* **Question 11-a**: Implement regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.
  * **Answer**:
```{r}
boston <- na.omit(Boston)
attach(boston)
```

```{r}
train <- sample(nrow(boston)*.8)
test <- (-train)
```


```{r}
boston.best_fit <- regsubsets(crim ~ . , data = boston, subset = train, nvmax = length(boston)-1)
boston.best_fit_summary <- summary(boston.best_fit)
```


```{r}
plot(boston.best_fit_summary$cp, type = "b", xlab = "Model Index", ylab = "Mallows' Cp", main = "Mallows' Cp vs. Model Index")
points(which.min(boston.best_fit_summary$cp), boston.best_fit_summary$cp[which.min(boston.best_fit_summary$cp)], col = "red", cex = 2, pch = 20)
coef(boston.best_fit, 8)
```


```{r}
plot(boston.best_fit_summary$bic, type = "b", xlab = "Model Index", ylab = "Bayesian Information Criterion", main = "Bayesian Information Criterion vs. Model Index")
points(which.min(boston.best_fit_summary$bic), boston.best_fit_summary$bic[which.min(boston.best_fit_summary$bic)], col = "red", cex = 2, pch = 20)
coef(boston.best_fit, 4)
```


```{r}
plot(boston.best_fit_summary$adjr2, type = "b", xlab = "Model Index", ylab = "Adjusted R-squared", main = "Adjusted R-squared vs. Model Index")
points(which.max(boston.best_fit_summary$adjr2), boston.best_fit_summary$adjr2[which.max(boston.best_fit_summary$adjr2)], col = "red", cex = 2, pch = 20)
coef(boston.best_fit, 10)
```

```{r}
boston.best_fit_summary
```

```{r echo=FALSE}
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]]) # Identify the formula of the fit of the model. 
  mat <- model.matrix(form, newdata) # Create a matrix using the formula and the subset of the data
  coefi <- coef(object, id = id) # select the coefficients of the model with i number of predictors
  xvars <- names(coefi) # identify the predictors by name
  mat[, xvars] %*% coefi # make a prediction by all the observations in the matrix of observations that is subset by the predictors included in each i model with the corresponding coefficients of that model.
}
```

```{r echo=FALSE}
k <- 10  
n <- nrow(boston)
set.seed(42)
p <- length(boston) - 1
folds <- sample(rep(1:k, length = n)) # grouping each observation into k groups
cv.errors <- matrix(NA, k, p, dimnames = list(NULL, paste(1:p))) # Creating a matrix to store the errors
```

```{r}
# boston[folds, ]
```


```{r}
# This performs training and testing 10 times. For each trained group of models this algorithm will make a prediction for each of the best models for i number of predictors. There are p models with 1 to n number of predictors used in each model where p is the number of predictors and n is the number of predictors used in the model.
for (j in 1:k) {
  best.fit <- regsubsets(crim ~ .,
                         data = boston[folds != j, ], nvmax = 12) # Training on all but k. Returns the best fit for 1 to 12 number of predictors. 
  for (i in 1:12) {
    pred <- predict(best.fit, boston[folds == j, ], id = i) # Predict using k as test. This will use the custom predict function above. Best i variable model. This is making predictions for all 20 models 10 times. 
    cv.errors[j, i] <-
      mean((boston$crim[folds == j] - pred) ^2) # Mean Square Error of each predictor within a k fold.
  }
}
```

```{r}
mean.cv.errors.test <- apply(cv.errors, 2, mean) # Identify the k which has the lowest mean test error. 2 selects the average of the rows using each column. 1 selects the average of the columns using each row.
par(mfrow = c(1,1))
plot(mean.cv.errors.test, type = "b", main = "Test Set Mean Squared Error Per Model Of Each Size", ylab = "Average Test MSE", xlab = "Number of Predictors in Model")
points(which.min(mean.cv.errors.test), mean.cv.errors.test[which.min(mean.cv.errors.test)], col = "red", cex = 2, pch = 20)

f_print(sprintf("The model with the lowest average MSE when using k-fold cross validation in conjuction with best subset selection is comprised of %0.0f predictors. The predictors and their corresponding coefficients include the following:", which.min(mean.cv.errors.test)))

cat("\n\n")
coef(best.fit, 11)[1:3]
cat("\n")
coef(best.fit, 11)[4:6]
cat("\n")
coef(best.fit, 11)[8:9]
cat("\n")
coef(best.fit, 11)[10:12]
```

```{r}
grid <- 10^seq(10, -2, length = 100)
x <- model.matrix(crim ~ ., data = boston)[, -1]
y <- boston$crim

ridge.model <- cv.glmnet(x[train, ], y[train], lambda = grid, alpha = 0)
ridge.model.predict <- predict(ridge.model, s = ridge.model$lambda.min, newx = x[test, ])
ridge.model.mse <- mean((ridge.model.predict - y[test])^2)
coef(ridge.model)
f_print(sprintf("The test error for the ridge regression model is %0.3f for the chosen value of λ: %0.3f.", ridge.model.mse, ridge.model$lambda.min))
```
```{r echo=FALSE}
lasso.model <- cv.glmnet(x[train, ], y[train], lambda = grid, alpha = 1)
lasso.model.predict <- predict(lasso.model, s = lasso.model$lambda.min, newx = x[test, ])
lasso.model.mse <- mean((lasso.model.predict - y[test])^2)
f_print(sprintf("The test error for the lasso model is %0.3f for the chosen value of λ: %0.3f.", lasso.model.mse, lasso.model$lambda.min))
cat("\n")
coef(lasso.model)
f_print(sprintf("The number of non-zero coefficient estimates is 2 including the intercept. The predictor is rad in the lasso model."))
```

```{r echo=FALSE}
boston.pcr.fit <- pcr(crim ~ ., data = boston, subset = train, scale = TRUE, validation = "CV")
summary(boston.pcr.fit)
validationplot(boston.pcr.fit, val.type = "MSEP", main = "Crime: MSEP vs. Number of Components", xlab = "Number of Components")
# M = 12
```


```{r echo=FALSE}
boston.pcr.pred <- predict(boston.pcr.fit, boston[test, ], ncomp = 12)
boston.pcr.test_error <- mean((boston$crim[test] - boston.pcr.pred)^2)
f_print(sprintf("The test error of the predictions generated from the pcr model is: %0.1f where the value of M that captures most of the variability in the response is: 12", boston.pcr.test_error))
```

```{r echo=FALSE}
boston.pls.fit <- plsr(crim ~ ., data = boston, subset = train, scale = TRUE, validation = "CV")
summary(boston.pls.fit)
validationplot(boston.pls.fit, val.type = "MSEP", main = "Crime: MSEP vs. Number of Components", xlab = "Number of Components")

# M = 8
```


```{r echo=FALSE}
boston.pls.pred <- predict(boston.pls.fit, boston[test, ], ncomp = 8)
boston.pls.test_error <- mean((boston.pls.pred - boston$crim[test])^2)
f_print(sprintf("The test error of the partial least squares function is: %0.1f. The ideal value of M that captures the most variablility in the response with the least number of components is: 8", boston.pls.test_error))
```

* **Question 11-b**: Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross-validation, or some other reasonable alternative, as opposed to using training error. 
  * **Answer**:

```{r echo=FALSE}
test_errors <- tibble(
  best_subset_selection_test_error  = mean.cv.errors.test[11],
  ridge_model_test_error = ridge.model.mse,
  lasso_model_test_error = lasso.model.mse,
  PCR_test_error = boston.pcr.test_error,
  PLS_test_error = boston.pls.test_error
)

mean_test_error <- sum(test_errors) / length(test_errors)
sd_test_error <- sd(test_errors)

f_print(sprintf("The mean test error of the 5 selected methods (best subset selection, ridge model, lasso model, PCR, and PLS) is %0.3f with a standard deviation of %0.03f. The lowest test MSE is derived from the best subset selection using k-fold cross validation. This best subset selection model is composed of 11 out of 12 predictors, and has a test MSE of %0.3f. The lasso, ridge, and PCR all share similar test errors. The principal component regression implements all principal components, indicating that there is a strong relationship between all the predictors. This supports the hypothesis that true function of ƒ is best described by a model with the majority of predictors in the boston dataset.  In the partial least squares model, 8 to 12 out of 12 components capture the majority of variability in the response. This indicates that there is a strong relationship between nearly all the predictors and the response. The lasso regression results in nearly average test error, but supports a best model composed of only a single predictor: rad. Given the majority of models supporting many predictors, multiple models supporting 11 predictors, and the lowest test error being held by the best subset selection model implementing 11 predictors, it is reasonable to conclude that the ideal model among those that have been created is the best subset selection model which implements 11 predictors. The values of all 5 test errors are observable as follows:", mean_test_error, sd_test_error, test_errors$best_subset_selection_test_error))
test_errors %>% select(everything(), -PCR_test_error, -PLS_test_error)
test_errors %>% select(everything(), -best_subset_selection_test_error, -ridge_model_test_error, -lasso_model_test_error)

```
  
* **Question 11-c**: Does your chosen model involve all of the features in the data set? Why or why not?
  * **Answer**:
```{r}
f_print(sprintf("The model chosen by best subset selection includes most but not all the features in the dataset. The ridge model, best subset selection, lasso, PLS, and PCR all support models with most if not all of the features in the data set. Because best subset selection has the lowest test error & is in consensus with the number of predictors in other models, the chosen model of best subset selection includes most, but not all predictors."))
```

