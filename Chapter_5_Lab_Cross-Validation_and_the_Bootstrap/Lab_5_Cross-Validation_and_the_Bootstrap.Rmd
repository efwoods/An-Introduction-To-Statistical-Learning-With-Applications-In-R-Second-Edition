---
title: "Lab 5 Cross-Validation and the Bootstrap"
author: "Evan Woods"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
  comment = ""
)
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")


library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
```

```{r include=FALSE}
LoadLibraries <- function() {
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")

library(caret)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
  print("Libraries have been loaded!")
}
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```


```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

```{r include=FALSE}
  set.seed(1)
```

## The Validation Set Approach

```{r}
# Selects 196 random numbers which range from 1 to 392
  train <- sample(392, 196) 
```

```{r}
  lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
```

```{r echo=FALSE, message=FALSE}
attach(Auto)
```


```{r}
# The estimated test mean squared error for the linear regression fit. 
mean((mpg - predict(lm.fit, Auto))[-train]^2) 
```

```{r}
mean((mpg - predict(lm.fit, Auto))[-train]^2) 

# length()
predict(lm.fit, Auto)

```


```{r}
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
```

```{r}
(mpg - predict(lm.fit2, Auto))[-train]

```



```{r}
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
```

## Leave-One-Out-Cross-Validation

```{r}
# Compute the test error for the linear model with using leave-one-out-cross-validation.
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.error <- cv.glm(Auto, glm.fit)
cv.error$delta
# The test error of the linear model is 24.23.
```

```{r}
set.seed(17)

loo_cross_validation <- function() {
  cv.error <- rep(0, 10)
  for (i in 1:10){
    glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
    cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
  }
  print(cv.error)
}

microbenchmark::microbenchmark(loo_cross_validation(), times = 1, unit = "milliseconds")
```

## k-Fold Cross Validation

```{r}
# This is at least an order of magnitude faster than loocv
set.seed(17)

k_fold_cross_validation <- function() {
  cv.error.10 <- rep(0, 10)
  for (i in 1:10){
    glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
    cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
  }
  print(cv.error.10)
}

microbenchmark::microbenchmark(k_fold_cross_validation(), times = 1, unit = "milliseconds")

```

## The Bootstrap

```{r}
# Creating an alpha function
alpha.fn <- function(data, index) {
  X <- data$X[index]
  Y <- data$Y[index]
  (var(Y) - cov(X,Y)) / (var(X) + var(Y) - 2*cov(X,Y))
}
```


```{r}
alpha.fn(Portfolio, 1:100)
```

```{r}
# Creating a new bootstrap dataset and computing alpha 100 times from the dataset. "sample" is the bootstrap dataset.
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = TRUE))
```

```{r}
# Creating 1000 bootstrap datatsets, computing alpha for each, and computing the standard deviation of the alphas. Alpha in this example is the investment that will minimize the risk of investing in two companies X & Y with returns that resemble those in the Portfolio dataset where the function that minimizes that risk is expressed as the Var(αX + (1 - α)Y). 
boot(Portfolio, alpha.fn, R = 1000)
```

## Estimating the Accuracy of a Linear Regression Model

```{r}
boot.fn <- function(data, index) {
  coef(lm(mpg ~ horsepower, data = data, subset = index))
}

boot.fn(Auto, 1:392)
```


```{r}
# Creating a single bootstrapped dataset
set.seed(42) 
boot.fn(Auto, sample(392, 392, replace = TRUE))

```

```{r}
# Bootstrapping the dataset 1000 times. Observing the standard error and coefficients. 
boot(Auto, boot.fn, R = 1000)
```

```{r}
summary(lm(mpg ~ horsepower, data = Auto))$coef
```

