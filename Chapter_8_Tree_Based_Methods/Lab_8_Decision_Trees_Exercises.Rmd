---
title: "Lab 8 Decision Trees Exercises"
author: "Evan Woods"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
  comment = ""
)
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression
if(!require("splines")) install.packages("splines")
if(!require("gam")) install.packages("gam")
if(!require("akima")) install.packages("akima")
if(!require("tree")) install.packages("tree") # Classification and Regression Trees
if(!require("randomForest")) install.packages("randomForest")
if(!require("gbm")) install.packages("gbm") # Boosted Trees
if(!require("BART")) install.packages("BART")
if(!require("reticulate")) install.packages("reticulate") # Use python objects in R

library(reticulate)
library(BART)
library(gbm)
library(randomForest)
library(tree)
library(akima)
library(gam)
library(splines)
library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```

```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

```{r}
# Input:
# response: This is a response variable. Example input is py$y_train
# predictors: These are the predictors. Example input is py$X. There are expected to be 10 predictors with 5 pairs of predictors corresponding to power spectral density bands with respect to 5 different frequencies.
# Output: A tibble containing a response (sleep state) and the predictors (power spectral density bands)
create_tibble_from_eeg_power_band <- function(response, predictor){
    sleep_per_psd_band <- tibble(sleep_stage = response[1], 
                    delta1 = predictor[1,1], 
                    delta2 = predictor[1,2], 
                    theta1 = predictor[1,3], 
                    theta2 = predictor[1,4], 
                    alpha1 = predictor[1,5], 
                    alpha2 = predictor[1,6], 
                    sigma1 = predictor[1,7], 
                    sigma2 = predictor[1,8], 
                    beta1 = predictor[1,9], 
                    beta2 = predictor[1,10])

    for(i in seq(2, length(response))){
        sleep_per_psd_band <- add_row(sleep_per_psd_band, tibble(
            sleep_stage = response[i], 
            delta1 = predictor[i,1], 
            delta2 = predictor[i,2], 
            theta1 = predictor[i,3], 
            theta2 = predictor[i,4], 
            alpha1 = predictor[i,5], 
            alpha2 = predictor[i,6], 
            sigma1 = predictor[i,7], 
            sigma2 = predictor[i,8], 
            beta1 = predictor[i,9], 
            beta2 = predictor[i,10]))
    }
    return(sleep_per_psd_band)
}
```

```{r message=FALSE}
attach(Boston)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
```

```{r}
# Use the mne conda environment for access to mne data
# use_python("/usr/local/bin/python")
use_condaenv("mne")
```


## Applied

### Question 7: 
In the lab, we applied random forests to the Boston data using mtry =
6 and using ntree = 25 and ntree = 500. Create a plot displaying the
test error resulting from random forests on this data set for a more
comprehensive range of values for mtry and ntree. You can model
your plot after Figure 8.10. Describe the results obtained.

```{r}
boston.test <- Boston[-train, "medv"]

df <- tibble(rf_half_mse = double(500), rf_sqrt_mse = double(500), bag_mse = double(500))

for (i in seq(1,500)) {
  bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, ntree = i)
  yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
  df[i, 'bag_mse'] <- mse.bag <- mean((yhat.bag - boston.test)^2)

  rf.boston_half <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, ntree = i)
  yhat.rf_half <- predict(rf.boston_half, newdata = Boston[-train, ])
  df[i, 'rf_half_mse'] <- mse.rf_half <- mean((yhat.rf_half - boston.test)^2)

  rf.boston_sqrt <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 3, ntree = i)
  yhat.rf_sqrt <- predict(rf.boston_sqrt, newdata = Boston[-train, ])
  df[i, 'rf_sqrt_mse'] <- mse.rf_sqrt <- mean((yhat.rf_sqrt - boston.test)^2)
}
```

```{r}
ggplot(df) +
  geom_line(aes(seq(1, 500), rf_half_mse, color = "m = p/2")) +
  geom_line(aes(seq(1, 500), rf_sqrt_mse, color = "m = âˆšp")) +
  geom_line(aes(seq(1, 500), bag_mse, color = "m = p")) +
  theme_linedraw() +
  labs(title = "Test MSE of Median Value of Homes",
       subtitle = "Modeling Methods: Random Forest & Bagging",
       x = "Number of Trees",
       y = "Test Mean Squared Error", colour = "# of Predictors")
f_print(sprintf("Of the three modeling methods, the bagged model had the highest mean squared error whereas the random forest model with the number of predictors equal to the square root of the total number of predictors had the lowest test mean squared error. This is owing to the fact that choosing a lower number of predictors per split of each internal node allows for a greater variety between trees. This leads to a decorrelation of the predictions between trees which reduces model variance and lowers the test mean squared error."))
```

### Question 8:
In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.

* **Question 8-a**: Split the data into a training set and test set.
  * **Answer**:
```{r message=FALSE}
attach(Carseats)
train <- sample(nrow(Carseats)*.8)
```


* **Question 8-b**: Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?
  * **Answer**: 
```{r}
tree.fit <- tree(Sales ~ ., data = Carseats, subset = train)
tree.pred <- predict(tree.fit, newdata = Carseats[-train, ])
Carseats.test_true <- Carseats[-train, "Sales"]
f_print(sprintf("The test MSE of the regression tree to predict carseat sales is: %0.3f.", mean((tree.pred - Carseats.test_true)^2)))
plot(tree.fit)
text(tree.fit, pretty = 0)
```

* **Question 8-c**: Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?
  * **Answer**: 
```{r}
set.seed(42)
cv.carseats <- cv.tree(tree.fit)
plot(cv.carseats, type = "b")
points(which.min(cv.carseats$size), cv.carseats$dev[which.min(cv.carseats$dev)], col = "red", cex = 2, pch = 20)
f_print(sprintf("The optimal level of tree complexity is: %0.0f with a deviation of %0.3f.", which.min(cv.carseats$size), cv.carseats$dev[which.min(cv.carseats$dev)]))
cat("\n\n")

tree.pred <- predict(tree.fit, Carseats[-train, ])
f_print(sprintf("The test MSE of the full tree is: %0.3f.", mean((tree.pred - Carseats[-train, "Sales"])^2)))
cat("\n\n")
prune.carseats_5 <- prune.tree(tree.fit, best = 5)
prune.carseats_5_pred <- predict(prune.carseats_5, newdata = Carseats[-train, ])
f_print(sprintf("The test MSE of the pruned tree is: %0.3f.", mean((prune.carseats_5_pred - Carseats[-train, "Sales"])^2)))
f_print(sprintf("Pruning the tree does not reduce the test mean squared error because the optimal level of complexity of the tree is greater than the pruned tree complexity. "))
```

* **Question 8-d**: Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.
  * **Answer**: 
```{r warning=FALSE}
bag.carseats <- randomForest(Sales ~ ., data = Carseats, subset = train, mtry = 11, importance = TRUE)
bag.pred <- predict(bag.carseats, newdata = Carseats[-train, ])
f_print(sprintf("The test MSE of the bagged model of carseat sales is: %0.3f.", mean((bag.pred - Carseats[-train, "Sales"])^2)))
cat("\n\n")
importance(bag.carseats)
```
* **Question 8-e**: Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of _m_, the number of variables considered at each split, on the error rate obtained. 
  * **Answer**: 
```{r}
rf.carseats_5 <- randomForest(Sales ~ ., data = Carseats, subset = train, mtry = 5, importance = TRUE)
rf.pred_5 <- predict(rf.carseats_5, newdata = Carseats[-train, ])
f_print(sprintf("The test MSE of the bagged model of carseat sales is: %0.3f.", mean((rf.pred_5 - Carseats[-train, "Sales"])^2)))
cat("\n\n")
importance(rf.carseats_5)
f_print(sprintf("Reducing the number of of variables considered at each split by half increased the test MSE by 0.03."))
```

* **Question 8-f**: Now analyze the data using BART, and report your results.
  * **Answer**:
```{r}
x <- Carseats[,2:length(Carseats)]
y <- Carseats[, "Sales"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bart.fit <- gbart(xtrain, ytrain, x.test = xtest)
f_print(sprintf("The test MSE of the BART model is: %0.3f.", mean((ytest - bart.fit$yhat.test.mean)^2)))

```

### Question 9: This problem involves the OJ data set which is part of the ISLR2 package.
* **Question 9-a**: Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.
  * **Answer**:
```{r}
train <- sample(nrow(OJ)*.8)
test <- (-train)
```

* **Question 9-b**: Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate. How many terminal nodes does the tree have?
  * **Answer**:
```{r}
tree.fit <- tree(Purchase ~ ., data = OJ, subset = train)
summary(tree.fit)
f_print(sprintf("The residual mean deviance is 0.7183. This indicates a poor fit to the training data of the resulting tree. The tree has 7 terminal nodes."))
```
* **Question 9-c**: Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed. 
  * **Answer**: 
```{r}
tree.fit
cat("\n\n")
f_print(sprintf("Terminal node 9 represents purchases orange juice where Citris Hill Orange juice was on special and the loyalty of the customer to Citris Hill orange juice is between 0.276142 and 0.035047. There are 112 observations at this node. The deviance is 105.10. The dominant prediction is Minute Maid Orange Juice. The mean probability that an observed purchase at this node is a purchase of Minute Maid orange juice is 0.82143. Conversely, the mean probability that an observed purchase was a purchase of Citris Hill orange juice given that Citris Hill orange juice was on special and the customer loyalty to Citris Hill orange juice is between 0.276142 and 0.035047 was 0.17857."))
```
* **Question 9-d**:
  * **Answer**:
```{r}
plot(tree.fit)
text(tree.fit, pretty = 0)
f_print(sprintf("The plot shows purchases of orange juice given various levels of customer loyalty to Citris Hill and price differences between Citris Hill and Minute Maid orange juice."))
```

* **Question 9-e**: Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?
  * **Answer**:
```{r}
tree.pred <- predict(tree.fit, newdata = OJ[-train, ], type = "class")
tree.truth <- OJ[-train, "Purchase"]
table(tree.pred, tree.truth)
cat("\n")
f_print(sprintf("The test MSE is %0.9f.",mean((as.integer(tree.pred) - as.integer(tree.truth))^2)))
```
* **Question 9-f**: Apply the cv.tree() function to the training set in order to determine the optimal tree size.
  * **Answer**:
```{r}
set.seed(42)
tree.cv_fit <- cv.tree(tree.fit, FUN = prune.tree)
f_print(sprintf("The optimal tree size is: %0.0f.", tree.cv_fit$size[which.min(tree.cv_fit$dev)]))
```

* **Question 9-g**: Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.
  * **Answer**:
```{r}
plot(tree.cv_fit, type = "b")
```

* **Question 9-h**: Which tree size corresponds to the lowest cross-validated classification error rate?
  * **Answer**:
```{r}
f_print(sprintf("The lowest cross-validated classification error rate is of tree size: %0.0f.", tree.cv_fit$size[which.min(tree.cv_fit$dev)]))
```

* **Question 9-i**: Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes. 
  * **Answer**:
```{r}
pruned.tree <- prune.tree(tree.fit, best = tree.cv_fit$size[which.min(tree.cv_fit$dev)])
```

* **Question 9-j**: Compare the training error rates between the pruned and unpruned trees. Which is higher?
  * **Answer**:
```{r}
tree.pred <- predict(tree.fit, newdata = OJ[train, ], type = "class")
tree.truth <- OJ[train, "Purchase"]


pruned.pred <- predict(pruned.tree, newdata = OJ[train, ], type = "class")
f_print(sprintf("The train MSE of the pruned & unpruned trees are identical. The best %0.0f terminal nodes was chosen for the pruned tree.",tree.cv_fit$size[which.min(tree.cv_fit$dev)]))
cat("\n")
f_print(sprintf("The train MSE of the pruned tree is: %0.9f.", mean((na.omit(as.integer(pruned.pred) - as.integer(tree.truth))^2))))
cat("\n")
f_print(sprintf("The train MSE of the unpruned tree is: %0.9f.", mean(na.omit((as.integer(tree.pred) - as.integer(tree.truth)) ^2))))
```
* **Question 9-k**: Compare the test error rate between the pruned and unpruned trees. Which is higher?
  * **Answer**:
```{r}
tree.pred <- predict(tree.fit, newdata = OJ[-train, ], type = "class")
tree.truth <- OJ[train, "Purchase"]


pruned.pred <- predict(pruned.tree, newdata = OJ[-train, ], type = "class")
f_print(sprintf("The test MSE of the pruned & unpruned trees are identical. The best 5 terminal nodes was chosen for the pruned tree, where the unpruned tree optimally contains 7 terminal nodes."))
cat("\n")
f_print(sprintf("The test MSE of the pruned tree is: %0.9f.", mean((as.integer(pruned.pred) - as.integer(tree.truth))^2)))
cat("\n")
f_print(sprintf("The test MSE of the unpruned tree is: %0.9f.",mean((as.integer(tree.pred) - as.integer(tree.truth))^2)))
```

### Question 10: 
We now use boosting to predict Salary in the Hitters dataset.

* **Question 10-a**: Remove the observations for whom the salary information is unknown, and then log-transform the salaries.
  * **Answer**:
```{r message=FALSE}
hitters <- na.omit(Hitters)
hitters_transformed <- hitters %>% select(Salary, everything()) %>% mutate(Salary = log(Salary))
attach(hitters_transformed)
```

* **Question 10-b**: Create a training set consisting of 200 observations, and a test set consisting of the remaining observations.
  * **Answer**:
```{r}
train <- sample(nrow(hitters_transformed), 200)
hitters_transformed.test <- hitters_transformed[-train, ]
```

* **Question 10-c**: Perform boosting on the training set with 1000 trees for a range of values of the shrinkage parameter Î». Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.
  * **Answer**:
```{r message=FALSE}
set.seed(42)
Î»_val <- seq(.001, .1, by = 0.001)
index <- 1
train_mse <- c(integer(100))
for (Î» in seq(.001, .1, by = .001)){
  index <- index + 1
  boost.model <- gbm(Salary ~ ., data = hitters_transformed[train, ], distribution = "gaussian", n.trees = 1000, interaction.depth = 4, shrinkage = Î»)
  boost.pred <- predict(boost.model, hitters_transformed[train, ])
  boost.truth <- hitters_transformed[train, "Salary"]
  train_mse[index] <- mean((boost.pred - boost.truth)^2)
}
```

```{r}
ggplot() +
  geom_point(aes(Î»_val, train_mse[1:100])) +
  labs(title = "Train MSE of Prediction of the Log of Salary for Various Values of Î»", x = "Value of Î»", y = "Training MSE")
```


* **Question 10-d**: Produce a plot with different shrinkage values on the x-axis and the corresponding test MSE on the y-axis.
  * **Answer**:

```{r message=FALSE}
set.seed(42)
Î»_val <- seq(.001, .1, by = 0.001)
index <- 1
test_mse <- c(integer(100))
for (Î» in seq(.001, .1, by = .001)){
  index <- index + 1
  boost.model <- gbm(Salary ~ ., data = hitters_transformed[train, ], distribution = "gaussian", n.trees = 1000, interaction.depth = 4, shrinkage = Î»)
  boost.pred <- predict(boost.model, hitters_transformed[-train, ])
  boost.truth <- hitters_transformed[-train, "Salary"]
  test_mse[index] <- mean((boost.pred - boost.truth)^2)
}
```

```{r}
ggplot() + 
  geom_point(aes(Î»_val, test_mse[1:100])) + 
  labs(title = "Test MSE of Prediction of the Log of Salary for Various Values of Î»", x = "Value of Î»", y = "Test MSE")
```

* **Question 10-e**: Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6.
  * **Answer**:
```{r}
# gbm(Salary ~ ., data = hitters_transformed[train, ], distribution = "gaussian", n.trees = 1000, interaction.depth = 4, shrinkage = Î»)
lm.fit <- lm(Salary ~., data = hitters_transformed, subset = train)
lm.pred <- predict(lm.fit, hitters_transformed[-train, ])
lm.mse <- mean((lm.pred - hitters_transformed[-train, "Salary"])^2)
```

```{r}
grid <- 10^seq(10, -2, length = 100)
x <- model.matrix(Salary ~ ., hitters_transformed)[, -1]
y <- hitters_transformed$Salary
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[-train, ])
ridge.mse <- mean((ridge.pred - hitters_transformed[-train, "Salary"])^2)
```
  
```{r}
f_print(sprintf("The mean test MSE of the boosted tree is: %0.3f", mean(test_mse)))
cat("\n\n")
f_print(sprintf("The test MSE of the linear model is: %0.3f", lm.mse))
cat("\n\n")
f_print(sprintf("The test MSE of the ridge regression is: %0.3f", ridge.mse))
cat("\n\n")
f_print(sprintf("The test MSE of the boosted model is the lowest between the three models. The ridge regression obtained the largest test MSE whereas the linear model's test MSE was middling."))
```
* **Question 10-f**: Which variables appear to be the most important predictors in
the boosted model?
  * **Answer**:
```{r}
summary(boost.model)
f_print(sprintf("The most important predictors in the boosted model are: CRBI, RBI, & Division."))
```
* **Question 10-g**: Now apply bagging to the training set. What is the test set MSE for this approach?
  * **Answer**:
```{r}
bag.fit <- randomForest(Salary ~., data = hitters_transformed, subset = train, mtry = 19, n.tree = 500)
bag.pred <- predict(bag.fit, hitters_transformed[-train, ])
bag.mse <- mean((bag.pred - hitters_transformed[-train, "Salary"])^2)
f_print(sprintf("The test MSE for the bagged approach is: %0.3f. This is the lowest testing error between the 4 approaches: bagging, linear model, ridge regression, & boosting.", bag.mse))
```

### Question 11:
This question uses the Caravan dataset.

* **Question 11-a**: Create a training set consisting of the first 1000 observations, and a test set of the remaining observations.
  * **Answer**:

```{r}
set.seed(42)
train <- sample(nrow(Caravan), 1000)
test <- Caravan[-train, ]
```
* **Question 11-b**: Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?
  * **Answer**:

```{r}
# Refactoring purchase as an integer.
caravan_formatted <- Caravan %>% mutate("Purchase_int" = as.integer(Purchase))
caravan_formatted$Purchase_int[caravan_formatted$Purchase_int == 1] <- 0
caravan_formatted$Purchase_int[caravan_formatted$Purchase_int == 2] <- 1
caravan_formatted <- caravan_formatted %>% select(everything(), -Purchase)
```

```{r}
# A binary classification problem. The distribution is "bernoulli".
set.seed(42)
boost.model <- gbm(Purchase_int ~ ., data = caravan_formatted[train, ], distribution = "bernoulli", n.trees = 1000, shrinkage = 0.01)
summary(boost.model)
f_print(sprintf("MSKC & ALEVEN appear to be the most important predictors."))
```
* **Question 11-c**: Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated probability of purchase is greater than 20%. 
  * **Answer**:
```{r message=FALSE}
set.seed(42)
boost.pred <- predict(boost.model, newdata = caravan_formatted[-train, ], type = "response")
boost.prob <- rep(0, length(boost.pred))
boost.prob[boost.pred > 0.2] <- 1

# It is significant to note that there are 5822 observations of purchases. The instructions indicate to train the model using 1000 of those observations and use the remaining observations as test. This indicates that the majority of observations will be test observations resulting in poor model performance.
f_print(sprintf("Caravan Purchase Truth"))
table(boost.prob, caravan_formatted[-train, "Purchase_int"])
accuracy.boost <- ((54 + 4322) / (4322 + 54 + 224 + 222)) * 100
purchase_percent.boost <- ((54) / (4322 + 54 + 224 + 222)) * 100
mse.boost <- mean((boost.pred - as.integer(Caravan[-train, "Purchase"]))^2)
```
```{r warning=FALSE}
# Note: The training set is approximately four times smaller than the test set. This leads to inaccurate models. 
set.seed(42)
glm.fit <- glm(Purchase ~ ., data = Caravan, subset = train, family = "binomial")
glm.pred <- predict(glm.fit, data = Caravan[-train, ], type = "response")
glm.prob <- rep(0, length(Caravan[-train, "Purchase"]))
glm.prob[glm.pred > 0.2] <- 1
f_print(sprintf("Caravan Purchase Truth"))
table(glm.prob, Caravan[-train, "Purchase"])
accuracy.glm <- ((28+4085) / (28+4085+248+461)) * 100
purchase_percent.glm <- ((28) / (28+4085+248+461)) * 100
mse.glm <- mean((glm.pred - as.integer(Caravan[-train, "Purchase"]))^2)
```

```{r message=FALSE}
attach(Caravan)

train.X <- cbind(MOSTYPE, MAANTHUI,MGEMOMV, MGEMLEEF,MOSHOOFD,MGODRK, MGODPR, MGODOV, MGODGE, MRELGE, MRELSA, MRELOV, MFALLEEN, MFGEKIND, MFWEKIND, MOPLHOOG, MOPLMIDD, MOPLLAAG, MBERHOOG, MBERZELF, MBERBOER, MBERMIDD, MBERARBG, MBERARBO, MSKA, MSKB1, MSKB2,
MSKC, MSKD, MHHUUR, MHKOOP, MAUT1, MAUT2, MAUT0, MZFONDS, MZPART, MINKM30, MINK3045, MINK4575, MINK7512, MINK123M, MINKGEM, MKOOPKLA, PWAPART, PWABEDR, PWALAND, PPERSAUT, PBESAUT, PMOTSCO, PVRAAUT, PAANHANG, PTRACTOR, PWERKT, PBROM,
PLEVEN, PPERSONG,PGEZONG, PWAOREG, PBRAND, PZEILPL, PPLEZIER, PFIETS, PINBOED, PBYSTAND, AWAPART, AWABEDR, AWALAND, APERSAUT, ABESAUT, AMOTSCO, AVRAAUT, AAANHANG, ATRACTOR, AWERKT, ABROM, ALEVEN, APERSONG, AGEZONG, AWAOREG, ABRAND, AZEILPL,
APLEZIER, AFIETS, AINBOED, ABYSTAND)[train, ]

test.X <- cbind(MOSTYPE, MAANTHUI,MGEMOMV, MGEMLEEF,MOSHOOFD,MGODRK, MGODPR, MGODOV, MGODGE, MRELGE, MRELSA, MRELOV, MFALLEEN, MFGEKIND, MFWEKIND, MOPLHOOG, MOPLMIDD, MOPLLAAG, MBERHOOG, MBERZELF, MBERBOER, MBERMIDD, MBERARBG, MBERARBO, MSKA, MSKB1, MSKB2,
MSKC, MSKD, MHHUUR, MHKOOP, MAUT1, MAUT2, MAUT0, MZFONDS, MZPART, MINKM30, MINK3045, MINK4575, MINK7512, MINK123M, MINKGEM, MKOOPKLA, PWAPART, PWABEDR, PWALAND, PPERSAUT, PBESAUT, PMOTSCO, PVRAAUT, PAANHANG, PTRACTOR, PWERKT, PBROM,
PLEVEN, PPERSONG,PGEZONG, PWAOREG, PBRAND, PZEILPL, PPLEZIER, PFIETS, PINBOED, PBYSTAND, AWAPART, AWABEDR, AWALAND, APERSAUT, ABESAUT, AMOTSCO, AVRAAUT, AAANHANG, ATRACTOR, AWERKT, ABROM, ALEVEN, APERSONG, AGEZONG, AWAOREG, ABRAND, AZEILPL,
APLEZIER, AFIETS, AINBOED, ABYSTAND)[-train, ]

train.Purchase <- Caravan$Purchase[train] # Labels for the training observations.

set.seed(42)
knn.pred <- knn(train.X, test.X, train.Purchase, k = 1)
f_print(sprintf("Caravan Purchase Truth"))
table(knn.pred, caravan_formatted$Purchase_int[-train])
accuracy.knn <- (35 + 4145) / (35+4145+241+401) * 100
purchase_percent.knn <- (35) / (35+4145+241+401) * 100
```


```{r}
f_print(sprintf("Using the boosted model, 28 (%0.3f%%) people predicted to make a purchase actually make one.", purchase_percent.boost))
cat("\n\n")
f_print(sprintf("Using the logistic regression, 28 (%0.3f%%) people predicted to make a purchase actually make one.", purchase_percent.glm))
cat("\n\n")
f_print(sprintf("Using the knn method, 28 (%0.3f%%) people predicted to make a purchase actually make one.", purchase_percent.knn))
cat("\n\n")

f_print(sprintf("The accuracies of the boosted model, the logistic regression, and the knn method are as follows: %0.3f%%, %0.3f%%, & %0.3f%%.", accuracy.boost, accuracy.glm, accuracy.knn))
cat("\n\n")
f_print(sprintf("The boosted model maintains the highest accuracy of the three procedures."))
```

### Question 12:
Apply boosting, bagging, random forests, and BART to a data set of your choice. Be sure to fit the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like linear or logistic regression? Which of these approaches yields the best performance?

```{python}
# Importing Python Libraries
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import FunctionTransformer

import mne
from mne.datasets.sleep_physionet.age import fetch_data
```

```{python message=FALSE}
# Fetching data
ALICE, BOB = 0, 1

[alice_files, bob_files] = fetch_data(subjects=[ALICE, BOB], recording=[1])

raw_train = mne.io.read_raw_edf(
    alice_files[0], stim_channel="Event marker", infer_types=True, preload=True
)
annot_train = mne.read_annotations(alice_files[1])

raw_train.set_annotations(annot_train, emit_warning=False)

# plot some data
# scalings were chosen manually to allow for simultaneous visualization of
# different channel types in this specific dataset
# raw_train.plot(
#     start=60,
#     duration=60,
#     scalings=dict(eeg=1e-4, resp=1e3, eog=1e-4, emg=1e-7, misc=1e-1),
#     
# )
```

```{python}
# Associating raw data with annotations of sleep events
annotation_desc_2_event_id = {
    "Sleep stage W": 1,
    "Sleep stage 1": 2,
    "Sleep stage 2": 3,
    "Sleep stage 3": 4,
    "Sleep stage 4": 4,
    "Sleep stage R": 5,
}

# keep last 30-min wake events before sleep and first 30-min wake events after
# sleep and redefine annotations on raw data
annot_train.crop(annot_train[1]["onset"] - 30 * 60, annot_train[-2]["onset"] + 30 * 60)
raw_train.set_annotations(annot_train, emit_warning=False)

events_train, _ = mne.events_from_annotations(
    raw_train, event_id=annotation_desc_2_event_id, chunk_duration=30.0
)

# create a new event_id that unifies stages 3 and 4
event_id = {
    "Sleep stage W": 1,
    "Sleep stage 1": 2,
    "Sleep stage 2": 3,
    "Sleep stage 3/4": 4,
    "Sleep stage R": 5,
}

# plot events
fig = mne.viz.plot_events(
    events_train,
    event_id=event_id,
    sfreq=raw_train.info["sfreq"],
    first_samp=events_train[0, 0],
)

# keep the color-code for further plotting
stage_colors = plt.rcParams["axes.prop_cycle"].by_key()["color"]
```

```{python}
# Creating Epoch Objects of individual 1 for training data
tmax = 30.0 - 1.0 / raw_train.info["sfreq"]  # tmax in included

epochs_train = mne.Epochs(
    raw=raw_train,
    events=events_train,
    event_id=event_id,
    tmin=0.0,
    tmax=tmax,
    baseline=None,
)
del raw_train

print(epochs_train)
```

```{python message=FALSE}
# Creating Epoch Objects of individual 2 for test data
raw_test = mne.io.read_raw_edf(
    bob_files[0], stim_channel="Event marker", infer_types=True, preload=True
)
annot_test = mne.read_annotations(bob_files[1])
annot_test.crop(annot_test[1]["onset"] - 30 * 60, annot_test[-2]["onset"] + 30 * 60)
raw_test.set_annotations(annot_test, emit_warning=False)
events_test, _ = mne.events_from_annotations(
    raw_test, event_id=annotation_desc_2_event_id, chunk_duration=30.0
)
epochs_test = mne.Epochs(
    raw=raw_test,
    events=events_test,
    event_id=event_id,
    tmin=0.0,
    tmax=tmax,
    baseline=None,
)
del raw_test

print(epochs_test)
```

```{python message=FALSE}
# visualize Alice vs. Bob PSD by sleep stage.
fig, (ax1, ax2) = plt.subplots(ncols=2)

# iterate over the subjects
stages = sorted(event_id.keys())
for ax, title, epochs in zip([ax1, ax2], ["Alice", "Bob"], [epochs_train, epochs_test]):
    for stage, color in zip(stages, stage_colors):
        spectrum = epochs[stage].compute_psd(fmin=0.1, fmax=20.0)
        spectrum.plot(
            ci=None,
            color=color,
            axes=ax,
            show=False,
            average=True,
            spatial_colors=False,
            picks="data",
            exclude="bads",
        )
    ax.set(title=title, xlabel="Frequency (Hz)")
ax1.set(ylabel="ÂµVÂ²/Hz (dB)")

ax2.legend(ax2.lines[2::3], stages)
```

```{python}
# Extracting Power Spectral Density Bands
def eeg_power_band(epochs):
    """EEG relative power band feature extraction.

    This function takes an ``mne.Epochs`` object and creates EEG features based
    on relative power in specific frequency bands that are compatible with
    scikit-learn.

    Parameters
    ----------
    epochs : Epochs
        The data.

    Returns
    -------
    X : numpy array of shape [n_samples, 5 * n_channels]
        Transformed data.
    """
    # specific frequency bands
    FREQ_BANDS = {
        "delta": [0.5, 4.5],
        "theta": [4.5, 8.5],
        "alpha": [8.5, 11.5],
        "sigma": [11.5, 15.5],
        "beta": [15.5, 30],
    }

    spectrum = epochs.compute_psd(picks="eeg", fmin=0.5, fmax=30.0)
    psds, freqs = spectrum.get_data(return_freqs=True)
    # Normalize the PSDs
    psds /= np.sum(psds, axis=-1, keepdims=True)

    X = []
    for fmin, fmax in FREQ_BANDS.values():
        psds_band = psds[:, :, (freqs >= fmin) & (freqs < fmax)].mean(axis=-1)
        X.append(psds_band.reshape(len(psds), -1))

    return np.concatenate(X, axis=1)
```

```{python}
# Fitting a Random Forest Classifier to the data. Making a prediction of the sleep state of a subject based on the sleep state of another subject
pipe = make_pipeline(FunctionTransformer(eeg_power_band, validate=False), 
                     RandomForestClassifier(n_estimators=100, random_state=42))

# Train
y_train = epochs_train.events[:, 2]
pipe.fit(epochs_train, y_train)

# Test
y_pred = pipe.predict(epochs_test)

# Assess the results
y_test = epochs_test.events[:, 2]
acc = accuracy_score(y_test, y_pred)

print("Accuracy score: {}".format(acc))
```
```{python}
print(confusion_matrix(y_test, y_pred))
```

```{python}
print(classification_report(y_test, y_pred, target_names=event_id.keys()))
```

```{r}
rf_pipeline.mse <- mean((py$y_test - py$y_pred)^2)
# rf_pipeline.mse
```


```{python}
# Extracting data to create individual models. 
x_train = eeg_power_band(epochs_train)
y_train = epochs_train.events[:, 2]

x_test = eeg_power_band(epochs_test)
y_test = epochs_train.events[:, 2]
```

```{r}
train <- create_tibble_from_eeg_power_band(py$y_train, py$x_train)
test <- create_tibble_from_eeg_power_band(py$y_test, py$x_test)
```

```{r}
# Creating a Linear Model
lm.fit <- lm(sleep_stage ~ ., data = train)
summary(lm.fit)
lm.pred <- predict(lm.fit, test)
lm_pred <- tibble(lm.pred)
ans <- (lm_pred - train[, "sleep_stage"])^2
lm.mse <- sum(ans)/nrow(ans)
```

```{r}
# Creating data to plot true values of sleep stage vs predictions
# y_train <- train[, "sleep_stage"]
# predictions <- lm_pred$lm.pred
# n_observations <- c(seq(1,841))

# Plotting data 
# ggplot() +
  # geom_point(aes(n_observations, y_train$sleep_stage), color = y_train$sleep_stage) +
  # geom_point(aes(n_observations, floor(predictions)), color = y_train$sleep_stage) +
  # geom_point(aes(n_observations, predictions), color = predictions) +
  # labs(title = "Predicted Sleep Stage vs. Actual", y = "Sleep Stage", x = "Sample Observation")
```


```{r}
bag.fit <- randomForest(sleep_stage ~., data = train, mtry = 10, n.tree = 500)
bag.pred <- predict(bag.fit, test)
bag.mse <- mean((bag.pred - test$sleep_stage)^2)
# f_print(sprintf("The test MSE for the bagged approach is: %0.3f.", bag.mse))
```

```{r}
set.seed(42)
rf.fit <- randomForest(sleep_stage ~., data = train, mtry = 5, n.tree = 500)
rf.pred <- predict(rf.fit, test)
rf.mse <- mean((rf.pred - test$sleep_stage)^2)
# rf.mse
```

```{r}
set.seed(42)
boost.model <- gbm(sleep_stage ~ ., data = train, distribution = "gaussian", n.trees = 1000, shrinkage = 0.01)
boost.pred <- predict(boost.model, newdata = test)
mse.boost <- mean((boost.pred - train$sleep_stage)^2)
# mse.boost
```

```{r}
train <- create_tibble_from_eeg_power_band(py$y_train, py$x_train)
test <- create_tibble_from_eeg_power_band(py$y_test, py$x_test)
```

```{r}
set.seed(42)

typeof(train[,2:length(train)])
xtrain_sleep <- train %>% select(everything(), -sleep_stage)

ytrain_sleep <- train %>% select(sleep_stage)

xtest_sleep <- test %>% select(everything(), -sleep_stage)
ytest_sleep <- test %>% select(sleep_stage)
bart.fit <- gbart(x.train = data.matrix(xtrain_sleep), y.train = data.matrix(ytrain_sleep), x.test = data.matrix(xtest_sleep))
f_print(sprintf("The test MSE of the BART model is: %0.3f.", mean((ytest - bart.fit$yhat.test.mean)^2)))
```

```{r}
f_print(sprintf("The test MSE of the randomforest with 500 trees is: %0.3f.", rf.mse))
cat("\n\n")
f_print(sprintf("The test MSE of the boosted model is: %0.3f.", mse.boost))
cat("\n\n")
f_print(sprintf("The test MSE of the bagged model is: %0.3f.", bag.mse))
cat("\n\n")
f_print(sprintf("The test MSE of the linear model is: %0.3f.", lm.mse))
cat("\n\n")
f_print(sprintf("The test MSE of the random forest created from 100 estimators is: %0.3f.", rf_pipeline.mse))
cat("\n\n")
f_print(sprintf("The test MSE of the BART model is: %0.3f.", mean((ytest - bart.fit$yhat.test.mean)^2)))
```
