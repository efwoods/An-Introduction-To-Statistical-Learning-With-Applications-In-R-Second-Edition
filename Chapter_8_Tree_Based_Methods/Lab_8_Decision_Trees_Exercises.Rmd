---
title: "Lab 8 Decision Trees Exercises"
author: "Evan Woods"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
  comment = ""
)
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression
if(!require("splines")) install.packages("splines")
if(!require("gam")) install.packages("gam")
if(!require("akima")) install.packages("akima")
if(!require("tree")) install.packages("tree") # Classification and Regression Trees
if(!require("randomForest")) install.packages("randomForest")
if(!require("gbm")) install.packages("gbm") # Boosted Trees
if(!require("BART")) install.packages("BART")

library(BART)
library(gbm)
library(randomForest)
library(tree)
library(akima)
library(gam)
library(splines)
library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```

```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

```{r message=FALSE}
attach(Boston)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
```

## Applied

### Question 7: 
In the lab, we applied random forests to the Boston data using mtry =
6 and using ntree = 25 and ntree = 500. Create a plot displaying the
test error resulting from random forests on this data set for a more
comprehensive range of values for mtry and ntree. You can model
your plot after Figure 8.10. Describe the results obtained.

```{r}
boston.test <- Boston[-train, "medv"]

df <- tibble(rf_half_mse = double(500), rf_sqrt_mse = double(500), bag_mse = double(500))

for (i in seq(1,500)) {
  bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, ntree = i)
  yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
  df[i, 'bag_mse'] <- mse.bag <- mean((yhat.bag - boston.test)^2)
  
  rf.boston_half <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, ntree = i)
  yhat.rf_half <- predict(rf.boston_half, newdata = Boston[-train, ])
  df[i, 'rf_half_mse'] <- mse.rf_half <- mean((yhat.rf_half - boston.test)^2)
  
  rf.boston_sqrt <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 3, ntree = i)
  yhat.rf_sqrt <- predict(rf.boston_sqrt, newdata = Boston[-train, ])
  df[i, 'rf_sqrt_mse'] <- mse.rf_sqrt <- mean((yhat.rf_sqrt - boston.test)^2)
}
```

```{r}
ggplot(df) + 
  geom_line(aes(seq(1, 500), rf_half_mse, color = "m = p/2")) + 
  geom_line(aes(seq(1, 500), rf_sqrt_mse, color = "m = âˆšp")) + 
  geom_line(aes(seq(1, 500), bag_mse, color = "m = p")) + 
  theme_linedraw() + 
  labs(title = "Test MSE of Median Value of Homes", 
       subtitle = "Modeling Methods: Random Forest & Bagging",
       x = "Number of Trees", 
       y = "Test Mean Squared Error", colour = "# of Predictors")
f_print(sprintf("Of the three modeling methods, the bagged model had the highest mean squared error whereas the random forest model with the number of predictors equal to the square root of the total number of predictors had the lowest test mean squared error. This is owing to the fact that choosing a lower number of predictors per split of each internal node allows for a greater variety between trees. This leads to a decorrelation of the predictions between trees which reduces model variance and lowers the test mean squared error."))
```

### Question 8:
In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.

* **Question 8-a**: Split the data into a training set and test set.
  * **Answer**:
```{r message=FALSE}
attach(Carseats)
train <- sample(nrow(Carseats)*.8)
```


* **Question 8-b**: Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?
  * **Answer**: 
```{r}
tree.fit <- tree(Sales ~ ., data = Carseats, subset = train)
tree.pred <- predict(tree.fit, newdata = Carseats[-train, ])
Carseats.test_true <- Carseats[-train, "Sales"]
f_print(sprintf("The test MSE of the regression tree to predict carseat sales is: %0.3f.", mean((tree.pred - Carseats.test_true)^2)))
plot(tree.fit)
text(tree.fit, pretty = 0)

```

* **Question 8-c**: Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?
  * **Answer**: 
```{r}
set.seed(42)
cv.carseats <- cv.tree(tree.fit)
plot(cv.carseats, type = "b")
points(which.min(cv.carseats$size), cv.carseats$dev[which.min(cv.carseats$dev)], col = "red", cex = 2, pch = 20)
f_print(sprintf("The optimal level of tree complexity is: %0.0f with a deviation of %0.3f.", which.min(cv.carseats$size), cv.carseats$dev[which.min(cv.carseats$dev)]))
cat("\n\n")

tree.pred <- predict(tree.fit, Carseats[-train, ])
f_print(sprintf("The test MSE of the full tree is: %0.3f.", mean((tree.pred - Carseats[-train, "Sales"])^2)))
cat("\n\n")
prune.carseats_5 <- prune.tree(tree.fit, best = 5)
prune.carseats_5_pred <- predict(cv.carseats_prune_5, newdata = Carseats[-train, ])
f_print(sprintf("The test MSE of the pruned tree is: %0.3f.", mean((prune.carseats_5_pred - Carseats[-train, "Sales"])^2)))
f_print(sprintf("Pruning the tree does not reduce the test mean squared error because the optimal level of complexity of the tree is greater than the pruned tree complexity. "))
```

* **Question 8-d**: Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.
  * **Answer**: 
```{r}
bag.carseats <- randomForest(Sales ~ ., data = Carseats, subset = train, mtry = 11, importance = TRUE)
bag.pred <- predict(bag.carseats, newdata = Carseats[-train, ])
f_print(sprintf("The test MSE of the bagged model of carseat sales is: %0.3f.", mean((bag.pred - Carseats[-train, "Sales"])^2)))
cat("\n\n")
importance(bag.carseats)
```
* **Question 8-e**: Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of _m_, the number of variables considered at each split, on the error rate obtained. 
  * **Answer**: 
```{r}
rf.carseats_5 <- randomForest(Sales ~ ., data = Carseats, subset = train, mtry = 5, importance = TRUE)
rf.pred_5 <- predict(rf.carseats_5, newdata = Carseats[-train, ])
f_print(sprintf("The test MSE of the bagged model of carseat sales is: %0.3f.", mean((rf.pred_5 - Carseats[-train, "Sales"])^2)))
cat("\n\n")
importance(rf.carseats_5)
f_print(sprintf("Reducing the number of of variables considered at each split by half increased the test MSE by 0.03."))
```

* **Question 8-f**: Now analyze the data using BART, and report your results.
  * **Answer**:
```{r}
x <- Carseats[,2:length(Carseats)]
y <- Carseats[, "Sales"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bart.fit <- gbart(xtrain, ytrain, x.test = xtest)
f_print(sprintf("The test MSE of the BART model is: %0.3f.", mean(ytest - bart.fit$yhat.test.mean)))
```


```{r}
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
yhat.bart <- bartfit$yhat.test.mean
f_print(sprintf("The test MSE of the BART model is: %0.3f.",mean((ytest - yhat.bart)^2)))
```


### Question 9: This problem involves the OJ data set which is part of the ISLR2 package.
* **Question 9-a**: Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.
  * **Answer**:
```{r}
train <- sample(nrow(OJ)*.8)
test <- (-train)
```

* **Question 9-b**: Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate. How many terminal nodes does the tree have?
  * **Answer**:
```{r}
tree.fit <- tree(Purchase ~ ., data = OJ, subset = train)
summary(tree.fit)
f_print(sprintf("The residual mean deviance is 0.7183. This indicates a poor fit to the training data of the resulting tree. The tree has 7 terminal nodes."))
```
* **Question 9-c**: Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed. 
  * **Answer**: 
```{r}
tree.fit
cat("\n\n")
f_print(sprintf("The terminal node 9 is LoyalCH < 0.035047. The mean probability of that a purchase of orange juice is of customer brand loyalty is less than 0.035047 is 0.17857, whereas the mean probability that the value is greater than 0.035047 is 0.82143."))
```

