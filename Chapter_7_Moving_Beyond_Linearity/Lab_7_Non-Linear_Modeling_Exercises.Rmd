---
title: "Lab 7 Non-Linear Modeling Exercises"
author: "Evan Woods"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
  comment = ""
)
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression
if(!require("splines")) install.packages("splines")
if(!require("gam")) install.packages("gam")
if(!require("akima")) install.packages("akima")

library(akima)
library(gam)
library(splines)
library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
```

```{r include=FALSE}
LoadLibraries <- function() {
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")

library(caret)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
  print("Libraries have been loaded!")
}
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```

```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

```{r}
# Input: Dataframe, x (a numerical that is converted to a factor), y (double), title, x-axis title, y-axis title
# Output: Transparent violin plots overlayed onto colored boxplots of y onto each x factor. Each boxplot is of a single x factor. 
create_custom_geom_box_grouped_by_x_factor <- function(df, x, y, title, x_title, y_title){
    ggplot(df, aes(as.factor(x), y)) +
      geom_boxplot(color = custom_darkblue, fill = custom_lightblue) +
      geom_violin(alpha = .1) + 
      theme_linedraw() +
      labs(title = title, x = x_title, y = y_title)  
}
```

```{r}
# Input: Dataframe, x, y, title of the plot, title of the x-axis, title of the y-axis
# Output: Scatterplot of x and y variables with correlation between the two on the grid & a smooth line of best fit to the data points. 
create_custom_geom_point <- function(df, x, y, title, x_title, y_title){
    correlation <- sprintf("r: %0.3f", cor(x,y))
    label <- df %>% summarise(x = max(x), y = max(y), label = correlation)
    ggplot(df, aes(x, y)) +
      geom_point(color = custom_darkblue) + 
      theme_linedraw() +
      geom_smooth(se = FALSE, color = custom_lightblue) +
      labs(title = title, x = x_title, y = y_title) + 
      geom_text(aes(label = label), data = label, vjust = "top", hjust = "right", color = "#a60808") 
}

```

```{r}
# Input: Dataframe, x, y, title of the plot, title of the x-axis, title of the y-axis
# Output: Scatterplot of x and y variables with correlation between the two on the grid & a smooth line of best fit to the data points. 
create_custom_geom_point_rss <- function(df, x, y, title, x_title, y_title){
    correlation <- sprintf("r: %0.3f", cor(x,y))
    label <- df %>% summarise(x = max(x), y = max(y), label = correlation)
    ggplot(df, aes(x, y)) +
      geom_point(color = custom_darkblue) + 
      theme_linedraw() +
      geom_smooth(se = FALSE, color = custom_lightblue) +
      labs(title = title, x = x_title, y = y_title) + 
      geom_text(aes(label = label), data = label, vjust = "top", hjust = "right", color = "#a60808") 
}

```


```{r}
# Input: 
    # df: dataframe containing x and y values to be plotted. 
    # plot_types: indicator (TRUE or NULL) that attributes of the the type of plot to create has been added to the dataframe before calling this function. All predictors must be included. The number of columns in the dataframe must equal the number of columns in the attribute that has been added. The name of the attribute must be "plot_types_when_on_x_axis". All variables will be plotted against another as in a pairs plot. Accepted values of the plot_types_when_on_x_axis include "point", and "box". Accepted values of plot_types_when_on_x_axis must be verbatim. When plot_types is NULL, the x axis will assume integers are categorical and convert them to factors in the custom plotting functions before creating box plots. 

# Example of adding attibutes to a dataframe:
# attr(auto_plot_formatted, "plot_types_when_on_x_axis") <- c("point", "box", "point", "point", "point", "point", "box", "box")
# attributes(auto_plot_formatted)

# Example df: $names
#  "mpg"          "cylinders"    "displacement" "horsepower"   "weight"       "acceleration" "year"         "origin"     

# Example attribute: $plot_types_when_on_x_axis
#  "point" "box"   "point" "point" "point" "point" "box"   "box" 
# In the example, when mpg is on the x axis, a scatter plot will be created. When origin is on the x axis, a box plot will be created. 


# Output: returns plots of all pairs of variables using ggplot. Plots are either of type scatter or box. 

create_all_custom_box_scatter_plots <- function(df, plot_types=NULL) {
    if(!is.null(plot_types)) {
    for (i in seq_along(df)) {
      current_x_axis_name <- names(df[i])
      current_x_axis <- df[[i]]
      current_plot_type <- attributes(df)$plot_types_when_on_x_axis
      # print(sprintf("current_x_axis_name: %s", current_x_axis_name))
      # print(sprintf("current_plot_type: %s", current_plot_type[[i]]))
      for (j in seq_along(df)) {
          
            current_y_axis_name <- names(df[j])
            
            if(identical(current_x_axis_name, current_y_axis_name)) {
              next
            }
            current_y_axis <- df[[j]]
          if (identical(current_plot_type[[i]], "point")) {
            if(identical(current_plot_type[[j]], "box")) {
              next
            # print("Dependent variable is a box plot & independent variable is a point plot.")
          } else {
              
              # print("Create Scatterplot 1")
              custom_plot <- create_custom_geom_point(
                df = df, 
                x = df[[i]], 
                y = df[[j]], 
                title = sprintf("%s vs. %s", current_x_axis_name, current_y_axis_name), 
                x_title = current_x_axis_name, 
                y_title = current_y_axis_name
              )
              # print(sprintf("j: %0.0f", j))
              # print(sprintf("current_plot_type[[j]]: %s", current_plot_type[[j]]))
              # print("create_custom_geom_point defined in 1")
            }
          } else if(identical(current_plot_type[[i]], "box")){
              # print("# Plot x as factor boxplot with double y 3.")
              custom_plot <- create_custom_geom_box_grouped_by_x_factor(
                df = df, 
                x = df[[i]], 
                y = df[[j]], 
                title = sprintf("%s vs. %s", current_x_axis_name, current_y_axis_name), 
                x_title = current_x_axis_name, 
                y_title = current_y_axis_name
              )
              # print(sprintf("j: %0.0f", j))
              # print(sprintf("current_plot_type[[j]]: %s", current_plot_type[[j]]))
              # print("create_custom_geom_box_grouped_by_x_factor defined in 3")
          } else {
            # print("Error in plot type. Plot type is neither scatter nor box.")
            next
          }
            print(custom_plot)
        }
    }
    } else {
    for (i in seq_along(df)) {
      current_x_axis_name <- names(df[i])
      current_x_axis <- df[[i]]
      # print(sprintf("current_x_axis_name: %s", current_x_axis_name))
      for (j in seq_along(df)) {
          
            current_y_axis_name <- names(df[j])
            
            if(identical(current_x_axis_name, current_y_axis_name)) {
              next
            }
            current_y_axis <- df[[j]]
          if (identical(typeof(df[[i]]), "double")) {
            if(identical(typeof(df[[j]]), "double")) {
              # print("Create Scatterplot 1")
              custom_plot <- create_custom_geom_point(
                df = df, 
                x = df[[i]], 
                y = df[[j]], 
                title = sprintf("%s vs. %s", current_x_axis_name, current_y_axis_name), 
                x_title = current_x_axis_name, 
                y_title = current_y_axis_name
              )
              # print("create_custom_geom_point defined in 1")
            } else if(identical(typeof(df[[j]]), "integer")) {
              # print("# Next: X is double, y is integer; Use y as x in boxplot. 2")
              next
            } else {
              # print("Error: X is a double. Y is neither a double nor an integer. Unhandled.")
            }
          } else if(identical(typeof(df[[i]]), "integer")){
            if(identical(typeof(df[[j]]), "integer")){
              # print("# Both x & y are integers. No custom plot exists yet")
              next
            } else if(identical(typeof(df[[j]]), "double")) {
              # print("# Plot x as factor boxplot with double y 3.")
              custom_plot <- create_custom_geom_box_grouped_by_x_factor(
                df = df, 
                x = df[[i]], 
                y = df[[j]], 
                title = sprintf("%s vs. %s", current_x_axis_name, current_y_axis_name), 
                x_title = current_x_axis_name, 
                y_title = current_y_axis_name
              )
              # print("create_custom_geom_box_grouped_by_x_factor defined in 3")
            } else {
              # print("Error: X is an integer. Y is neither a double nor an integer. Unhandled.")
            }
          } else {
            # print("X is neither integer nor double.")
          }
            print(custom_plot)
        }
    }
    }
}
```

## Applied

### Question 6:
In this exercise, you will further analyze the Wage data set considered throughout this chapter.

```{r message=FALSE}
set.seed(42)
attach(Wage)
```

* **Question 6-a**: Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree _d_ for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data. 
  * **Answer**:
```{r include=FALSE}
set.seed(42)

cv.error <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(wage ~ poly(age, i), data = Wage)
  cv.error[i] <- cv.glm(Wage, glm.fit)$delta[1]
}
```

```{r}
lm.fit1 <- lm(wage ~ poly(age, 1), data = Wage)
lm.fit2 <- lm(wage ~ poly(age, 2), data = Wage)
lm.fit3 <- lm(wage ~ poly(age, 3), data = Wage)
lm.fit4 <- lm(wage ~ poly(age, 4), data = Wage)
lm.fit5 <- lm(wage ~ poly(age, 5), data = Wage)
lm.fit6 <- lm(wage ~ poly(age, 6), data = Wage)
lm.fit7 <- lm(wage ~ poly(age, 7), data = Wage)
lm.fit8 <- lm(wage ~ poly(age, 8), data = Wage)
lm.fit9 <- lm(wage ~ poly(age, 9), data = Wage)
lm.fit10 <- lm(wage ~ poly(age, 10), data = Wage)
anova(lm.fit1, lm.fit2, lm.fit3, lm.fit4, lm.fit5, lm.fit6, lm.fit7, lm.fit8, lm.fit9, lm.fit10)
```

```{r}
f_print(sprintf("The model of degree %0.0f was chosen as the model with the lowest test error when tested using cross-validation.", which.min(cv.error)))
cat("\n")
f_print(sprintf("The model of degree 9 is a reasonable fit to the data when examined using the ANOVA test."))
```

* **Question 6-b**: Fit a step function to predict wage using age, and perform cross-validation to choose the optimal number of cuts. Make a plot of the fit obtained.
  * **Answer**:
```{r}
train <- sample(nrow(Wage)*.8)
test <- (-train)
```

```{r warning=FALSE}
k <- 10
folds <- sample(1:k, nrow(Wage), replace = TRUE)

error_matrix <- matrix(rep(0, 10),nrow=10,ncol=10)

for (i in 2:10){
  for (k in 1:10) {
    lm.cut_fit <- lm(wage ~ cut(age, i), data = Wage[folds != k, ])
    lm.cut_pred <- predict(lm.cut_fit, Wage)
    error_matrix[i,k] <- mean((Wage$wage - lm.cut_pred)[folds == k]^2)  
  }
}
clean_error_matrix <- error_matrix[-1, ]

average_test_mse_per_cut <- apply(clean_error_matrix, 1, mean)

lm.cut_fit <- lm(wage ~ cut(age, which.min(average_test_mse_per_cut)), data = Wage)
lm.cut_pred <- predict(lm.cut_fit, Wage)
```


```{r include=FALSE}
lm.cut_fit$xlevels[1]
breaks <- c(0, 26.9, 35.7, 44.6, 53.4, 62.3, 71.1, 80.1)
```


```{r warning=FALSE}
ggplot(Wage) +
  geom_point(aes(age, wage, color = wage)) +
  geom_line(aes(age, lm.cut_pred, color = lm.cut_pred), color = "white", size = 1) + 
  theme_dark() +
  scale_x_continuous(breaks = breaks) + 
  labs(title = "Step Function of Wage vs. Age", x = "Age", y = "Wage", subtitle = "Optimal ")
```

### Question 7:
The Wage data set contains a number of other features not explored
in this chapter, such as marital status (maritl), job class (jobclass),
and others. Explore the relationships between these other
predictors and wage, and use non-linear fitting techniques in order to
fit flexible models to the data. Create plots of the results obtained,
and write a summary of your findings.


```{r}
# maritl, jobclass, region
ggplot(Wage) + 
  geom_violin(aes(maritl, wage), fill = custom_lightblue) +
  geom_boxplot(aes(maritl, wage), color = custom_darkblue, fill = custom_lightblue, alpha = 0.3) +
  labs(title = "Marital Status vs. Wage", x = "Marital Status", y = "Wage ($1000s)")
```

```{r}
# maritl, jobclass, region
ggplot(Wage) + 
  geom_violin(aes(jobclass, wage), fill = custom_lightblue) +
  geom_boxplot(aes(jobclass, wage), color = custom_darkblue, fill = custom_lightblue, alpha = 0.3) +
  labs(title = "Wage vs. Job Class", x = "Job Class", y = "Wage ($1000s)")
```

```{r}
# Remove Widowed
wage_not_widowed <- Wage %>% filter(maritl != "3. Widowed") 
```

```{r}
levels(wage_not_widowed$maritl)
```


```{r}
gam.maritl <- gam(wage ~ maritl + jobclass + s(year, 5), data = wage_not_widowed)
par(mfrow = c(1, 1))
plot(gam.maritl, se = TRUE, col = "green")
```


```{r}
f_print(sprintf("There is a strong relationship between marital status & Information job class with respect to higher wages. Those that have never been married have the lowest wages whereas those that have been divorced or separated are associated with wages that are higher than those that have never been married but lower than those that have been married. There is a positive trend with increasing years with respect to wage."))
```

### Question 8:
Fit the non-linear models investigated in this chapter to the
Auto data set. Is there evidence for non-linear relationships in this
data set? Create informative plots to justify your answer.

```{r message=FALSE}
auto <- na.omit(Auto)
attach(Auto)
```

```{r}
head(Auto)
```

```{r}
auto_plot_formatted <- auto %>% select(everything(), -name)
```


```{r}
# auto
attr(auto_plot_formatted, "plot_types_when_on_x_axis") <- c("point", "box", "point", "point", "point", "point", "box", "box")
# attributes(auto_plot_formatted)
```

```{r warning=FALSE, message=FALSE}
create_all_custom_box_scatter_plots(auto_plot_formatted, plot_types = TRUE)
```


```{r warning=FALSE, message=FALSE}
lm.mpg_displacement_natural_spline.fit <- lm(mpg ~ ns(displacement, df = 6), data = auto)
lm.mpg_displacement_natural_spline.pred <- predict(lm.mpg_displacement_natural_spline.fit, auto)

lm.mpg_displacement_regression_spline.fit <- lm(mpg ~ bs(displacement, degree = 6), data = Wage)
lm.mpg_displacement_regression_spline.pred <- predict(lm.mpg_displacement_regression_spline.fit, auto)

ggplot() + 
  geom_point(aes(mpg, displacement), color = custom_lightblue) +
  geom_smooth(aes(lm.mpg_displacement_natural_spline.pred, displacement), color = custom_darkblue) +
  geom_smooth(aes(lm.mpg_displacement_regression_spline.pred, displacement), color = custom_red) +
  labs(title = "Splines of MPG vs. Displacement", x = "Miles Per Gallon", y = "Displacement", subtitle = "Natural Spline Prediction: Dark Blue\nRegression Spline Prediction: Red") +
  theme_grey()
```
```{r warning=FALSE}
lm.mpg_displacement_smooth_spline.fit <- smooth.spline(mpg, displacement, cv = TRUE)
```


```{r warning=FALSE}
# xlim = range(displacement)
plot(mpg, displacement, cex = .5, col = "darkgrey", main = "Smooth Spline of MPG vs. Displacement", xlab = "Miles Per Gallon", ylab = "Displacement")
lines(lm.mpg_displacement_smooth_spline.fit, col = "blue")
```

```{r warning=FALSE}
dof <- 3

lm.mpg_acceleration_natural_spline.fit <- lm(mpg ~ ns(acceleration, dof), data = Wage)
lm.mpg_acceleration_natural_spline.pred <- predict(lm.mpg_acceleration_natural_spline.fit, auto)

lm.mpg_acceleration_regression_spline.fit <- lm(mpg ~ bs(acceleration, df = dof), data = Wage)
lm.mpg_acceleration_regression_spline.pred <- predict(lm.mpg_acceleration_regression_spline.fit, auto)

ggplot() + 
  geom_point(aes(acceleration, mpg), color = custom_lightblue) +
  geom_line(aes(acceleration, lm.mpg_acceleration_regression_spline.pred), color = custom_red) +
  geom_line(aes(acceleration, lm.mpg_acceleration_natural_spline.pred), color = custom_darkblue) +
  labs(title = "Splines of MPG vs. Displacement", x = "Acceleration", y = "Miles Per Gallon", subtitle = "Natural Spline Prediction: Dark Blue\nRegression Spline Prediction: Red\n") +
  theme_grey()
```

```{r}
acceleration_limits <- range(acceleration)
acceleration.grid <- seq(acceleration_limits[1], acceleration_limits[2])

lo.mpg_acceleration_local_regression_spline.fit <- loess(mpg ~ acceleration, span = 0.2, data = auto)

plot(acceleration, mpg, xlim = acceleration_limits, cex = .05, col = "darkgrey", main = "Local Regression of Mpg vs. Acceleration", xlab = "Acceleration", ylab = "Miles Per Gallon")
lines(acceleration.grid, predict(lo.mpg_acceleration_local_regression_spline.fit, data.frame(acceleration = acceleration.grid)), col = "blue")
```

### Question 9:
This question uses the variables dis (the weighted mean of distances
to five Boston employment centers) and nox (nitrogen oxides concen-
tration in parts per 10 million) from the Boston data. We will treat
dis as the predictor and nox as the response.

```{r warning=FALSE, message=FALSE}
boston <- na.omit(Boston)
attach(boston)
```


* **Question 9-a**: Use the poly() function to fit a cubic polynomial regression to
predict nox using dis. Report the regression output, and plot
the resulting data and polynomial fits.
  * **Answer**:
```{r}
lm.fit9a <- lm(nox ~ poly(dis, 4))
summary(lm.fit9a)
lm.fit9a.pred <- predict(lm.fit9a, boston)

ggplot() + 
  geom_line(aes(dis, lm.fit9a.pred), color = custom_lightblue, size = 2) + 
  geom_point(aes(dis, nox), color = custom_darkblue) +
  labs(title = "NOx Concentration vs. Mean Distance to Employment Centers", x = "Weighted Mean of Distances to 5 Boston Employment Centers", y = "Nitrogen Oxides Concentration (Parts Per 10 Million)")
```

* **Question 9-b**: Plot the polynomial fits for a range of different polynomial
degrees (say, from 1 to 10), and report the associated residual
sum of squares.
  * **Answer**:
  
```{r}
RSS <- sprintf("RSS:\nHighest Degree of Polynomial:")
label <- boston %>% summarise(x = max(dis), y = max(nox), label = RSS)
```
  
  
```{r}
# Input: Dataframe, x, y, title of the plot, title of the x-axis, title of the y-axis
# Output: Scatterplot of x and y variables with correlation between the two on the grid & a smooth line of best fit to the data points. 
create_custom_geom_point_rss <- function(df, x, y, title, x_title, y_title, pred){
    RSS <- sprintf("RSS: %0.3f\nHighest Degree of Polynomial: %0.0f", sum((nox - pred)^2), i)
    label <- df %>% summarise(x = max(x), y = max(y), label = RSS)
    ggplot(df, aes(x, y)) +
      geom_point(color = custom_darkblue) + 
      theme_linedraw() +
      geom_line(aes(dis, pred), color = custom_lightblue, size = 1.5) +
      labs(title = title, x = x_title, y = y_title) + 
      geom_text(aes(label = label), data = label, vjust = "top", hjust = "right", color = "#a60808") 
}

rss <- c(integer(0))
for(i in 1:10) {
  lm.fit <- lm(nox ~ poly(dis, i))
  lm.pred <- predict(lm.fit, boston)
  rss[i] <- sum((nox - lm.pred)^2)
  print(create_custom_geom_point_rss(boston, dis, nox, "Polynomial of NOx Vs. Distance to Employment Centers", "Weighted Mean of Distances to 5 Boston Employment Centers", "Nitrogen Oxides Concentration (Parts Per 10 Million)", lm.pred))
}
```

* **Question 9-c**: Perform cross-validation or another approach to select the opti-
mal degree for the polynomial, and explain your results.
  * **Answer**:

```{r}
train <- sample(nrow(boston) * .8)

mse <- c(integer(10))
for (i in 1:10) {
# i <- 1
lm.fit <- lm(nox ~ poly(dis, i), data = boston, subset = train)
lm.pred <- predict(lm.fit, boston)
mse[i] <- mean((nox - lm.pred)[-train]^2)
}
```

```{r}
  f_print(sprintf("A training and testing validation set was used to identify the optimal value of the highest degree of the polynomial used to pred nox regressed onto weighted mean distance to 5 boston employment centers. The optimal value of the degree of polynomial which creates the lowest mse is %0.0f with a test mse value of %0.3f. This is supported by the graph Polynomial of NOx Vs. Distance to Employment Centers where the highest degree of the polynomial is 2. It is observable in the remaining graphs that degrees above 2 have high variance and fit too closely to the data. And the resulting decrease in variance is not outweighed by the increase in bias gained from using the linear model of degree 1. ", which.min(mse), mse[which.min(mse)]))
```
* **Question 9-d**:Use the bs() function to fit a regression spline to predict nox
using dis. Report the output for the fit using four degrees of
freedom. How did you choose the knots? Plot the resulting fit
  * **Answer**:
```{r}
lm.fit <- lm(nox ~ bs(dis, df = 4))
lm.pred <- predict(lm.fit, boston)
summary(lm.fit)
f_print(sprintf("The output for the fit using four degrees of freedom is observable from the summary above. The knots were selected automatically using the selected 4 degrees of freedom."))
```

```{r}
ggplot() + 
  geom_point(aes(dis, nox), color = custom_darkblue) + 
  geom_line(aes(dis, lm.pred), color = custom_lightblue, size = 1.5) +
  labs(title = "Polynomial of NOx Vs. Distance to Employment Centers", x = "Weighted Mean of Distances to 5 Boston Employment Centers", y = "Nitrogen Oxides Concentration (Parts Per 10 Million)", subtitle = "Highest Degree of Polynomial: 4") 
```


* **Question 9-e**: Now fit a regression spline for a range of degrees of freedom, and
plot the resulting fits and report the resulting RSS. Describe the
results obtained.
  * **Answer**:
  
  
  
```{r}
max_dis <- max(dis)
max_nox <- max(nox)

for (i in 3:10) {
  lm.fit <- lm(nox ~ bs(dis, df = i))
  lm.pred <- predict(lm.fit, boston)
  rss <- sum((boston$nox - lm.pred)^2)
  poly_degree <- sprintf("RSS: %0.3f\nDegrees of Freedom: %0.0f", rss, i)
  label <- boston %>% summarise(x = max_dis, y = max_nox, label = poly_degree)
  regression_spline <- ggplot() + 
    geom_point(aes(dis, nox), color = custom_darkblue) + 
    geom_line(aes(dis, lm.pred), color = custom_lightblue, size = 1.5) +
    labs(title = "Regression Spline of NOx Vs. Distance to Employment Centers", x = "Weighted Mean of Distances to 5 Boston Employment Centers", y = "Nitrogen Oxides Concentration (Parts Per 10 Million)", subtitle = "Varying Degrees of Freedom") +
  geom_text(aes(max_dis, max_nox, label = label), data = label, vjust = "top", hjust = "right", color = "#a60808")
  print(regression_spline)
}
```

* **Question 9-f**: Perform cross-validation or another approach in order to select
the best degrees of freedom for a regression spline on this data.
Describe your results.
  * **Answer**:
```{r}
train <- sample(nrow(boston)*.8)
test <- (-train)
mse <- integer(length = 7)
for (i in 3:10) {
  lm.fit <- lm(nox ~ bs(dis, i), data = boston, subset = train)
  lm.pred <- predict(lm.fit, boston[test, ])
  mse[i - 2] = mean((nox[test] - lm.pred)^2)
}

f_print(sprintf("The degree of freedom that promotes the minimum mean squared error for the created regression spline is: %0.0f degrees of freedom. The calculated test mse is: %0.7f.", which.min(mse) + 2,  mse[which.min(mse)]))
```

### Question 10:
This question relates to the College data set. 

* **Question 10-a**: Split the data into a training set and a test set. Using out-of-state
tuition as the response and the other variables as the predictors,
perform forward stepwise selection on the training set in order
to identify a satisfactory model that uses just a subset of the
predictors.

```{r message=FALSE}
college <- na.omit(College)
attach(college)
train <- sample(nrow(college) * 0.8)
test <- (!train)
```


```{r}
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]]) # Identify the formula of the fit of the model. 
  mat <- model.matrix(form, newdata) # Create a matrix using the formula and the subset of the data
  coefi <- coef(object, id = id) # select the coefficients of the model with i number of predictors
  xvars <- names(coefi) # identify the predictors by name
  mat[, xvars] %*% coefi # make a prediction by all the observations in the matrix of observations that is subset by the predictors included in each i model with the corresponding coefficients of that model.
}
```


```{r}
k <- 10
n <- nrow(college)
set.seed(1)
p <- length(college) - 1
folds <- sample(rep(1:k, length = n)) # grouping each observation into k groups
cv.errors <- matrix(NA, k, p, dimnames = list(NULL, paste(1:p)))
```

```{r}
# This performs training and testing 10 times. For each trained group of models this algorithm will make a prediction for each of the best models for i number of predictors. There are p models with 1 to n number of predictors used in each model where p is the number of predictors and n is the number of predictors used in the model.
for (j in 1:k) {
  best.fit <- regsubsets(Outstate ~ .,
                         data = college[folds != j, ], nvmax = p, method = "forward") # Training on all but k. Returns the best fit for 1 to 19 number of predictors. 
  for (i in 1:p) {
    pred <- predict(best.fit, college[folds == j, ], id = i) # Predict using k as test. This will use the custom predict function above. Best i variable model. This is making predictions for all 19 models 10 times. 
    cv.errors[j, i] <-
      mean((college$Outstate[folds == j] - pred) ^2) # Mean Square Error of each predictor within a k fold.
  }
}
```

```{r}
mean.cv.errors <- apply(cv.errors, 2, mean) # Identify the k which has the lowest mean test error. 2 selects the average of the rows using each column. 1 selects the average of the columns using each row.
par(mfrow = c(1,1))
plot(mean.cv.errors, type = "b")
points(which.min(mean.cv.errors), mean.cv.errors[which.min(mean.cv.errors)], col = "red", cex = 2, pch = 20)
```

```{r}
college.fit <- regsubsets(Outstate ~ ., data = college, subset = train, method = "forward", nvmax = length(college)-1)
summary(college.fit)
```


```{r echo=FALSE}
f_print(sprintf("The model that implements forward stepping and minimizes the means squared error is model %0.0f. The test mean squared error of this model is: %0.3f. This model is comprised of the following predictors: Private, Apps, Accept, Enroll, Top10perc, F. Undergrad, P. Undergrad, Room.Board, Books, Personal, PhD Terminal, S.F. Ratio, perc.alumni, Expend, & Graduation Rate.", which.min(mean.cv.errors), mean.cv.errors[which.min(mean.cv.errors)]))
```
* **Question 10-b**: Fit a GAM on the training data, using out-of-state tuition as
the response and the features selected in the previous step as
the predictors. Plot the results, and explain your findings.
  * **Answer**:

```{r}
names(college)
```


```{r}
gam.fit <- gam(Outstate ~ Private + s(Apps, 5) + s(Accept, 5) + s(Enroll, 5) + s(Top10perc, 5) + s(F.Undergrad, 5) + s(P.Undergrad, 5) + s(Room.Board, 5) + s(Books, 5) + s(Personal, 5) + s(PhD, 5) + s(Terminal, 5) + s(S.F.Ratio, 5) + s(perc.alumni, 5) + s(Expend, 5) + s(Grad.Rate, 5), data = college, subset = train)
gam.pred <- predict(gam.fit, college)

plot(gam.fit, se = TRUE, col = "blue")
```

```{r}
f_print(sprintf("Not only are private universities associated with higher out-of-state tuition, but there is also an increase in the number of applications accepted, the percent of students from the top 10%% of the high school graduating class, the cost of room and board, the percent of faculty with Ph.D.'s, the percent of faculty with terminal degrees, the percent of alumni who donate, the instructional expenditure per student, the student faculty ratio, and the graduation rate. However, increases in out-of-state tuition fees are also associated with a decrease in the number of applications, the number of new students enrolled, the number of fulltime undergraduates, the number of part time undergraduates, estimated book costs, and estimated personal spending."))
```
* **Question 10-c**: Evaluate the model obtained on the test set, and explain the
results obtained.
  * **Answer**:
```{r}
mse <- mean((Outstate - gam.pred)[-train]^2)
f_print(sprintf("The test mse is: %0.3f. This value is the sum of the squares of the residual test observations divided by the number of observation in the test set. The test error is higher on the test set than for the training set. This is expected as the training error tends to underestimate the test error.", mean((Outstate - gam.pred)[-train]^2)))
```

* **Question 10-d**: For which variables, if any, is there evidence of a non-linear relationship with the response?
  * **Answer**:
  
```{r include=FALSE}
head(college)
```
  
  
```{r}
ggplot() + 
  geom_point(aes(Apps, Outstate)) 
 ggplot() +  
  geom_point(aes(Accept, Outstate)) 
 ggplot() + 
  geom_point(aes(Enroll, Outstate)) 
 ggplot() + 
  geom_point(aes(Top10perc, Outstate)) 
 ggplot() + 
  geom_point(aes(F.Undergrad, Outstate)) 
 ggplot() + 
  geom_point(aes(P.Undergrad, Outstate)) 
 ggplot() + 
  geom_point(aes(Room.Board, Outstate)) 
 ggplot() + 
  geom_point(aes(Books, Outstate)) 
 ggplot() + 
  geom_point(aes(Personal, Outstate)) 
 ggplot() + 
  geom_point(aes(PhD, Outstate)) 
 ggplot() + 
  geom_point(aes(Terminal, Outstate)) 
 ggplot() + 
  geom_point(aes(S.F.Ratio, Outstate)) 
 ggplot() + 
  geom_point(aes(perc.alumni, Outstate)) 
 ggplot() + 
  geom_point(aes(Expend, Outstate)) 
 ggplot() + 
  geom_point(aes(Grad.Rate, Outstate)) 
```

```{r}
f_print(sprintf("It appears from the plots above that there is evidence of a non-linear relationship between Ph.D.'s, terminal degrees, and student faculty ratio with respect to out-of-state tuition."))
```

### Question 11: In Section 7.7, it was mentioned that GAMs are generally fit using
a backfitting approach. The idea behind backfitting is actually quite
simple. We will now explore backfitting in the context of multiple
linear regression.
Suppose that we would like to perform multiple linear regression, but
we do not have software to do so. Instead, we only have software
to perform simple linear regression. Therefore, we take the following
iterative approach: we repeatedly hold all but one coefficient esti-
mate fixed at its current value, and update only that coefficient
estimate using a simple linear regression. The process is continued un-
til convergence—that is, until the coefficient estimates stop changing.
View the following example:

* **Question 11-a**: Generate a response Y and two predictors X~1~ and X~2~, with n = 100
* **Question 11-b**: Initialize βˆ1 to take on a value of your choice. It does not matter
what value you choose.
* **Question 11-c**: Keeping ˆβ1 fixed, fit the model Y − ˆβ1X1 = β0 + β2X2 + ε.
* **Question 11-d**: Keeping ˆβ2 fixed, fit the model Y − ˆβ2X2 = β0 + β1X1 + ε.
* **Question 11-e**: Write a for loop to repeat (c) and (d) 1,000 times. Report the
estimates of ˆβ0, ˆβ1, and ˆβ2 at each iteration of the for loop.
Create a plot in which each of these values is displayed, with ˆβ0,
ˆβ1, and ˆβ2 each shown in a different color.
* **Question 11-f**: Compare your answer in (e) to the results of simply performing
multiple linear regression to predict Y using X1 and X2. Use
the abline() function to overlay those multiple linear regression
coefficient estimates on the plot obtained in (e).
* **Question 11-g**: On this data set, how many backfitting iterations were required
in order to obtain a “good” approximation to the multiple re-
gression coefficient estimates?

```{r}
set.seed(42)
X1 <- rnorm(100)
X2 <- rnorm(100)
ε <- rnorm(100)
β0 <- 1
β1 <- 2
β2 <- 3
Y <- β0 + β1*X1 + β2*X2 + ε

βˆ1<- 5

βˆ0_c <- c(integer(1000))
βˆ1_c <- c(integer(1000))
βˆ2_c <- c(integer(1000))
```

```{r}
for (i in 1:1000) { 
    a <- Y - βˆ1 * X1
    βˆ0<- lm(a ~ X2)$coef[1]
    βˆ2 <- lm(a ~ X2)$coef[2]
    βˆ0_c[i] <- βˆ0
    βˆ2_c[i] <- βˆ2
    
    a <- Y - βˆ2 * X2
    βˆ0<- lm(a ~ X1)$coef[1]
    βˆ1 <- lm(a ~ X1)$coef[2]
    βˆ0_c[i] <- βˆ0
    βˆ1_c[i] <- βˆ1  
    
    if (i < 5) {
      
      f_print(sprintf("Iteration %0.0f: \nβˆ0: %0.7f\nβˆ1: %0.7f\nβˆ2: %0.7f", i,  βˆ0, βˆ1, βˆ2))
      cat("\n\n")    
    }
    
}
```


```{r}
df <- tibble(Y, X1, X2)
lm.fit <- lm(Y ~ X1 + X2, data = df)
lm.pred <- predict(lm.fit, df)

ggplot() + 
  geom_point(aes(seq(1:1000), βˆ0_c), color = "#000000", size = .1) + 
  geom_point(aes(seq(1:1000), βˆ1_c), color = custom_lightblue, size = .1) + 
  geom_point(aes(seq(1:1000), βˆ2_c), color = custom_red, size = .1) + 
  geom_hline(yintercept = lm.fit$coefficients[1], alpha = .7, color = "purple") +
  geom_hline(yintercept = lm.fit$coefficients[2], alpha = .7, color = "purple") +
  geom_hline(yintercept = lm.fit$coefficients[3], alpha = .7, color = "purple") +
  labs(title = "Backfit Estimated Coefficients β0, β1, β2", x = "Backfitting Iteration", y = "Estimated Coefficients β0, β1, & β2", subtitle = "β2:Red\nβ1: Blue\nβ0: Black")

f_print(sprintf("Coefficient Estimates β0, β1, & β2 fit to near the true value of the coefficients in the true value of ƒ in the first 2 iterations. This is observable from the value of the initial point displayed on the plot followed by the constant values of the estimated coefficients in the subsequent iterations. This is expected behavior of a model that is generated using the backfitting approach. The estimated coefficients generated from a mulitple linear regression are displayed with each backfit estimated coefficient as purple horizontal lines for reference."))
```

* **Question 12**: This problem is a continuation of the previous exercise. In an
example with p = 100, show that one can approximate the multiple
linear regression coefficient estimates by repeatedly performing simple
linear regression in a backfitting procedure. How many backfitting
iterations are required in order to obtain a “good” approximation to
the multiple regression coefficient estimates? Create a plot to justify
your answer.
  * **Answer**: A good approximation to multiple regression coefficient estimates is observable after 3 iterations of the backfitting procedure when the number of predictors is restricted to 2 or 3. However, after fitting 100 predictors, the model immediately fits and does not subsequently change. There is significant error between the true value of the coefficients and the collected estimated coefficients. The response does apparently change, and the methods used are the same as when there are fewer predictors, yet the linear model is producing estimates that are no different even though the formula alters the predictors that is backfit and the same predictors is excluded from the dataframe. Multiple attempts have been made to investigate and resolve this issue including investigating predictors, estimated outputs, and wrangling the data.

```{r echo=TRUE}
## Example Case of 2 variables
set.seed(42)
X1 <- rnorm(100)
X2 <- rnorm(100)
ε <- rnorm(100)
β0 <- 1
β1 <- 2
β2 <- 3
Y <- β0 + β1*X1 + β2*X2 + ε

βˆ1<- 5

predictors <- matrix(data = NA, nrow = 100, ncol = 2)
true_coefficients <- c(integer(2))

predictors[, 1] <- X1
predictors[, 2] <- X2
true_intercept <- β0
true_coefficients[1] <- β1
true_coefficients[2] <- β2

βˆ0_c <- c(integer(1000))
βˆ1_c <- c(integer(1000))
βˆ2_c <- c(integer(1000))

for (i in 1:4) {
  if(i == 1) {
    a <- Y - βˆ1 * X1
    coefficient_list <- lm(a ~ predictors[, -1])$coeff  
  } else {
    a <- Y - coefficient_list[2] * X1
    coefficient_list <- lm(a ~ predictors[, -1])$coeff
    coefficient_list
  }
  
  βˆ0_c[i] <-  coefficient_list[1]
  βˆ2_c[i] <- coefficient_list[2]

  a <- Y - coefficient_list[2] * X2
  coefficient_list <- lm(a ~ predictors[, -2])$coeff
  coefficient_list
  βˆ0_c[i] <- coefficient_list[1]
  βˆ1_c[i] <- coefficient_list[2]
}

```

```{r}
f_print(sprintf("After 4 iterations, it is observable that the estimated coefficients are unchanging for up to 7 significant digits."))
cat("\n")

f_print(sprintf("Intercept for each iteration:\n"))
cat(str_wrap(string = sprintf("\n%0.7f",βˆ0_c[c(1:i)])))
cat("\n\n")

f_print(sprintf("First Coefficient for each iteration:"))
cat(str_wrap(sprintf(" %0.7f",βˆ1_c[c(1:i)])))
cat("\n\n")

f_print(sprintf("Second Coefficient for each iteration:"))
cat(sprintf(" %0.7f",βˆ2_c[c(1:i)]))
cat("\n\n")

f_print(sprintf("Intercept MSE:"))
cat("\n")
cat(mean((true_intercept - βˆ0_c[i])^2))
cat("\n\n")

f_print(sprintf("First Coefficient MSE:"))
cat("\n")
cat(mean((true_coefficients[1] - βˆ1_c[i])^2))
cat("\n\n")

f_print(sprintf("Second Coefficient MSE:"))
cat("\n")
cat(mean((true_coefficients[2] - βˆ2_c[i])^2))
cat("\n\n")
```

```{r echo=TRUE}
## Example Case of 3 variables
set.seed(42)
X1 <- rnorm(100)
X2 <- rnorm(100)
X3 <- rnorm(100)
ε <- rnorm(100)
β0 <- 1
β1 <- 2
β2 <- 3
β3 <- 4
Y <- β0 + β1*X1 + β2*X2 + β3*X3 + ε

βˆ1 <- 5

predictors <- matrix(data = NA, nrow = 100, ncol = 3)
true_coefficients <- c(integer(3))

predictors[, 1] <- X1
predictors[, 2] <- X2
predictors[, 3] <- X3
true_intercept <- β0
true_coefficients[1] <- β1
true_coefficients[2] <- β2
true_coefficients[3] <- β3

βˆ0_c <- c(integer(1000))
βˆ1_c <- c(integer(1000))
βˆ2_c <- c(integer(1000))
βˆ3_c <- c(integer(1000))

for (i in seq(1:4)) {
  if (i == 1) {
    a <- Y - βˆ1 * predictors[, 1]
    coefficient_list <- lm(a ~ predictors[, -1])$coeff  
  } else {
    a <- Y - coefficient_list[2] * X1 
    coefficient_list <- lm(a ~ predictors[, -1])$coeff
  }
  βˆ0_c[i] <-  coefficient_list[1]
  βˆ2_c[i] <- coefficient_list[2]
  βˆ3_c[i] <- coefficient_list[3]

  a <- Y - coefficient_list[2] * X2
  coefficient_list <- lm(a ~ predictors[, -2])$coeff
  coefficient_list
  βˆ0_c[i] <- coefficient_list[1]
  βˆ1_c[i] <- coefficient_list[2]
  βˆ3_c[i] <- coefficient_list[3]

  a <- Y - coefficient_list[3] * X3
  coefficient_list <- lm(a ~ predictors[, -3])$coeff
  βˆ0_c[i] <- coefficient_list[1]
  βˆ1_c[i] <- coefficient_list[2]
  βˆ2_c[i] <- coefficient_list[3]  
}
```


```{r echo=FALSE}
f_print(sprintf("After 4 iterations, it is observable that the estimated coefficients are unchanging for up to 7 significant digits."))
cat("\n\n")

f_print(sprintf("Intercept for each iteration:\n"))
cat(str_wrap(string = sprintf("\n%0.7f",βˆ0_c[c(1:i)])))
cat("\n\n")

f_print(sprintf("First Coefficient for each iteration:"))
cat(str_wrap(sprintf(" %0.7f",βˆ1_c[c(1:i)])))
cat("\n\n")

f_print(sprintf("Second Coefficient for each iteration:"))
cat(sprintf(" %0.7f",βˆ2_c[c(1:i)]))
cat("\n\n")

f_print(sprintf("Third Coefficient for each iteration:"))
cat(sprintf(" %0.7f", βˆ3_c[c(1:i)]))
cat("\n\n")

f_print(sprintf("Intercept MSE:"))
cat("\n")
cat(mean((true_intercept - βˆ0_c[i])^2))
cat("\n\n")

f_print(sprintf("First Coefficient MSE:"))
cat("\n")
cat(mean((true_coefficients[1] - βˆ1_c[i])^2))
cat("\n\n")

f_print(sprintf("Second Coefficient MSE:"))
cat("\n")
cat(mean((true_coefficients[2] - βˆ2_c[i])^2))
cat("\n\n")

f_print(sprintf("Third Coefficient MSE:"))
cat("\n")
cat(mean((true_coefficients[2] - βˆ3_c[c(1:i)])^2))
cat("\n\n")
```

```{r echo=TRUE}
# 100 Predictors: Initialization of the true function of ƒ
set.seed(42)
true_coefficients <- seq(1:100)
ε <- rnorm(100)
intercept <-rnorm(1)

predictors <- tibble(x1 = rnorm(100))
for (i in seq(2,100)) {
  predictors[i] <- rnorm(100)
}

for ( i in seq(1, 100)) {
  if(i == 1) {
    scaled_predictors <- tibble(true_coefficients[i] * predictors[i])
  } else {
    scaled_predictors[i] <- true_coefficients[i] * predictors[i]
  }
}

sum_of_scaled_predictors <- predictors[1]
for (i in seq(2,100)) {
  sum_of_scaled_predictors <- sum_of_scaled_predictors + scaled_predictors[i]
}

Y <- intercept + sum_of_scaled_predictors + ε
collected_estimated_coefficients <- c(integer(100))
collected_intercept <- c(integer(1))
Y <- Y %>% rename('a' = 'x1')

βˆ1 <- 5

# First Pass of Collecting Estimated Coefficients
a <- Y - βˆ1 * predictors[1]
df <- tibble(a, predictors[-1])
estimated_coefficients <- lm(a ~ ., data = df)$coeff
collected_estimated_coefficients[2] <- estimated_coefficients[2]
collected_intercept <- estimated_coefficients[1]

a <- Y - estimated_coefficients[i] * predictors[i]
  df <- tibble(a, predictors[-i])
  estimated_coefficients <- lm(a ~ ., data = df)$coeff
  collected_intercept <- estimated_coefficients[1]
  collected_estimated_coefficients[i+1] <- estimated_coefficients[i+1]

for(i in seq(2, 100)){
  a <- Y - estimated_coefficients[i] * predictors[i]
  df <- tibble(a, predictors[-i])
  estimated_coefficients <- lm(a ~ ., data = df)$coeff
  collected_intercept <- estimated_coefficients[1]
  if(i == 100) {
    collected_estimated_coefficients[1] <- estimated_coefficients[2]
  } else {
    collected_estimated_coefficients[i+1] <- estimated_coefficients[i+1]
  }
} 

first_pass_collected_estimated_coefficients <- collected_estimated_coefficients
  
# 10 Iterations of the backfitting procedure
for (i in seq(1, 10)) {
    for(i in seq(1, 100)){
      if(i == 1) {
        a <- Y - collected_estimated_coefficients[1] * predictors[i]
        df <- tibble(a, predictors[-i])
        estimated_coefficients <- lm(a ~ ., data = df)$coeff
        collected_intercept <- estimated_coefficients[1]
      } else {
        a <- Y - estimated_coefficients[i] * predictors[i]
        df <- tibble(a, predictors[-i])
        estimated_coefficients <- lm(a ~ ., data = df)$coeff
        collected_intercept <- estimated_coefficients[1]  
      }
    
      if(i == 100) {
        collected_estimated_coefficients[1] <- estimated_coefficients[2]
      } else {
        collected_estimated_coefficients[i+1] <- estimated_coefficients[i+1]
      }
    }
}  
```

```{r}
cat("Intercept & Predictors 1 - 5 after a single pass of the backfitting procedure:\n")
f_print(sprintf(" %0.3f", head(first_pass_collected_estimated_coefficients)))
cat("\n\n")
cat("Intercept & Predictors 1 - 5 after 10 iterations of the backfitting procedure:\n")
f_print(sprintf(" %0.3f", head(collected_estimated_coefficients)))
```