---
title: "Lab 3 Linear Regression Exercises"
author: "Evan Woods"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
  comment = ""
)
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF

library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
```

```{r include=FALSE}
LoadLibraries <- function() {
  if(!require("MASS")) install.packages("MASS")
  if(!require("ISLR2")) install.packages("ISLR2`")

  library(MASS)
  library(ISLR2)
  print("Libraries have been loaded!")
}
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```


```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```


# §3.7 Exercises

## Applied
### Question 8:
This question involves the use of simple linear regression on the Auto data set. 

```{r include=FALSE, message=FALSE}
auto <- na.omit(Auto)
attach(auto)
```

* **Question 8-a**:
  i. Is there a relationship between the predictor and the response?
  ii. How strong is the relationship between the predictor and the response?
  iii. Is the relationship between the predictor and the response positive or negative?
  iv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?
  
* **Answer**:

  i. There is a relationship between the predictor and the response. The F-statistic is very high which indicates that at least one variable is a predictor of the response, and there is only one predictor. 
  ii. The Residual Standard Error is 4.906. The mean value of the response, mpg, is 23.45. This indicates that the percentage of error of this model is high: 20.9%. The Adjusted R^2^  is 60.49. This indicatest that 60.49% of the variability present in the data is captured by the model. This is a weak model. 
  iii. There is a negative relationship between the predictor and the response. This is observable from the negative coefficient of horsepower. 
  iv. The predicted mpg associated with a horsepower of 98 is 24.47 mpg. The confidence interval is 23.97 to 24.96. The prediction interval is 14.8 to 34.12.


```{r}
lm.fit <- lm(mpg ~ horsepower)

# Calculating Percentage of Error from Residual Standard Error
mean_value_of_response <- mean(mpg)
mean_value_of_response

RSE <- summary.lm(lm.fit)$sigma

percentage_of_error <- (RSE / mean(mpg)) * 100
percentage_of_error
```


```{r}
# Summary Statistics
summary.lm(lm.fit)
```


```{r}
# Prediction & Confidence Intervals
predict(lm.fit, data.frame(horsepower = c(98)), interval = "prediction")
predict(lm.fit, data.frame(horsepower = c(98)), interval = "confidence")
```

* **Question 8-b**: Plot the response and the predictor. Use the abline() function to display the least squares regression line. 
  * **Answer**:
  
```{r echo=FALSE, warning=FALSE}
ggplot() + 
  geom_point(aes(horsepower, mpg), color = "#1A0875") +
  geom_abline(aes(intercept = lm.fit$coefficients[[1]], slope = lm.fit$coefficients[[2]]), color = "#34ABEB", size = 1) + 
  theme_linedraw() + 
  labs(title = "Least Squares Regression Line Vs Predictor and Response", x = "Horsepower", y = "Miles Per Gallon")
```

* **Question 8-c**: Use the plot() function to produce diagnostic plots of the least squares regression fit. 
  * **Answer**: There are two or more points with high leverage. There is heteroskedasticity in the residuals. There appears to be a non-linearity in the data after observing the Residuals vs Fitted plot. There are multiple outliers in the dataset.

```{r echo=FALSE}
plot(lm.fit)

# Non-linearity
f_print(sprintf('There appears to be a non-linearity in the data shown from the following plot: Residuals vs Fitted.'))

# Heteroskedasticity: Non-constant variance in the error terms. 
f_print(sprintf("There is heteroskedasticity in the residuals shown by the Residuals vs. Fitted plot. The non-constant Variance is also visible be the Q-Q Reisdual plot where points 331, 328, & 321 do not have constant variance."))
```
```{r echo=FALSE}
# Studentized residuals: Identify outliers if values are outside -3 to 3.

ggplot() + 
  geom_point(aes(
    predict(lm.fit), rstudent(lm.fit),
  ), color = "#1A0875") + 
  geom_hline(yintercept = 3, color = '#a60808') +
  geom_hline(yintercept = -3, color = '#a60808') +
  labs(title = "Studentized Residuals Vs. Fitted Values", subtitle = "Detecting outliers in the model", x = "Fitted Values", y = "Studentized residuals")
```

**Detecting outliers**:

```{r}
# Detecting Outliers
lm.fit$model <- lm.fit$model %>% mutate(row_n = row_number()) 
outliers <- subset(lm.fit$model, rstudent(lm.fit) > 3 | rstudent(lm.fit) < -3)
```


```{r echo=FALSE}
f_print(sprintf("There are %0.0f outliers. They are observations %0.0f and %0.0f.", nrow(outliers), outliers[["row_n"]][[1]], outliers[["row_n"]][[2]]))
```

**Identifying the high-leverage point**: 

```{r}
# Identifying high-leverage point
p <- ncol(lm.fit$model)
n <- nrow(lm.fit$model)

# High-Leverage: value > 3 * (p number of parameters) / (n number of observations)
high_leverage_cutoff <- (3*p/n)

# Identifying high-leverage values
lm.hatvalues <- hatvalues(lm.fit)
high_leverage_values <- lm.hatvalues[lm.hatvalues > high_leverage_cutoff]
```

```{r}
high_leverage_values
```



```{r echo=FALSE}
# High Leverage 
f_print(sprintf("The cutoff value for high-leverage is %0.3f given %0.0f predictors and %0.0f observations.", high_leverage_cutoff, p, n))
f_print(sprintf('There are %0.0f values with high-leverage with respect to the cutoff value of %0.3f. Observations 14, 9, and 116 are displayed as high-leverage on the following plot: Residuals Vs. Leverage. Their values are as follows:', length(high_leverage_values), high_leverage_cutoff))
high_leverage_values
```


```{r include=FALSE}
# Removing High-Leverage value 116
auto_no_high_leverage <- auto %>% mutate(row_number = row_number()) %>% filter(row_number != 116)
lm.fit_auto_no_high_leverage <- lm(auto_no_high_leverage$mpg ~ auto_no_high_leverage$horsepower)
```

**Model Summary Statistics After Removing the High-Leverage Observation**:

```{r echo=FALSE}
summary(lm.fit_auto_no_high_leverage)
```


```{r echo=FALSE}
f_print(sprintf("The R-squared value increased from %0.04f to %0.04f after removing the high leverage value! This indicates a model that captures more of the variability in the data.", summary(lm.fit)$r.squared, summary(lm.fit_auto_no_high_leverage)$r.squared))
```

### Question 9:
This question involves the use of multiple linear regression on the
Auto data set. 

* **Question 9-a**: Produce a scatterplot matrix which includes all of the variables of the data set. 
  * **Answer**:


Variables in Auto
```{r echo=FALSE}
names(auto)
```

```{r echo=FALSE}
pairs(auto, cex = .05, pch = 20, main = "Scatterplot Matrix of All Variables in Auto")
```

* **Question 9-b**: Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.
  * **Answer**:

```{r echo=FALSE}
auto_no_name_col <- auto %>% select(everything(), -name)

cor(auto_no_name_col)
```

* **Question 9-c**: Use the lm() function to perform a multiple linear regression
with mpg as the response and all other variables except name as
the predictors. Use the summary() function to print the results.
Comment on the output.
i. Is there a relationship between the predictors and the response? 
ii. Which predictors appear to have a statistically significant relationship to the response?
iii. What does the coefficient for the year variable suggest?
  * **Answer**:

```{r echo=FALSE}
lm.fit_all_pred_no_name <- lm(mpg ~ ., data = auto_no_name_col)
summary(lm.fit_all_pred_no_name)
```
```{r include=FALSE}
names(summary.lm(lm.fit_all_pred_no_name))
```

```{r include=FALSE}
summary.lm(lm.fit_all_pred_no_name)$fstatistic[["value"]]
```

```{r echo=FALSE}
f_print(sprintf("There is a relationship between the predictors and the response as indicated by the F-statistic of %0.1f.", 
              summary.lm(lm.fit_all_pred_no_name)$fstatistic[["value"]]))

f_print(sprintf("Displacement, Weight, Year, and Origin all appear to have a statistically significant relationship to the response."))

f_print(sprintf("The coefficient of the year variable suggests that for every year, miles per gallon increases by 0.750773."))

```
* **Question 9-d**: Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?
  * **Answer**:
  
```{r echo=FALSE}
plot(lm.fit_all_pred_no_name)
ggplot() +
  geom_point(aes(
    predict(lm.fit_all_pred_no_name), rstudent(lm.fit),
  ), color = "#1A0875") +
  geom_hline(yintercept = 3, color = '#a60808') +
  geom_hline(yintercept = -3, color = '#a60808') +
  labs(title = "Studentized Residuals Vs. Fitted Values", subtitle = "Detecting outliers in the model", x = "Fitted Values", y = "Studentized residuals")
```
```{r echo=FALSE}
f_print(sprintf("There are two outliers in this data as observed in the following plot: Studentized Residuals Vs. Fitted Values. There is heteroskedasticity as seen in the Residuals vs Fitted plot and the Q-Q plot. There is a high-leverage observation observable in the plot Residuals vs Leverage. There is a non-linearity observable in the Residuals vs Fitted plot."))
```
* **Question 9-e**: User the * and : symbols to fit linear models with interaction effects. Do any interactions appear to be statistically significant?
  * **Answer**:
  
```{r echo=FALSE}
f_print(sprintf("The following interactions have significant relationships with respect to mpg: year and weight, horsepower and cylinders, and horsepower and displacement."))
```

```{r echo=FALSE, include=FALSE}
names(auto)
lm.fit_auto_horsepower_displacement_interaction <- lm(mpg ~ horsepower*displacement, data = auto)
lm.fit_auto_weight_year_interaction <- lm(mpg ~ weight*year, data = auto)
lm.fit_auto_cylinders_horsepower_interaction <- lm(mpg ~ cylinders*horsepower, data = auto)

```


```{r echo=FALSE}
summary(lm.fit_auto_horsepower_displacement_interaction)
```

```{r echo=FALSE}
summary(lm.fit_auto_weight_year_interaction)
```

```{r echo=FALSE}
summary(lm.fit_auto_cylinders_horsepower_interaction)
```

* **Question 9-f**: Try a few different combinations of the variables such as log(X), √(X), X^2^. Comment on your findings.
  * **Answer**:
  
```{r echo=FALSE}
f_print(sprintf("Three transformations were performed on weight: the log, square root, and the square of weight. These transformations yielded R-squared values that were highest with the log transformation of the weight. The log transformation model furthmore exhibited improved values with respect to F-statistics and Residual Standard Error. The transformed weight is a significant indicator of mpg in all three transformed linear models."))
```


```{r include=FALSE}
lm.fit_log_weight <- lm(mpg ~ log(weight), data = auto)
lm.fit_sqrt_weight <- lm(mpg ~ sqrt(weight), data = auto)
lm.fit_weight_squared <- lm(mpg ~ I(weight^2), data = auto)
```

```{r echo=FALSE}
summary(lm.fit_log_weight)
```

```{r echo=FALSE}
summary(lm.fit_sqrt_weight)
```

```{r echo=FALSE}
summary(lm.fit_weight_squared)
```

### Question 10:
This question should be answered using the Carseats data set. 

```{r include = FALSE}
attach(Carseats)
```

* **Question 10-a**: Fit a multiple regression model to fit Sales using Price, Urban, and US.
  * **Answer**: 

```{r echo=FALSE}
lm.fit_carseats <- lm(Sales ~ Price + Urban + US, data = Carseats)
lm.fit_carseats
```

* **Question 10-b**: Provide an interpretation of each coefficient in the model. Be careful - some of the variables in the model are qualitative!
  * **Answer**: 

```{r}
f_print(sprintf("Every 1 unit of increase in price will decrease sales by 54.46 units. Stocking carseats in an Urban location decreases sales by 21.92 units. Stocking carseats in US stores increase sales by 1200 units."))
```

* **Question 10-c**: Write out the model in equation form, being careful to handle the qualitative variables properly.
  * **Answer**: 
  
```{r}
# If the Carseat is stocked in an Urban And US store:
# Sales ≈ β_0 + β_1 * Price + β_2 + β_3

# If the Carseat is stocked in an Urban store only:
# Sales ≈ β_0 + β_1 * Price + β_2

# If the Carseat is stocked in a US store only:
# Sales ≈ β_0 + β_1 * Price + β_3
```

* **Question 10-d**: For which of the predictors can you reject the null hypothesis H~0~ : β~j~ = 0?
  * **Answer**: 
  
```{r echo=FALSE}
f_print(sprintf("The following are significant predictors of sales: Price & US."))
```

  
```{r echo=FALSE}
summary(lm.fit_carseats)
```

* **Question 10-e**: On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome. 
  * **Answer**: 
  
```{r echo=FALSE}
lm.fit_carseats_sm <- lm(Sales ~ Price + US)
lm.fit_carseats_sm
```
* **Question 10-f**: How well do the models fit the data?
  * **Answer**:

```{r include=FALSE}
names(summary.lm(lm.fit_carseats))

percentage_error_carseats <- summary.lm(lm.fit_carseats)$sigma/mean(Sales) * 100
fit_carseats_r_squared <- summary.lm(lm.fit_carseats)$r.squared * 100

percentage_error_carseats_sm <- summary.lm(lm.fit_carseats_sm)$sigma/mean(Sales) * 100
fit_carseats_sm_r_squared <- summary.lm(lm.fit_carseats_sm)$r.squared * 100
```


```{r echo=FALSE}
f_print(sprintf("The model composed of US, Urban, & Price captures %0.4f%% of the variablility of the data with %0.2f percent error with respect to the response, sales.", fit_carseats_r_squared, percentage_error_carseats))

f_print(sprintf("The model exclusively composed of US & Price captures %0.4f%% of the variablility of the data with %0.2f percent error with respect to the response, sales.", fit_carseats_sm_r_squared, percentage_error_carseats_sm))

f_print(sprintf("The US & Price model captures less variability in the data but accomodates this with less response error."))
```
* **Question 10-g**: Using the model from (e), obtain 95 % confidence intervals for
the coefficient(s).
  * **Answer**:
```{r echo=FALSE}
confint(lm.fit_carseats_sm)
```
* **Question 10-h**: Is there evidence of outliers or high leverage observations in the model from (e)?
  * **Answer**:
  
Observations with High Leverage
```{r include=FALSE}
high_leverage_cutoff <- 3 * ncol(lm.fit_carseats_sm$model) / nrow(lm.fit_carseats_sm$model)
high_leverage_cutoff
```

```{r echo=FALSE}
# Detecting High Leverage
fit_carseats_sm_high_leverage <- subset(lm.fit_carseats_sm$model, hatvalues(lm.fit_carseats_sm) > high_leverage_cutoff)
fit_carseats_sm_high_leverage
```

Detecting Outliers

```{r echo=FALSE}
# Detecting Outliers
fit_carseats_sm_outliers <- subset(lm.fit_carseats_sm$model, (rstudent(lm.fit_carseats_sm)  < -3 | rstudent(lm.fit_carseats_sm) > 3))
fit_carseats_sm_outliers
```


```{r echo=FALSE}
f_print(sprintf("There is evidence of %0.0f observations with high leverage given a cutoff value of %0.3f. This is observable in the plot Residuals vs Leverage. There are no outliers.", nrow(fit_carseats_sm_high_leverage), high_leverage_cutoff))
```

  
```{r echo=FALSE}
plot(lm.fit_carseats_sm)
ggplot() +
  geom_point(aes(
    predict(lm.fit_carseats_sm), rstudent(lm.fit_carseats_sm),
  ), color = "#1A0875") +
  geom_hline(yintercept = 3, color = '#a60808') +
  geom_hline(yintercept = -3, color = '#a60808') +
  labs(title = "Studentized Residuals Vs. Fitted Values", subtitle = "Detecting outliers in the model", x = "Fitted Values", y = "Studentized residuals")
```

### Question 11: 
In this problem, we will investigate the t-statistic for the null hypothesis H~0~: β = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.


```{r}
set.seed(1)
x <- rnorm(100)
y <- 2*x + rnorm(100)
```

* **Question 11-a**: Perform a simple linear regression of y onto x, without an in-
tercept. Report the coefficient estimate ˆβ, the standard error of
this coefficient estimate, and the t-statistic and p-value associ-
ated with the null hypothesis H0 : β = 0. Comment on these
results. (You can perform regression without an intercept using
the command lm(y∼x+0).)

  * **Answer**: 

```{r echo=FALSE}
lm.fit_11 <- lm(y ~ x + 0)
summary.lm(lm.fit_11)$coefficients
```

```{r echo=FALSE}
f_print(sprintf("The estimate indicates that for every unit value in x, y changes by 1.993876. The standard error indicates the size of the standard deviation of the error of the estimate of y regressed onto x. In this case, its value is 0.1064767. The t value is a measure of the number of standard deviations the x coefficient is away from 0. The p-value indicates x is a significant predictor of y. The p-value is the probability of observing any number equal to the absolute value of t or larger assuming y is not regressed onto x."))
```

* **Question 11-b**: Now perform a simple linear regression of x onto y without an
intercept, and report the coefficient estimate, its standard error,
and the corresponding t-statistic and p-values associated with
the null hypothesis H0 : β = 0. Comment on these results.
  * **Answer**: 

```{r echo=FALSE}
lm.fit_11b <- lm(x ~ y + 0)
summary.lm(lm.fit_11b)$coefficients
```

```{r echo=FALSE}
f_print(sprintf("For every 1 unit increase in 1, there is an increase of x of 0.3911145. The size of the standard deviation of the error of y regressed onto x is 0.02088625. The y coefficient is 18.72593 standard deviations away from 0. The p value is significant at 2.642197e-34 and indicates the probability of observing any number equal to the absolute value of t or larger given x is not regressed onto y."))
```
* **Question 11-c**: 
  * **Answer**: 

```{r echo=FALSE}
f_print(sprintf("Both regressions share the same intercept (0), t-values, and p-values. They are both positive in slope."))
```


```{r echo=FALSE}
lm(y ~ x)
```


```{r echo=FALSE}
ggplot() +
  geom_point(aes(x, y)) +
  geom_abline(intercept = -.03769, slope = 1.99894, color = "green") +
  geom_abline(intercept = 0, slope = 0.3911145, color = "red") +
  geom_abline(intercept = 0, slope = 1.993876, color = "blue", alpha = 0.7) + 
  theme_dark() + 
  labs(title = "Plot of linear models with and without intercept.", subtitle = "lm(y~x+0): blue\nlm(x~y+0): red\nlm(y~x): green")
```

### Question 14:
This problem focuses on the *collinearity* problem.
  
* **Question 14-a**: Perform the following commands in R then write out the form of the linear model.
What are the regression coefficients?
  * **Answer**:

```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 + x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```

```{r echo=FALSE}
f_print(sprintf("The correlation between x1 and x2 is: %0.03f.", cor(x1,x2)))
```

```{r echo=FALSE}
f_print(sprintf("The form of the linear model is Y ≈ β_0 + β_1 * x1 + β_2 * x2 + ε"))
```

* **Question 14-b**: What is the correlation between x1 and x2? Create a scatterplot
displaying the relationship between the variables.
  * **Answer**:
```{r echo=FALSE}
f_print(sprintf("The correlation between x1 & x2 is %0.2f.", cor(x1, x2)))
ggplot() + 
  geom_point(aes(x1, x2)) +
  labs(title = "Plot of x1 vs x2", subtitle = "Correlation: 0.95")
```

* **Question 14-c**: Using this data, fit a least squares regression to predict y using
x1 and x2. Describe the results obtained. What are ˆβ0, ˆβ1, and
ˆβ2? How do these relate to the true β0, β1, and β2? Can you
reject the null hypothesis H0 : β1 = 0? How about the null
hypothesis H0 : β2 = 0?
  * **Answer**:

```{r echo=FALSE}
lm.fit_14 <- lm(y ~ x1 + x2)
summary(lm.fit_14)
```

```{r include=FALSE}
fit_14_vif <- vif(lm.fit_14)
```


```{r echo=FALSE}
f_print(sprintf("The predicted coefficients are all underestimations of the coefficients of the true function representing y regressed onto x. I cannot reject the null hypothesis with respect to β1 nor β2 due to high p-values and vif scores greater than 5 for x1 and x2 respectively. The vif scores for x1 and x2 respectively are %0.02f and %0.02f.",
                fit_14_vif[[1]],
                fit_14_vif[[2]]))
```


```{r echo=FALSE}
f_print(sprintf("ˆβ0: 1.7757\nˆβ1: 1.0847\nˆβ2: 1.0097\n"))
f_print(sprintf("β0: 2\nβ1: 2\nβ2: 0.3"))
```

* **Question 14-d**: Now fit a least squares regression to predict y using only x1.
Comment on your results. Can you reject the null hypothesis
H0 : β1 = 0?
  * **Answer**: 
  
```{r echo=FALSE}
lm.fit_14d <- lm(y ~ x1)
summary(lm.fit_14d)
f_print(sprintf("The x1 is a significant predictor of y. The p-value is 5.42e-07."))
```

* **Question 14-e**: Now fit a least squares regression to predict y using only x2.
Comment on your results. Can you reject the null hypothesis
H0 : β1 = 0?
  * **Answer**:

```{r echo=FALSE}
lm.fit_14e <- lm(y ~ x2)
summary(lm.fit_14e)
f_print(sprintf("x2 is also a significant predictor of y. I reject the null using the p-value of 5.361e-07"))
```

* **Question 14-f**: Do the results obtained contradict each other? Explain your answer.
  * **Answer**: The results do not contradict each other. Each variable separately is a significant indicator of the response. Because the two variables are highly correlated, it is ambiguous as to which is the significant predictor with respect to the response. An intermediate variable is potentially available as a solution by using the standardized versions of x1 and x2 if such a standard were to exist. Another remedy is to use either variable exclusively as predictors of the response.
  
  

* **Question 14-g**: Now suppose we obtain one additional observation, which was
unfortunately mismeasured. Re-fit the linear models using this new data. What effect does this new observation have on each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.
  * **Answer**: 

```{r include=FALSE}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
```

```{r echo=FALSE}
lm.fit_14g1 <- lm(y ~ x1 + x2)
lm.fit_14g2 <- lm(y ~ x1)
lm.fit_14g3 <- lm(y ~ x2)
summary(lm.fit_14g1)
vif(lm.fit_14g1)
```

```{r echo=FALSE}
summary(lm.fit_14g2)
```

```{r echo=FALSE}
summary(lm.fit_14g3)
```
```{r include = FALSE}
fit_14g1_vif <- vif(lm.fit_14g1)
```

```{r echo=FALSE}
f_print(sprintf("y regressed onto x1 and x2 now shows that x2 is a significant predictor of the response given a p-value of .00391. The vif scores indicate that x1 and x2 are collinear, albeit less so, with values of %0.2f and %0.2f respectively. x1 and x2 individually are still both significant predictors of the response, y.", fit_14g1_vif[[1]], fit_14g1_vif[[2]]))
```

### Question 15: 
This problem involves the Boston data set, which we saw in the lab
for this chapter. We will now try to predict per capita crime rate
using the other variables in this data set. In other words, per capita
crime rate is the response, and the other variables are the predictors.

* **Question 15-a**: For each predictor, fit a simple linear regression model to predict
the response. Describe your results. In which of the models is
there a statistically significant association between the predictor
and the response? Create some plots to back up your assertions.
  * **Answer**:
  
```{r include=FALSE}
boston <- na.omit(Boston)
attach(boston)
```

```{r include=FALSE}
names(Boston)
```

```{r echo=FALSE}
f_print(sprintf("There are statistically significant results in the models where per capita crime rate regressed onto the following predictors: zn, indus, nox, rm, age, dis, rad, tax, ptratio, lstat, medv. There is a slope associated with the response regressed exclusively onto each predictor observable in each plot as well as shapes of the data which suggest a relationship between the response and each predictor that has been determined to have a significant relationship."))
```


```{r echo=FALSE}
lm.fit_15_zn <- lm(crim ~ zn)
lm.fit_15_indus <- lm(crim ~ indus)
lm.fit_15_chas <- lm(crim ~ chas)
lm.fit_15_nox <- lm(crim ~ nox)
lm.fit_15_rm <- lm(crim ~ rm)
lm.fit_15_age <- lm(crim ~ age)
lm.fit_15_dis <- lm(crim ~ dis)
lm.fit_15_rad <- lm(crim ~ rad)
lm.fit_15_tax <- lm(crim ~ tax)
lm.fit_15_ptratio <- lm(crim ~ ptratio)
lm.fit_15_lstat <- lm(crim ~ lstat)
lm.fit_15_medv <- lm(crim ~ medv)

summary(lm.fit_15_zn)
summary(lm.fit_15_indus)
summary(lm.fit_15_chas)
summary(lm.fit_15_nox)
summary(lm.fit_15_rm)
summary(lm.fit_15_age)
summary(lm.fit_15_dis)
summary(lm.fit_15_rad)
summary(lm.fit_15_tax)
summary(lm.fit_15_ptratio)
summary(lm.fit_15_lstat)
summary(lm.fit_15_medv)
```


```{r echo=FALSE}
custom_crim_pair_plot <- function(predictor, titles, predictor_labels, lm_intercept, lm_slope) {
custom_plot <- ggplot(boston) + 
  geom_point(aes(
    predictor,
    crim
  ), color = custom_darkblue) + 
  geom_abline(intercept = lm_intercept, slope = lm_slope, color = custom_red) +
  labs(title = titles, x = predictor_labels, y = "Per Capita Crime Rate") + 
  theme_linedraw()
  print(custom_plot)
}

custom_crim_pair_plot(
  zn, 
  "Per Capita Crime Rate Vs. Proportion of Residential Land Zoned", 
  "proportion of residential land zoned for lots over 25,000 sq.ft.",
  lm.fit_15_zn$coefficients[[1]],
  lm.fit_15_zn$coefficients[[2]]
  )

custom_crim_pair_plot(
  indus, 
  "Per Capita Crime Rate Vs. Proportion of Non-retail Business Acres", 
  "proportion of non-retail business acres per town",
  lm.fit_15_indus$coefficients[[1]],
  lm.fit_15_indus$coefficients[[2]]
  )

custom_crim_pair_plot(
  nox, 
  "Per Capita Crime Rate Vs. Nitrogen Oxides Concentration", 
  "nitrogen oxides concentration (parts per 10 million)",
  lm.fit_15_nox$coefficients[[1]],
  lm.fit_15_nox$coefficients[[2]]
  )

custom_crim_pair_plot(
  rm, 
  "Per Capita Crime Rate Vs. Average Number of Rooms", 
  "average number of rooms per dwelling",
  lm.fit_15_rm$coefficients[[1]],
  lm.fit_15_rm$coefficients[[2]]
  )

custom_crim_pair_plot(
  age, 
  "Per Capita Crime Rate Vs. Age of Home", 
  "proportion of owner-occupied units built prior to 1940",
  lm.fit_15_age$coefficients[[1]],
  lm.fit_15_age$coefficients[[2]]
  )

custom_crim_pair_plot(
  dis, 
  "Per Capita Crime Rate Vs. Distnace to Employment Centres", 
  "weighted mean of distances to five Boston employment centres",
  lm.fit_15_dis$coefficients[[1]],
  lm.fit_15_dis$coefficients[[2]]
  )

custom_crim_pair_plot(
  rad, 
  "Per Capita Crime Rate Vs. Accessibility Index to Highways", 
  "index of accessibility to radial highways",
  lm.fit_15_rad$coefficients[[1]],
  lm.fit_15_rad$coefficients[[2]]
  )

custom_crim_pair_plot(
  tax, 
  "Per Capita Crime Rate Vs. Property-Tax Rate", 
  "full-value property-tax rate per $10,000",
  lm.fit_15_tax$coefficients[[1]],
  lm.fit_15_tax$coefficients[[2]]
  )

custom_crim_pair_plot(
  ptratio, 
  "Per Capita Crime Rate Vs. Pupil-Teacher Ratio", 
  "pupil-teacher ratio by town",
  lm.fit_15_ptratio$coefficients[[1]],
  lm.fit_15_ptratio$coefficients[[2]]
  )

custom_crim_pair_plot(
  lstat, 
  "Per Capita Crime Rate Vs. % of Lower Status Population", 
  "lower status of the population (percent)",
  lm.fit_15_lstat$coefficients[[1]],
  lm.fit_15_lstat$coefficients[[2]]
  )

custom_crim_pair_plot(
  medv, 
  "Per Capita Crime Rate Vs. Median Value of Homes", 
  "median value of owner-occupied homes in $1000s",
  lm.fit_15_medv$coefficients[[1]],
  lm.fit_15_medv$coefficients[[2]]
  )


```


* **Question 15-b**: Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H~0~ β~j~ = 0?
  * **Answer**:
  
```{r echo=FALSE}
f_print(sprintf("It is appropriate to reject the null hypothesis in the multiple regression model for the following predictors: zn, dis, rad, & medv. The F-statistic is low yet still indicates that one or more of the predictors are significant indicators of the response."))
```


```{r echo=FALSE}
lm.fit_15_mr <- lm(crim ~ ., data = boston)
summary(lm.fit_15_mr)
```
* **Question 15-c**: How do your results from (a) compare to your results from (b)?
Create a plot displaying the univariate regression coefficients
from (a) on the x-axis, and the multiple regression coefficients
from (b) on the y-axis. That is, each predictor is displayed as a
single point in the plot. Its coefficient in a simple linear regres-
sion model is shown on the x-axis, and its coefficient estimate
in the multiple linear regression model is shown on the y-axis.
  * **Answer**: 
  

```{r echo=FALSE}
#zn, indus, nox, rm, age, dis, rad, tax, ptratio, lstat, medv
x <- c(
    lm.fit_15_zn$coefficients[[2]],
    lm.fit_15_indus$coefficients[[2]],
    lm.fit_15_nox$coefficients[[2]],
    lm.fit_15_rm$coefficients[[2]],
    lm.fit_15_rm$coefficients[[2]],
    lm.fit_15_age$coefficients[[2]],
    lm.fit_15_dis$coefficients[[2]],
    lm.fit_15_rad$coefficients[[2]],
    lm.fit_15_tax$coefficients[[2]],
    lm.fit_15_ptratio$coefficients[[2]],
    lm.fit_15_lstat$coefficients[[2]],
    lm.fit_15_medv$coefficients[[2]]
    )
y <- c(double(0L))
for (index in seq_along(lm.fit_15_mr$coefficients[-1])) {
 y <- c(y, lm.fit_15_mr$coefficients[-1][[index]]) 
}

ggplot() + 
  geom_point(aes(x, y)) + 
  labs(x = "Univariate Regression Coefficients", y = "Multivariate Regression Coefficients") +
  theme_linedraw()

f_print(sprintf("There are more significant predictors in the models where the response is regressed exclusively by a single predictor versus the multiple regression model."))
```
* **Question 15-d**: Is there evidence of non-linear association between any of the
predictors and the response? To answer this question, for each
predictor X, fit a model of the form: Y = β~0~ + β~1~X + β~2~X^2^ + β~3~X^3^ + ε
  * **Answer**:
  
```{r echo=FALSE}
f_print(sprintf("There is evidence of a non-linear association of the response regressed onto the following predictors as determined by significant p-values: indus, nox, age, dis, ptratio, & medv."))
```
  
```{r echo=FALSE}
lm.fit_15_zn_mr <- lm(crim ~ zn + I(zn^2) + I(zn^3))
lm.fit_15_indus_mr <- lm(crim ~ indus + I(indus^2) + I(indus^3))
lm.fit_15_chas_mr <- lm(crim ~ chas + I(chas^2) + I(chas^3))
lm.fit_15_nox_mr <- lm(crim ~ nox + I(nox^2) + I(nox^3))
lm.fit_15_rm_mr <- lm(crim ~ rm + I(rm^2) + I(rm^3))
lm.fit_15_age_mr <- lm(crim ~ age + I(age^2) + I(age^3))
lm.fit_15_dis_mr <- lm(crim ~ dis + I(dis^2) + I(dis^3))
lm.fit_15_rad_mr <- lm(crim ~ rad + I(rad^2) + I(rad^3))
lm.fit_15_tax_mr <- lm(crim ~ tax + I(tax^2) + I(tax^3))
lm.fit_15_ptratio_mr <- lm(crim ~ ptratio + I(ptratio^2) + I(ptratio^3))
lm.fit_15_lstat_mr <- lm(crim ~ lstat + I(lstat^2) + I(lstat^3))
lm.fit_15_medv_mr <- lm(crim ~ medv + I(medv^2) + I(medv^3))

summary(lm.fit_15_zn_mr)
summary(lm.fit_15_indus_mr)
summary(lm.fit_15_chas_mr)
summary(lm.fit_15_nox_mr)
summary(lm.fit_15_rm_mr)
summary(lm.fit_15_age_mr)
summary(lm.fit_15_dis_mr)
summary(lm.fit_15_rad_mr)
summary(lm.fit_15_tax_mr)
summary(lm.fit_15_ptratio_mr)
summary(lm.fit_15_lstat_mr)
summary(lm.fit_15_medv_mr)
```
  

  
  