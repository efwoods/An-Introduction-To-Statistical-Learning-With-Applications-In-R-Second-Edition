```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
  comment = ""
)
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression
if(!require("splines")) install.packages("splines")
if(!require("gam")) install.packages("gam")
if(!require("akima")) install.packages("akima")
if(!require("tree")) install.packages("tree") # Classification and Regression Trees
if(!require("randomForest")) install.packages("randomForest")
if(!require("gbm")) install.packages("gbm") # Boosted Trees
if(!require("BART")) install.packages("BART")
if(!require("reticulate")) install.packages("reticulate") # Use python objects in R
if(!require("ROCR")) install.packages("ROCR")
if(!require("keras")) install.packages("keras") # Install keras for deep learning

library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
library(ROCR)
library(reticulate)
library(BART)
library(gbm)
library(randomForest)
library(tree)
library(akima)
library(gam)
library(splines)
library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
```

```{r}
keras::install_keras(method = "conda", python_version = "3.10")
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```

```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

```{r}
accuracy <- function(pred, truth) {
  mean(drop(as.numeric(pred)) == drop(truth))
}
```

## Applied
### Question 6:
Consider the simple function R(β) = sin(β) + β/10

* **Question 6-a**: Draw a graph of this funciton over the range β ∈ [-6, 6]
  * **Answer**:
```{r}
β <- seq(-6, 6)
R <- sin(β) + β/10
ggplot() + 
  geom_point(aes(β, R)) 
```

* **Question 6-b**: What is the derivative of this function?
  * **Answer**:
```{r}
f_print(sprintf("The derivative of the funciton R(β) = sin(β) + β/10 is δR(β) = cos(β)."))
δR <- cos(β)
```

* **Question 6-c**:
  * **Answer**:

```{r}
grad <- function(starting_point, learning_rate) {
  gradient_1x <- starting_point - learning_rate*cos(starting_point)
  gradient_2x <- gradient_1x - learning_rate*cos(gradient_1x)
  
  gradient_1y <- sin(gradient_1x) + gradient_1x/10
  gradient_2y <- sin(gradient_2x) + gradient_2x/10
  
  gradients_x <- c(gradient_1x, gradient_2x)
  gradients_y <- c(gradient_1y, gradient_2y)
  while(gradient_1y - gradient_2y > 0) {
    gradient_1y <- gradient_2y
    gradient_2x <- gradient_2x - learning_rate*cos(gradient_2x)
    gradient_2y <- sin(gradient_2x) + gradient_2x/10
    gradients_x <- c(gradients_x, gradient_2x)
    gradients_y <- c(gradients_y, gradient_2y)
  }
  gradients <- tibble(gradients_x, gradients_y)
  # print(gradients_x)
  return(gradients)
}
```


```{r}
β_0 <- 2.3
ρ <- 0.1

gradients <- grad(β_0, ρ)
gradients$gradients_y[which.min(gradients$gradients_y)]
```


```{r}
label <- tibble(β[7:length(β)], R[7:length(β)], "this")
ggplot() +
  geom_point(aes(gradients$gradients_x, gradients$gradients_y), color = custom_darkblue, size = 1) +
  geom_point(aes(β,
R), color = custom_red) + 
  labs(title = "Gradient Descent of R(β) = sin(β) + β/10", x = "β", y = "R(β)", subtitle = "Initial β = 2.3\nMinimum R(β) = -0.53375735")
```

* **Question 6-d**:
  * **Answer**:
```{r}
β_0 <- 1.4
ρ <- 0.1
gradients2 <- grad(β_0, ρ)
gradients2$gradients_y[which.min(gradients2$gradients_y)]
```


```{r}
ggplot() +
  geom_point(aes(gradients2$gradients_x, gradients2$gradients_y), color = custom_darkblue, size = 1) +
  geom_point(aes(β,
R), color = custom_red) + 
  labs(title = "Gradient Descent of R(β) = sin(β) + β/10", x = "β", y = "R(β)", subtitle = "Initial β = 1.4\nMinimum R(β) = -1.15708")
```

### Question 7:
Fit a neural network to the Default data. Use a single hidden layer with 10 units, and dropout regularization. Have a look at Labs 10.9.1-10.9.2 for guidance. Compare the classification performance of your model with that of linear logistic regression.

```{r}
Default <- na.omit(Default)
n <- nrow(Default)
set.seed(42)
ntest <- trunc(n / 3)
testid <- sample(1:n, ntest)
```

```{r}
glm.true <- Default
glm.true <- glm.true %>% select(default, everything()) %>% mutate(default_int = as.integer(default) - 1) %>% select(default, default_int, everything()) 
glm.true <- glm.true %>% select(default_int, everything(), -default) 
glm.true <- glm.true %>% rename("default" = "default_int") 

glm.fit <- glm(default ~ ., data = Default[-testid, ], family = "binomial")
glm.prob <- rep(0, nrow(Default[testid, ]))
glm.pred <- predict(glm.fit, Default[testid, ], type = "response")
glm.prob[glm.pred > 0.5] <- 1

glm.var <- var(glm.true[testid, "default"])

glm.R_2 <- 1 - (mean((glm.true[testid, "default"] - glm.prob)^2) / glm.var)
# glm.R_2

glm.table <- table(truth = glm.true[testid, "default"], prediction = glm.prob)
glm.acc <- (glm.table[1] + glm.table[4]) / sum(glm.table) * 100
# glm.acc 
```

```{r}
x <- scale(model.matrix(default ~ . - 1, data = Default))
y <- glm.true$default
```

```{r}
model <- keras_model_sequential() %>%
  layer_dense(input_shape = ncol(x), units = 10, activation = "relu") %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1, activation = "sigmoid")
summary(model)
```


```{r results='hide'}
model %>% compile(loss = "mse", optimizer = optimizer_rmsprop(), metrics = c("accuracy"))
model %>% fit(x[-testid, ], y[-testid], epochs = 30, batch_size = 128, validation_split = 0.2)
```


```{r}
model.probs <- rep(0, length(predictions))
model.probs[predictions > 0.5] <- 1

model.table <- table(prediction = probs, truth = glm.true[testid, "default"])

model.testacc <- (val[1] + val[4]) /sum(val) * 100

f_print(sprintf("The accuracy of the neural network model is %0.3f%%.", model.testacc))
cat("\n")
model.table

cat("\n")

f_print(sprintf("The accuracy of the logistic regression model is %0.3f%%.", glm.acc))
cat("\n")
glm.table
```


### Question 8:
From your collection of personal photographs, pick 10 images of animals (such as dogs, cats, birds, farm animals, etc.). If the subject does not occupy a reasonable part of the image, then crop the image. Now use a pretrained image classification CNN as in Lab 10.9.4 to predict the class of each of your images, and report the probabilities for the top five predicted classes for each image. 
```{r}

```

