
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
  comment = ""
)
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression
if(!require("splines")) install.packages("splines")
if(!require("gam")) install.packages("gam")
if(!require("akima")) install.packages("akima")
if(!require("tree")) install.packages("tree") # Classification and Regression Trees
if(!require("randomForest")) install.packages("randomForest")
if(!require("gbm")) install.packages("gbm") # Boosted Trees
if(!require("BART")) install.packages("BART")
if(!require("reticulate")) install.packages("reticulate") # Use python objects in R
if(!require("ROCR")) install.packages("ROCR")
if(!require("keras")) install.packages("keras") # Install keras for deep learning
if(!require("jpeg")) install.packages("jpeg")
if(!require("imager")) install.packages("imager")

library(imager)
library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
library(ROCR)
library(reticulate)
library(BART)
library(gbm)
library(randomForest)
library(tree)
library(akima)
library(gam)
library(splines)
library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
library(jpeg)
```

```{r output=FALSE, results = 'hide', message=FALSE}
# keras::install_keras(method = "conda", python_version = "3.10")
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```

```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

```{r}
accuracy <- function(pred, truth) {
  mean(drop(as.numeric(pred)) == drop(truth))
}
```

## Applied
### Question 6:
Consider the simple function R(β) = sin(β) + β/10

* **Question 6-a**: Draw a graph of this funciton over the range β ∈ [-6, 6]
  * **Answer**:
```{r}
β <- seq(-6, 6)
R <- sin(β) + β/10
ggplot() + 
  geom_point(aes(β, R)) 
```

* **Question 6-b**: What is the derivative of this function?
  * **Answer**:
```{r}
f_print(sprintf("The derivative of the funciton R(β) = sin(β) + β/10 is δR(β) = cos(β)."))
δR <- cos(β)
```

* **Question 6-c**:
  * **Answer**:

```{r}
grad <- function(starting_point, learning_rate) {
  gradient_1x <- starting_point - learning_rate*cos(starting_point)
  gradient_2x <- gradient_1x - learning_rate*cos(gradient_1x)
  
  gradient_1y <- sin(gradient_1x) + gradient_1x/10
  gradient_2y <- sin(gradient_2x) + gradient_2x/10
  
  gradients_x <- c(gradient_1x, gradient_2x)
  gradients_y <- c(gradient_1y, gradient_2y)
  while(gradient_1y - gradient_2y > 0) {
    gradient_1y <- gradient_2y
    gradient_2x <- gradient_2x - learning_rate*cos(gradient_2x)
    gradient_2y <- sin(gradient_2x) + gradient_2x/10
    gradients_x <- c(gradients_x, gradient_2x)
    gradients_y <- c(gradients_y, gradient_2y)
  }
  gradients <- tibble(gradients_x, gradients_y)
  # print(gradients_x)
  return(gradients)
}
```


```{r}
β_0 <- 2.3
ρ <- 0.1

gradients <- grad(β_0, ρ)
gradients$gradients_y[which.min(gradients$gradients_y)]
```


```{r}
label <- tibble(β[7:length(β)], R[7:length(β)], "this")
ggplot() +
  geom_point(aes(gradients$gradients_x, gradients$gradients_y), color = custom_darkblue, size = 1) +
  geom_point(aes(β,
R), color = custom_red) + 
  labs(title = "Gradient Descent of R(β) = sin(β) + β/10", x = "β", y = "R(β)", subtitle = "Initial β = 2.3\nMinimum R(β) = -0.53375735")
```

* **Question 6-d**:
  * **Answer**:
```{r}
β_0 <- 1.4
ρ <- 0.1
gradients2 <- grad(β_0, ρ)
gradients2$gradients_y[which.min(gradients2$gradients_y)]
```


```{r}
ggplot() +
  geom_point(aes(gradients2$gradients_x, gradients2$gradients_y), color = custom_darkblue, size = 1) +
  geom_point(aes(β,
R), color = custom_red) + 
  labs(title = "Gradient Descent of R(β) = sin(β) + β/10", x = "β", y = "R(β)", subtitle = "Initial β = 1.4\nMinimum R(β) = -1.15708")
```

### Question 7:
Fit a neural network to the Default data. Use a single hidden layer with 10 units, and dropout regularization. Have a look at Labs 10.9.1-10.9.2 for guidance. Compare the classification performance of your model with that of linear logistic regression.

```{r}
Default <- na.omit(Default)
n <- nrow(Default)
set.seed(42)
ntest <- trunc(n / 3)
testid <- sample(1:n, ntest)
```

```{r}
glm.true <- Default
glm.true <- glm.true %>% select(default, everything()) %>% mutate(default_int = as.integer(default) - 1) %>% select(default, default_int, everything()) 
glm.true <- glm.true %>% select(default_int, everything(), -default) 
glm.true <- glm.true %>% rename("default" = "default_int") 

glm.fit <- glm(default ~ ., data = Default[-testid, ], family = "binomial")
glm.prob <- rep(0, nrow(Default[testid, ]))
glm.pred <- predict(glm.fit, Default[testid, ], type = "response")
glm.prob[glm.pred > 0.5] <- 1

glm.var <- var(glm.true[testid, "default"])

glm.R_2 <- 1 - (mean((glm.true[testid, "default"] - glm.prob)^2) / glm.var)
# glm.R_2

glm.table <- table(truth = glm.true[testid, "default"], prediction = glm.prob)
glm.acc <- (glm.table[1] + glm.table[4]) / sum(glm.table) * 100
# glm.acc 
```

```{r}
x <- scale(model.matrix(default ~ . - 1, data = Default))
y <- glm.true$default
```

```{r}
model <- keras_model_sequential() %>%
  layer_dense(input_shape = ncol(x), units = 10, activation = "relu") %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1, activation = "sigmoid")
summary(model)
```


```{r message=FALSE, warning=FALSE, output =FALSE}
model %>% compile(loss = "mse", optimizer = optimizer_rmsprop(), metrics = c("accuracy"))
model %>% fit(x[-testid, ], y[-testid], epochs = 30, batch_size = 128, validation_split = 0.2)
```


```{r}
predictions <- predict(model, x[testid, ])
model.probs <- rep(0, length(predictions))
model.probs[predictions > 0.5] <- 1

model.table <- table(prediction = model.probs, truth = glm.true[testid, "default"])

model.testacc <- (model.table[1] + model.table[4]) /sum(model.table) * 100

f_print(sprintf("The accuracy of the neural network model is %0.3f%%.", model.testacc))
cat("\n")
model.table

cat("\n")

f_print(sprintf("The accuracy of the logistic regression model is %0.3f%%.", glm.acc))
cat("\n")
glm.table
```


### Question 8:
From your collection of personal photographs, pick 10 images of animals (such as dogs, cats, birds, farm animals, etc.). If the subject does not occupy a reasonable part of the image, then crop the image. Now use a pretrained image classification CNN as in Lab 10.9.4 to predict the class of each of your images, and report the probabilities for the top five predicted classes for each image. 

```{r}
images <- load.dir('animals/')
```

```{r}
img_dir <- "animals"
image_names <- list.files(img_dir)
num_images <- length(image_names)
x <- array(dim = c(num_images, 224, 224, 3))
for (i in 1:num_images) {
  img_path <- paste(img_dir, image_names[i], sep = "/")
  img <- image_load(img_path, target_size = c(224, 224))
  x[i,,,] <- image_to_array(img)
}
```


```{r}
x <- imagenet_preprocess_input(x)

resnet_50_animals_model <- application_resnet50(weights = "imagenet")
summary(model)
```


```{r}
predicted_animals <- resnet_50_animals_model %>% predict(x) %>%
  imagenet_decode_predictions(top = 5)
names(predicted_animals) <- image_names
```

```{r}
for(i in seq(1,10)) {
  plot(images[i])
}
```


```{r}
f_print(sprintf("The predicted classes and probabilities of the images of dogs and cats are as follows with respect to order of appearance:"))
cat("\n")
print(predicted_animals)

```

### Question 9:
Fit a lag-5 autoregressive model to the NYSE data, as described in the text and Lab 10.9.6. Refit the model with a 12-level factor representing the month. Does this factor improve the performance of the model?
```{r}
xdata <- data.matrix(
  NYSE[, c("DJ_return", "log_volume", "log_volatility")]
)
  istrain <- NYSE[, "train"]
  xdata <- scale(xdata)
```

```{r}
lagm <- function(x, k = 1) {
  n <- nrow(x)
  pad <- matrix(NA, k, ncol(x))
  rbind(pad, x[1:(n-k), ])
}
```

```{r}
arframe <- data.frame(log_volume = xdata[, "log_volume"], L1 = lagm(xdata, 1), L2 = lagm(xdata, 2), L3 = lagm(xdata, 3), L4 = lagm(xdata, 4), L5 = lagm(xdata, 5))
```

```{r}
arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
```

```{r}
arfit <- lm(log_volume ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
1 - mean((arpred - arframe[!istrain, "log_volume"])^2) / V0
```

```{r}
arframed <- data.frame(day = NYSE[-(1:5), "day_of_week"], arframe)
arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])
arpredd <- predict(arfitd, arframed[!istrain, ])
1 - mean((arpredd - arframe[!istrain, "log_volume"])^2) / V0
```


```{r}
#???
arframe
```

```{r}
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 3, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn , c(1, 3, 2))
dim(xrnn)
```

```{r}
model <- keras_model_sequential() %>%
  layer_simple_rnn(units = 12, input_shape = list(5, 3), dropout = 0.1, recurrent_dropout = 0.1) %>%
  layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(), loss = "mse")
```

```{r}
history <- model %>% fit(
  xrnn[istrain,,], arframe[istrain, "log_volume"],
  batch_size = 64, epochs = 200, 
  validation_data = list(xrnn[!istrain,,], arframe[!istrain, "log_volume"])
)
kpred <- predict(model, xrnn[!istrain,,])
1 - mean((kpred - arframe[!istrain, "log_volume"])^2) / V0

# 0.415817
```

```{r}
x <- model.matrix(log_volume ~ . - 1, data = arframed)
colnames(x)
```

```{r}
arnnd <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', 
              input_shape = ncol(x)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1)
arnnd %>% compile(loss = "mse",
                  optimizer = optimizer_rmsprop())
```

```{r}
history <- arnnd %>% fit(
  x[istrain, ], arframe[istrain, "log_volume"], epochs = 100,
  batch_size = 32, validation_data = list(x[!istrain, ], arframe[!istrain, "log_volume"])
)
```

```{r}
plot(history)
npred <- predict(arnnd, x[!istrain, ])
1 - mean((arframe[!istrain, "log_volume"] - npred)^2) / V0
```


```{r}
month_n <- as.factor(month(NYSE$date))
xdata <- cbind(xdata, month_n)
```

```{r}

```


