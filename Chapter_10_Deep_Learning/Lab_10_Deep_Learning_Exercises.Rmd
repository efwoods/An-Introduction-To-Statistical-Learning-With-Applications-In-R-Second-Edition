
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
  comment = ""
)
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression
if(!require("splines")) install.packages("splines")
if(!require("gam")) install.packages("gam")
if(!require("akima")) install.packages("akima")
if(!require("tree")) install.packages("tree") # Classification and Regression Trees
if(!require("randomForest")) install.packages("randomForest")
if(!require("gbm")) install.packages("gbm") # Boosted Trees
if(!require("BART")) install.packages("BART")
if(!require("reticulate")) install.packages("reticulate") # Use python objects in R
if(!require("ROCR")) install.packages("ROCR")
if(!require("keras")) install.packages("keras") # Install keras for deep learning
if(!require("jpeg")) install.packages("jpeg")
if(!require("imager")) install.packages("imager")

library(imager)
library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
library(ROCR)
library(reticulate)
library(BART)
library(gbm)
library(randomForest)
library(tree)
library(akima)
library(gam)
library(splines)
library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
library(jpeg)
```

```{r output=FALSE, results = 'hide', message=FALSE}
# keras::install_keras(method = "conda", python_version = "3.10")
```

```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```

```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

```{r}
accuracy <- function(pred, truth) {
  mean(drop(as.numeric(pred)) == drop(truth))
}
```

## Applied
### Question 6:
Consider the simple function R(β) = sin(β) + β/10

* **Question 6-a**: Draw a graph of this funciton over the range β ∈ [-6, 6]
  * **Answer**:
```{r}
β <- seq(-6, 6)
R <- sin(β) + β/10
ggplot() + 
  geom_point(aes(β, R)) 
```

* **Question 6-b**: What is the derivative of this function?
  * **Answer**:
```{r}
f_print(sprintf("The derivative of the funciton R(β) = sin(β) + β/10 is δR(β) = cos(β)."))
δR <- cos(β)
```

* **Question 6-c**:
  * **Answer**:

```{r}
grad <- function(starting_point, learning_rate) {
  gradient_1x <- starting_point - learning_rate*cos(starting_point)
  gradient_2x <- gradient_1x - learning_rate*cos(gradient_1x)
  
  gradient_1y <- sin(gradient_1x) + gradient_1x/10
  gradient_2y <- sin(gradient_2x) + gradient_2x/10
  
  gradients_x <- c(gradient_1x, gradient_2x)
  gradients_y <- c(gradient_1y, gradient_2y)
  while(gradient_1y - gradient_2y > 0) {
    gradient_1y <- gradient_2y
    gradient_2x <- gradient_2x - learning_rate*cos(gradient_2x)
    gradient_2y <- sin(gradient_2x) + gradient_2x/10
    gradients_x <- c(gradients_x, gradient_2x)
    gradients_y <- c(gradients_y, gradient_2y)
  }
  gradients <- tibble(gradients_x, gradients_y)
  # print(gradients_x)
  return(gradients)
}
```


```{r}
β_0 <- 2.3
ρ <- 0.1

gradients <- grad(β_0, ρ)
gradients$gradients_y[which.min(gradients$gradients_y)]
```


```{r}
label <- tibble(β[7:length(β)], R[7:length(β)], "this")
ggplot() +
  geom_point(aes(gradients$gradients_x, gradients$gradients_y), color = custom_darkblue, size = 1) +
  geom_point(aes(β,
R), color = custom_red) + 
  labs(title = "Gradient Descent of R(β) = sin(β) + β/10", x = "β", y = "R(β)", subtitle = "Initial β = 2.3\nMinimum R(β) = -0.53375735")
```

* **Question 6-d**:
  * **Answer**:
```{r}
β_0 <- 1.4
ρ <- 0.1
gradients2 <- grad(β_0, ρ)
gradients2$gradients_y[which.min(gradients2$gradients_y)]
```


```{r}
ggplot() +
  geom_point(aes(gradients2$gradients_x, gradients2$gradients_y), color = custom_darkblue, size = 1) +
  geom_point(aes(β,
R), color = custom_red) + 
  labs(title = "Gradient Descent of R(β) = sin(β) + β/10", x = "β", y = "R(β)", subtitle = "Initial β = 1.4\nMinimum R(β) = -1.15708")
```

### Question 7:
Fit a neural network to the Default data. Use a single hidden layer with 10 units, and dropout regularization. Have a look at Labs 10.9.1-10.9.2 for guidance. Compare the classification performance of your model with that of linear logistic regression.

```{r}
Default <- na.omit(Default)
n <- nrow(Default)
set.seed(42)
ntest <- trunc(n / 3)
testid <- sample(1:n, ntest)
```

```{r}
glm.true <- Default
glm.true <- glm.true %>% select(default, everything()) %>% mutate(default_int = as.integer(default) - 1) %>% select(default, default_int, everything()) 
glm.true <- glm.true %>% select(default_int, everything(), -default) 
glm.true <- glm.true %>% rename("default" = "default_int") 

glm.fit <- glm(default ~ ., data = Default[-testid, ], family = "binomial")
glm.prob <- rep(0, nrow(Default[testid, ]))
glm.pred <- predict(glm.fit, Default[testid, ], type = "response")
glm.prob[glm.pred > 0.5] <- 1

glm.var <- var(glm.true[testid, "default"])

glm.R_2 <- 1 - (mean((glm.true[testid, "default"] - glm.prob)^2) / glm.var)
# glm.R_2

glm.table <- table(truth = glm.true[testid, "default"], prediction = glm.prob)
glm.acc <- (glm.table[1] + glm.table[4]) / sum(glm.table) * 100
# glm.acc 
```

```{r}
x <- scale(model.matrix(default ~ . - 1, data = Default))
y <- glm.true$default
```

```{r}
model <- keras_model_sequential() %>%
  layer_dense(input_shape = ncol(x), units = 10, activation = "relu") %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1, activation = "sigmoid")
summary(model)
```


```{r message=FALSE, warning=FALSE, output =FALSE}
model %>% compile(loss = "mse", optimizer = optimizer_rmsprop(), metrics = c("accuracy"))
model %>% fit(x[-testid, ], y[-testid], epochs = 30, batch_size = 128, validation_split = 0.2, verbose = 0)
```


```{r}
predictions <- predict(model, x[testid, ])
model.probs <- rep(0, length(predictions))
model.probs[predictions > 0.5] <- 1

model.table <- table(prediction = model.probs, truth = glm.true[testid, "default"])

model.testacc <- (model.table[1] + model.table[4]) /sum(model.table) * 100

f_print(sprintf("The accuracy of the neural network model is %0.3f%%.", model.testacc))
cat("\n")
model.table

cat("\n")

f_print(sprintf("The accuracy of the logistic regression model is %0.3f%%.", glm.acc))
cat("\n")
glm.table
```


### Question 8:
From your collection of personal photographs, pick 10 images of animals (such as dogs, cats, birds, farm animals, etc.). If the subject does not occupy a reasonable part of the image, then crop the image. Now use a pretrained image classification CNN as in Lab 10.9.4 to predict the class of each of your images, and report the probabilities for the top five predicted classes for each image. 

```{r}
images <- load.dir('animals/')
```

```{r}
img_dir <- "animals"
image_names <- list.files(img_dir)
num_images <- length(image_names)
x <- array(dim = c(num_images, 224, 224, 3))
for (i in 1:num_images) {
  img_path <- paste(img_dir, image_names[i], sep = "/")
  img <- image_load(img_path, target_size = c(224, 224))
  x[i,,,] <- image_to_array(img)
}
```


```{r}
x <- imagenet_preprocess_input(x)

resnet_50_animals_model <- application_resnet50(weights = "imagenet")
summary(model)
```


```{r}
predicted_animals <- resnet_50_animals_model %>% predict(x) %>%
  imagenet_decode_predictions(top = 5)
names(predicted_animals) <- image_names
```

```{r}
for(i in seq(1,10)) {
  plot(images[i])
}
```

```{r}
f_print(sprintf("The predicted classes and probabilities of the images of dogs and cats are as follows with respect to order of appearance:"))
cat("\n")
print(predicted_animals)
```

### Question 9:
Fit a lag-5 autoregressive model to the NYSE data, as described in the text and Lab 10.9.6. Refit the model with a 12-level factor representing the month. Does this factor improve the performance of the model?
```{r}
xdata <- data.matrix(
  NYSE[, c("DJ_return", "log_volume", "log_volatility")]
)
  istrain <- NYSE[, "train"]
  xdata <- scale(xdata)
```

```{r}
lagm <- function(x, k = 1) {
  n <- nrow(x)
  pad <- matrix(NA, k, ncol(x))
  rbind(pad, x[1:(n-k), ])
}
```

```{r}
arframe <- data.frame(log_volume = xdata[, "log_volume"], L1 = lagm(xdata, 1), L2 = lagm(xdata, 2), L3 = lagm(xdata, 3), L4 = lagm(xdata, 4), L5 = lagm(xdata, 5))
```

```{r}
arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
```


```{r}
arframed <- data.frame(day = NYSE[-(1:5), "day_of_week"], arframe)
```

```{r}
x <- model.matrix(log_volume ~ . - 1, data = arframed)
colnames(x)
```

```{r warning=FALSE, message=FALSE}
arnnd <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', 
              input_shape = ncol(x)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1)
arnnd %>% compile(loss = "mse",
                  optimizer = optimizer_rmsprop())
```

```{r results='hide', warning=FALSE, message=FALSE}
history <- arnnd %>% fit(
  x[istrain, ], arframe[istrain, "log_volume"], epochs = 100,
  batch_size = 32, validation_data = list(x[!istrain, ], arframe[!istrain, "log_volume"]), verbose = 0
)
```

```{r}
plot(history)
```


```{r}
npred <- predict(arnnd, x[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
r_2 <- 1 - mean(((arframe[!istrain, "log_volume"] - npred)^2)) / V0
```

```{r}
f_print(sprintf("The R-squared value of the lag-5 autoregressive model with a 5-level factor representing the day of the week is %0.3f.", r_2))
```


```{r}
arframem <- data.frame(month = month(NYSE[-(1:5),"date"]), arframe)
xmm <- model.matrix(log_volume ~ . - 1, data = arframem)
months <- to_categorical(xmm[,"month"])[,-1]
colnames(months) <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
arnnm.data <- cbind(months, xmm[,-1])[,]
```

```{r warning=FALSE}
arnnm.model <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', 
              input_shape = ncol(arnnm.data)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1)
arnnm.model %>% compile(loss = "mse",
                  optimizer = optimizer_rmsprop())
```

```{r results='hide'}
history <- arnnm.model %>% fit(
  arnnm.data[istrain, ], arframe[istrain, "log_volume"], epochs = 100,
  batch_size = 32, validation_data = list(arnnm.data[!istrain, ], arframe[!istrain, "log_volume"]), verbose = 0
)
```

```{r}
plot(history)
```

```{r}
arm_pred <- predict(arnnm.model, arnnm.data[!istrain, ])
arm_r_2 <- 1 - mean((arframe[!istrain, "log_volume"] - arm_pred)^2) / V0
f_print(sprintf("The lag-5 autoregressive model refit with a 12-level factor representing the month has an R-squared value of %0.3f.", arm_r_2))
```
```{r}
f_print(sprintf("The inclusion of the 12-level factor representing the month reduced the fit of the model to the data."))
```


### Question 10:
In Section 10.9.6, we showed how to fit a linear AR model to the NYSE data using the lm() function. However, we also mentioned that we can "flatten" the short sequences produced for the RNN model in order to fit a linear AR model. Use this latter approach to fit a linear AR model to the NYSE data. Compare the test R-squared of this linear AR model to that of the linear AR model that we fit in the lab. What are the advantages/disadvantages of each approach?

```{r}
arfit <- lm(log_volume ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
lar_lab_r_2 <- 1 - mean((arpred - arframe[!istrain, "log_volume"])^2) / V0
```

```{r}
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 3, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn , c(1, 3, 2))
dim(xrnn)
```

```{r warning=FALSE}
model <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(5, 3)) %>%
  layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(), loss = "mse")
```

```{r}
history <- model %>% fit(
  xrnn[istrain,,], arframe[istrain, "log_volume"],
  batch_size = 64, epochs = 200, 
  validation_data = list(xrnn[!istrain,,], arframe[!istrain, "log_volume"]), verbose = 0
)
```

```{r}
V0 <- var(arframe[!istrain, "log_volume"])
larm_kpred <- predict(model, xrnn[!istrain,,])
larm_r_2 <- 1 - mean((larm_kpred - arframe[!istrain, "log_volume"])^2) / V0
```

```{r}
f_print(sprintf("The R-squared value of the linear autoregressive model fit in the lab is: %0.6f. The R-squared value of the linear autoregressive model fit using a flattened sequences produced for the RNN model is %0.6f. These figures are identical to at least 5 significant digits. The advantages of using the lm function to create an autoregressive model is a reduction in time training the model. The advantage of using a neural network is the ability to add non-linearities to the existing model by including hidden layers in the future.", lar_lab_r_2, larm_r_2))
```
### Question 11:
Repeat the previous exercise, but now fit a nonlinear AR model by "flattening" the short sequences produced for the RNN model.
```{r warning=FALSE}
ar_nl_flat_model <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(5, 3)) %>%
  layer_dense(units = 12, activation = "relu") %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 1)
ar_nl_flat_model %>% compile(optimizer = optimizer_rmsprop(), loss = "mse")

history <- ar_nl_flat_model %>% fit(
  xrnn[istrain,,], arframe[istrain, "log_volume"],
  batch_size = 64, epochs = 200, 
  validation_data = list(xrnn[!istrain,,], arframe[!istrain, "log_volume"]), verbose = 0
)
```

```{r}
V0 <- var(arframe[!istrain, "log_volume"])
ar_nl_flat_model.pred <- predict(ar_nl_flat_model, xrnn[!istrain,,])
ar_nl_flat_model.r_2 <- 1 - mean((ar_nl_flat_model.pred - arframe[!istrain, "log_volume"])^2) / V0
f_print(sprintf("The R-squared value of the flat-input, non-linear autoregressive model is %0.5f.", ar_nl_flat_model.r_2))
```

### Question 12:
Consider the RNN fit to the NYSE data in Section 10.9.6. Modify the code to allow inclusion of the variable day_of_week, and fit the RNN. Compute the test R-squared.

```{r}
arframem <- data.frame(month = month(NYSE[-(1:5),"date"]), arframe)
xmm <- model.matrix(log_volume ~ . - 1, data = arframem)
months <- to_categorical(xmm[,"month"])[,-1]
colnames(months) <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
```


```{r}
day_of_week <- to_categorical(as.integer(as.factor(NYSE[, "day_of_week"])))
day_of_week <- (day_of_week[,-1])
col_names_day_of_week <- c(unique(NYSE[, "day_of_week"]))
colnames(day_of_week) <- col_names_day_of_week
```

```{r}
xdata <- cbind(day_of_week, xdata)
```

```{r}
arframe <- data.frame(log_volume = xdata[, "log_volume"], L1 = lagm(xdata, 1), L2 = lagm(xdata, 2), L3 = lagm(xdata, 3), L4 = lagm(xdata, 4), L5 = lagm(xdata, 5))

arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
```

```{r}
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 8, 5))
xrnn <- array(xrnn, c(n, 8, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn , c(1, 3, 2))
```

```{r warning=FALSE}
model <- keras_model_sequential() %>%
  layer_simple_rnn(units = 12, input_shape = list(5, 8), dropout = 0.1, recurrent_dropout = 0.1) %>%
  layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(), loss = "mse")
```

```{r}
history <- model %>% fit(
  xrnn[istrain,,], arframe[istrain, "log_volume"],
  batch_size = 64, epochs = 200, 
  validation_data = list(xrnn[!istrain,,], arframe[!istrain, "log_volume"]), verbose = 0
)
```


```{r}
rnn_day_of_week_kpred <- predict(model, xrnn[!istrain,,])
rnn_day_of_week_test_r_2 <- 1 - mean((rnn_day_of_week_kpred - arframe[!istrain, "log_volume"])^2) / V0
f_print(sprintf("The test R-squared of the RNN fit to include the day of the week is %0.7f.", rnn_day_of_week_test_r_2))
```

### Question 13:
Repeat the analysis of Lab 10.9.5 on the IMDb data using a similarly structured neural network. There we used a dictionary of size 10,000. Consider the effects of varying the dictionary size. Try the values 1000, 3000, 5000, and 10,000, and compare the results.

```{r}
library(Matrix)
one_hot <- function(sequences, dimension) {
  seqlen <- sapply(sequences, length)
  n <- length(seqlen)
  rowind <- rep(1:n, seqlen)
  colind <- unlist(sequences)
  sparseMatrix(i = rowind, j = colind, dims = c(n, dimension))
}
```

```{r message=FALSE}

dictionary_size <- c(1000, 3000, 5000, 10000)
results_train <- c("The model training accuracy for dictionary size of 1000:", "The model training accuracy for dictionary size of 3000:", "The model training accuracy for dictionary size of 5000:", "The model training accuracy for dictionary size of 10000:")
results_test <- c("The model test accuracy for dictionary size of 1000:", "The model test accuracy for dictionary size of 3000:", "The model test accuracy for dictionary size of 5000:", "The model test accuracy for dictionary size of 10000:")

for(i in seq(1, length(dictionary_size))){
    max_features <- dictionary_size[i]
    imdb <- dataset_imdb(num_words = max_features)
    c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
    
    x_train_1h <- one_hot(x_train, dictionary_size[i])
    x_test_1h <- one_hot(x_test, dictionary_size[i])
    
    set.seed(3)
    ival <- sample(seq(along = y_train), dictionary_size[i]/5)
    
    # Compute train accuracy
    model <- keras_model_sequential() %>%
      layer_dense(units = 16, activation = "relu", input_shape = c(dictionary_size[i])) %>%
      layer_dense(units = 16, activation = "relu") %>%
      layer_dense(units = 1, activation = "sigmoid")
    model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("accuracy"))
    history_train <- model %>% fit(x_train_1h[-ival, ], y_train[-ival], epochs = 20, batch_size = 512, validation_data = list(x_train_1h[ival, ], y_train[ival]), verbose = 0)
    
    # Compute test accuracy
    model <- keras_model_sequential() %>%
      layer_dense(units = 16, activation = "relu", input_shape = c(dictionary_size[i])) %>%
      layer_dense(units = 16, activation = "relu") %>%
      layer_dense(units = 1, activation = "sigmoid")
    model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("accuracy"))
    history_test <- model %>% fit(
      x_train_1h[-ival, ], y_train[-ival], epochs = 20, 
      batch_size = 512, validation_data = list(x_test_1h, y_test),
      verbose = 0
    )
    f_print(sprintf("%s", results_train[i]))
    print(history_train)
    cat("\n\n")
    f_print(sprintf("%s", results_test[i]))
    print(history_test)
}

```

```{r}
# Compare results
f_print(sprintf("The training accuracy of the model increased with dictionary size. The test validation accuracy peaked at a dictionary size of 3000. Increasing the dicitonary size did not necessarily increase the test validation accuracy."))
```



