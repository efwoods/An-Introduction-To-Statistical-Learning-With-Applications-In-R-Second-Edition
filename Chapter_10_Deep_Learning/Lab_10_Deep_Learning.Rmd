---
title: "Lab 10 Deep Learning"
author: "Evan Woods"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.asp = 0.618)
knitr::opts_chunk$set(out.width = "70%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(
  comment = ""
)
```

```{r message=FALSE, include=FALSE}
if(!require("MASS")) install.packages("MASS")
if(!require("ISLR2")) install.packages("ISLR2")
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("HH")) install.packages("HH") # VIF
if(!require("e1071")) install.packages("e1071") # naiveBayes
if(!require("class")) install.packages("class") # knn
if(!require("formulaic")) install.packages("formulaic")
if(!require("caTools")) install.packages("caTools")
if(!require("caret")) install.packages("caret")
if(!require("boot")) install.packages("boot")
if(!require("leaps")) install.packages("leaps") # regsubsets
if(!require("glmnet")) install.packages("glmnet") # Ridge and Lasso Regression
if(!require("pls")) install.packages("pls") # Partial Least Squares & Principal Component Regression
if(!require("splines")) install.packages("splines")
if(!require("gam")) install.packages("gam")
if(!require("akima")) install.packages("akima")
if(!require("tree")) install.packages("tree") # Classification and Regression Trees
if(!require("randomForest")) install.packages("randomForest")
if(!require("gbm")) install.packages("gbm") # Boosted Trees
if(!require("BART")) install.packages("BART")
if(!require("reticulate")) install.packages("reticulate") # Use python objects in R
if(!require("ROCR")) install.packages("ROCR")
if(!require("keras")) install.packages("keras") # Install keras for deep learning

library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
library(ROCR)
library(reticulate)
library(BART)
library(gbm)
library(randomForest)
library(tree)
library(akima)
library(gam)
library(splines)
library(glmnet)
library(pls)
library(leaps)
library(formulaic)
library(class)
library(e1071)
library(HH)
library(MASS)
library(ISLR2)
library(tidyverse)
library(caTools)
library(caret)
library(boot)
```

```{r}
keras::install_keras(method = "conda", python_version = "3.10")
```


```{r include=FALSE}
custom_darkblue = "#1A0875"
custom_lightblue = "#34ABEB"
custom_red = "#a60808"
```

```{r include=FALSE}
f_print <- function(string){
  cat(str_wrap(string = string, width = 80, indent = 0, exdent = 0, whitespace_only = TRUE))
}
```

## Single layer network on the hitters data
```{r}
Gitters <- na.omit(Hitters)
n <- nrow(Gitters)
set.seed(13)
ntest <- trunc(n / 3)
testid <- sample(1:n, ntest)
```

```{r}
lfit <- lm(Salary ~., data = Gitters[-ntest, ])
```

```{r}
lpred <- predict(lfit, Gitters[testid, ])
with(Gitters[testid, ], mean(abs(lpred - Salary)))
```

```{r}
x <- scale(model.matrix(Salary ~ . - 1, data = Gitters))
y <- Gitters$Salary
```


```{r}
cvfit <- cv.glmnet(x[-testid, ], y[-testid], type.measure = "mae")
cpred <- predict(cvfit, x[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
```

```{r}
modnn <- keras_model_sequential() %>%
  layer_dense(units = 50, activation = "relu", 
              input_shape = ncol(x)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1)
```

```{r}
modnn %>% compile(loss = "mse", 
                  optimizer = optimizer_rmsprop(), 
                  metrics = list("mean_absolute_error")
)
```

```{r}
history <- modnn %>% fit(
  x[-testid, ], y[-testid], epochs = 1500, batch_size = 32, validation_data = list(x[testid, ], y[testid])
)
```

```{r}
plot(history)
```

```{r}
npred <- predict(modnn, x[testid, ])
mean(abs(y[testid] - npred))
```


## A multilayer network on the MNIST Digit Data
```{r}
mnist <- dataset_mnist()
x_train <- mnist$train$x
g_train <- mnist$train$y
x_test <- mnist$test$x
g_test <- mnist$test$y
dim(x_train)

dim(x_test)
```

```{r}
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
y_train <- to_categorical(g_train, 10)
y_test <- to_categorical(g_test, 10)
```

```{r}
x_train <- x_train / 255
x_test <- x_test / 255
```


```{r}
modelnn <- keras_model_sequential() %>%
  layer_dense(units = 256, activation = "relu", 
              input_shape = c(784)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = "softmax")
```

```{r}
summary(modelnn)
```


```{r}
modelnn %>% compile(loss = "categorical_crossentropy", 
                    optimizer = optimizer_rmsprop(), metrics = c("accuracy")
                    )
```

```{r}
system.time(
  history <- modelnn %>%
    fit(x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2)
)
plot(history, smooth = FALSE)
```
```{r}
accuracy <- function(pred, truth) {
  mean(drop(as.numeric(pred)) == drop(truth))
}

modelnn %>% predict(x_test) %>% k_argmax() %>% accuracy(g_test)
```

```{r}
modellr <- keras_model_sequential() %>%
  layer_dense(input_shape = 784, units = 10, 
              activation = "softmax")
summary(modellr)
```

```{r}
modellr %>% compile(loss = "categorical_crossentropy", 
                    optimizer = optimizer_rmsprop(), metrics = c("accuracy"))
modellr %>% fit(x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2)
modellr %>% predict(x_test) %>% k_argmax() %>% accuracy(g_test)
```

## Convolutional Neural Networks
```{r}
cifar100 <- dataset_cifar100()
names(cifar100)
x_train <- cifar100$train$x
g_train <- cifar100$train$y
x_test <- cifar100$test$x
g_test <- cifar100$test$y
dim(x_train)

range(x_train[1,,,1])
```

```{r}
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(g_train, 100)
dim(y_train)
```

```{r}
library(jpeg)
par(mar = c(0,0,0,0), mfrow = c(5,5))
index <- sample(seq(50000), 25)
for (i in index) plot(as.raster(x_train[i,,,]))
```

```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), 
                padding = "same", activation = "relu", 
                input_shape = c(32, 32, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3,3),
                padding = "same", activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3),
                padding = "same", activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same", activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 100, activation = "softmax")
summary(model)
```

```{r}
model %>% compile(loss = "categorical_crossentropy", 
                  optimizer = optimizer_rmsprop(), metrics = c("accuracy"))
history <- model %>% fit(x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2)
model %>% predict(x_test) %>% k_argmax() %>% accuracy(g_test)
```

## Using Pretrained CNN Models
```{r}
img_dir <- "book_images"
image_names <- list.files(img_dir)
num_images <- length(image_names)
x <- array(dim = c(num_images, 224, 224, 3))
for (i in 1:num_images) {
  img_path <- paste(img_dir, image_names[i], sep = "/")
  img <- image_load(img_path, target_size = c(224, 224))
  x[i,,,] <- image_to_array(img)
}
x <- imagenet_preprocess_input(x)
```

```{r}
model <- application_resnet50(weights = "imagenet")
summary(model)
```

```{r}
pred6 <- model %>% predict(x) %>%
  imagenet_decode_predictions(top = 3)
names(pred6) <- image_names
print(pred6)
```

## IMDb Document Classification
```{r}
max_features <- 10000
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
```


```{r}
word_index <- dataset_imdb_word_index()
decode_review <- function(text, word_index) {
  word <- names(word_index)
  idx <- unlist(word_index, use.names = FALSE)
  word <- c("<PAD>", "<START>", "<UNK>", "<UNUSED>", word)
  idx <- c(0:3, idx + 3)
  words <- word[match(text, idx, 2)]
  paste(words, collapse = " ")
}

decode_review(x_train[[1]][1:12], word_index)
```

```{r}
library(Matrix)
one_hot <- function(sequences, dimension) {
  seqlen <- sapply(sequences, length)
  n <- length(seqlen)
  rowind <- rep(1:n, seqlen)
  colind <- unlist(sequences)
  sparseMatrix(i = rowind, j = colind, dims = c(n, dimension))
}
```


```{r}
x_train_1h <- one_hot(x_train, 10000)
x_test_1h <- one_hot(x_test, 10000)
dim(x_train_1h)

nnzero(x_train_1h) / (25000 * 10000)
```

```{r}
set.seed(3)
ival <- sample(seq(along = y_train), 2000)
```

```{r}
library(glmnet)
fitlm <- glmnet(x_train_1h[-ival, ], y_train[-ival], family = "binomial", standardize = FALSE)
classlmv <- predict(fitlm, x_train_1h[ival, ]) >0
acclmv <- apply(classlmv, 2, accuracy, y_train[ival] > 0)
```

```{r}
par(mar = c(4, 4, 4, 4), mfrow = c(1, 1))
plot(-log(fitlm$lambda), acclmv)
```

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("accuracy"))
history <- model %>% fit(x_train_1h[-ival, ], y_train[-ival], epochs = 20, batch_size = 512, validation_data = list(x_train_1h[ival, ], y_train[ival]))
```

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("accuracy"))
history <- model %>% fit(
  x_train_1h[-ival, ], y_train[-ival], epochs = 20, 
  batch_size = 512, validation_data = list(x_test_1h, y_test)
)
```

## Recurrent Neural Networks
### Sequential Models for Document Classification
```{r}
wc <- sapply(x_train, length)
median(wc)
sum(wc <= 500) / length(wc)
```

```{r}
maxlen <- 500
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
dim(x_train)
dim(x_test)
x_train[1, 490:500]
```

```{r}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = 10000, output_dim = 32) %>%
                    layer_lstm(units = 32) %>%
                    layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("acc"))
history <- model %>% fit(x_train, y_train, epochs = 10, batch_size = 128, validation_data = list(x_test, y_test))
plot(history)
predy <- predict(model, x_test) > 0.5
mean(abs(y_test == as.numeric(predy)))
```

## Time Series Prediction
```{r}
xdata <- data.matrix(
  NYSE[, c("DJ_return", "log_volume", "log_volatility")]
)
  istrain <- NYSE[, "train"]
  xdata <- scale(xdata)
```

```{r}
lagm <- function(x, k = 1) {
  n <- nrow(x)
  pad <- matrix(NA, k, ncol(x))
  rbind(pad, x[1:(n-k), ])
}
```

```{r}
arframe <- data.frame(log_volume = xdata[, "log_volume"], L1 = lagm(xdata, 1), L2 = lagm(xdata, 2), L3 = lagm(xdata, 3), L4 = lagm(xdata, 4), L5 = lagm(xdata, 5))
```

```{r}
arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
```

```{r}
arfit <- lm(log_volume ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
1 - mean((arpred - arframe[!istrain, "log_volume"])^2) / V0
```

```{r}
arframed <- data.frame(day = NYSE[-(1:5), "day_of_week"], arframe)
arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])
arpredd <- predict(arfitd, arframed[!istrain, ])
1 - mean((arpredd - arframe[!istrain, "log_volume"])^2) / V0
```

```{r}
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 3, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn , c(1, 3, 2))
dim(xrnn)
```

```{r}
model <- keras_model_sequential() %>%
  layer_simple_rnn(units = 12, input_shape = list(5, 3), dropout = 0.1, recurrent_dropout = 0.1) %>%
  layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(), loss = "mse")
```

```{r}
history <- model %>% fit(
  xrnn[istrain,,], arframe[istrain, "log_volume"],
  batch_size = 64, epochs = 200, 
  validation_data = list(xrnn[!istrain,,], arframe[!istrain, "log_volume"])
)
kpred <- predict(model, xrnn[!istrain,,])
1 - mean((kpred - arframe[!istrain, "log_volume"])^2) / V0
```

```{r}
x <- model.matrix(log_volume ~ . - 1, data = arframed)
colnames(x)
```

```{r}
arnnd <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', 
              input_shape = ncol(x)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1)
arnnd %>% compile(loss = "mse",
                  optimizer = optimizer_rmsprop())
```

```{r}
history <- arnnd %>% fit(
  x[istrain, ], arframe[istrain, "log_volume"], epochs = 100,
  batch_size = 32, validation_data = list(x[!istrain, ], arframe[!istrain, "log_volume"])
)
```

```{r}
plot(history)
npred <- predict(arnnd, x[!istrain, ])
1 - mean((arframe[!istrain, "log_volume"] - npred)^2) / V0
```

















